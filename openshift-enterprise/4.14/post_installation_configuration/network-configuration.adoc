:_mod-docs-content-type: ASSEMBLY
:context: post-install-network-configuration
[id="post-install-network-configuration"]
= Postinstallation network configuration
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS

toc::[]

After installing {product-title}, you can further expand and customize your
network to your requirements.

:leveloffset: +1

// Module included in the following assemblies:
//
// * installing/installing_aws/installing-aws-network-customizations.adoc
// * installing/installing_azure/installing-azure-network-customizations.adoc
// * installing/installing_bare_metal/installing-bare-metal-network-customizations.adoc
// * installing/installing_gcp/installing-gcp-network-customizations.adoc
// * installing/installing_ibm_power/installing-ibm-power.adoc
// * installing/installing_ibm_power/installing-restricted-networks-ibm-power.adoc
// * installing/installing_ibm_z/installing-ibm-z-kvm.adoc
// * installing/installing_ibm_z/installing-ibm-z.adoc
// * installing/installing_ibm_z/installing-restricted-networks-ibm-z-kvm.adoc
// * installing/installing_ibm_z/installing-restricted-networks-ibm-z.adoc
// * installing/installing_vsphere/installing-vsphere-installer-provisioned-network-customizations.adoc
// * installing/installing_vsphere/installing-vsphere-network-customizations.adoc
// * networking/cluster-network-operator.adoc
// * networking/network_policy/logging-network-policy.adoc
// * post_installation_configuration/network-configuration.adoc
// * installing/installing_ibm_cloud_public/installing-ibm-cloud-network-customizations.adoc
// * installing/installing_ibm_z/installing-ibm-z.adoc
// * installing/installing_ibm_z/installing-ibm-z-kvm.adoc
// * installing/installing_ibm_z/installing-restricted-networks-ibm-z.adoc
// * installing/installing_ibm_z/installing-restricted-networks-ibm-z-kvm.adoc
// * installing/installing_ibm_power/installing-ibm-power.adoc
// * installing/installing_ibm_power/installing-restricted-networks-ibm-power.adoc
// * installing/installing_azure_stack_hub/installing-azure-stack-hub-network-customizations.adoc

// Installation assemblies need different details than the CNO operator does

:post-install-network-configuration:

:_mod-docs-content-type: CONCEPT
[id="nw-operator-cr_{context}"]
= Cluster Network Operator configuration

The configuration for the cluster network is specified as part of the Cluster Network Operator (CNO) configuration and stored in a custom resource (CR) object that is named `cluster`. The CR specifies the fields for the `Network` API in the `operator.openshift.io` API group.

The CNO configuration inherits the following fields during cluster installation from the `Network` API in the `Network.config.openshift.io` API group and these fields cannot be changed:

`clusterNetwork`:: IP address pools from which pod IP addresses are allocated.
`serviceNetwork`:: IP address pool for services.
`defaultNetwork.type`:: Cluster network plugin, such as OpenShift SDN or OVN-Kubernetes.

// For the post installation assembly, no further content is provided.
[NOTE]
====
After cluster installation, you cannot modify the fields listed in the previous section.
====


:!post-install-network-configuration:

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * networking/configuring-a-custom-pki.adoc
// * networking/enable-cluster-wide-proxy.adoc
// * post_installation_configuration/network-configuration.adoc

:_mod-docs-content-type: PROCEDURE
[id="nw-proxy-configure-object_{context}"]
= Enabling the cluster-wide proxy

The `Proxy` object is used to manage the cluster-wide egress proxy. When a cluster is installed or upgraded without the proxy configured, a `Proxy` object is still generated but it will have a nil `spec`. For example:

[source,yaml]
----
apiVersion: config.openshift.io/v1
kind: Proxy
metadata:
  name: cluster
spec:
  trustedCA:
    name: ""
status:
----

A cluster administrator can configure the proxy for {product-title} by modifying this `cluster` `Proxy` object.

[NOTE]
====
Only the `Proxy` object named `cluster` is supported, and no additional proxies can be created.
====

.Prerequisites

* Cluster administrator permissions
* {product-title} `oc` CLI tool installed

.Procedure

. Create a config map that contains any additional CA certificates required for proxying HTTPS connections.
+
[NOTE]
====
You can skip this step if the proxy's identity certificate is signed by an authority from the RHCOS trust bundle.
====

.. Create a file called `user-ca-bundle.yaml` with the following contents, and provide the values of your PEM-encoded certificates:
+
[source,yaml]
----
apiVersion: v1
data:
  ca-bundle.crt: | <1>
    <MY_PEM_ENCODED_CERTS> <2>
kind: ConfigMap
metadata:
  name: user-ca-bundle <3>
  namespace: openshift-config <4>
----
<1> This data key must be named `ca-bundle.crt`.
<2> One or more PEM-encoded X.509 certificates used to sign the proxy's
identity certificate.
<3> The config map name that will be referenced from the `Proxy` object.
<4> The config map must be in the `openshift-config` namespace.

.. Create the config map from this file:
+
[source,terminal]
----
$ oc create -f user-ca-bundle.yaml
----

. Use the `oc edit` command to modify the `Proxy` object:
+
[source,terminal]
----
$ oc edit proxy/cluster
----

. Configure the necessary fields for the proxy:
+
[source,yaml]
----
apiVersion: config.openshift.io/v1
kind: Proxy
metadata:
  name: cluster
spec:
  httpProxy: http://<username>:<pswd>@<ip>:<port> <1>
  httpsProxy: https://<username>:<pswd>@<ip>:<port> <2>
  noProxy: example.com <3>
  readinessEndpoints:
  - http://www.google.com <4>
  - https://www.google.com
  trustedCA:
    name: user-ca-bundle <5>
----
+
--
<1> A proxy URL to use for creating HTTP connections outside the cluster. The URL scheme must be `http`.
<2> A proxy URL to use for creating HTTPS connections outside the cluster. The URL scheme must be either `http` or `https`. Specify a URL for the proxy that supports the URL scheme. For example, most proxies will report an error if they are configured to use `https` but they only support `http`. This failure message may not propagate to the logs and can appear to be a network connection failure instead. If using a proxy that listens for `https` connections from the cluster, you may need to configure the cluster to accept the CAs and certificates that the proxy uses.
<3> A comma-separated list of destination domain names, domains, IP addresses or other network CIDRs to exclude proxying.
+
Preface a domain with `.` to match subdomains only. For example, `.y.com` matches `x.y.com`, but not `y.com`. Use `*` to bypass proxy for all destinations.
If you scale up workers that are not included in the network defined by the `networking.machineNetwork[].cidr` field from the installation configuration, you must add them to this list to prevent connection issues.
+
This field is ignored if neither the `httpProxy` or `httpsProxy` fields are set.
<4> One or more URLs external to the cluster to use to perform a readiness check before writing the `httpProxy` and `httpsProxy` values to status.
<5> A reference to the config map in the `openshift-config` namespace that contains additional CA certificates required for proxying HTTPS connections. Note that the config map must already exist before referencing it here. This field is required unless the proxy's identity certificate is signed by an authority from the RHCOS trust bundle.
--

. Save the file to apply the changes.

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * post_installation_configuration/configuring-private-cluster.adoc
// * post_installation_configuration/network-configuration.adoc

:_mod-docs-content-type: PROCEDURE
[id="private-clusters-setting-dns-private_{context}"]
= Setting DNS to private

After you deploy a cluster, you can modify its DNS to use only a private zone.

.Procedure

. Review the `DNS` custom resource for your cluster:
+
[source,terminal]
----
$ oc get dnses.config.openshift.io/cluster -o yaml
----
+
.Example output
[source,yaml]
----
apiVersion: config.openshift.io/v1
kind: DNS
metadata:
  creationTimestamp: "2019-10-25T18:27:09Z"
  generation: 2
  name: cluster
  resourceVersion: "37966"
  selfLink: /apis/config.openshift.io/v1/dnses/cluster
  uid: 0e714746-f755-11f9-9cb1-02ff55d8f976
spec:
  baseDomain: <base_domain>
  privateZone:
    tags:
      Name: <infrastructure_id>-int
      kubernetes.io/cluster/<infrastructure_id>: owned
  publicZone:
    id: Z2XXXXXXXXXXA4
status: {}
----
+
Note that the `spec` section contains both a private and a public zone.

. Patch the `DNS` custom resource to remove the public zone:
+
[source,terminal]
----
$ oc patch dnses.config.openshift.io/cluster --type=merge --patch='{"spec": {"publicZone": null}}'
dns.config.openshift.io/cluster patched
----
+
Because the Ingress Controller consults the `DNS` definition when it creates `Ingress` objects, when you create or modify `Ingress` objects, only private records are created.
+
[IMPORTANT]
====
DNS records for the existing Ingress objects are not modified when you remove the public zone.
====

. Optional: Review the `DNS` custom resource for your cluster and confirm that the public zone was removed:
+
[source,terminal]
----
$ oc get dnses.config.openshift.io/cluster -o yaml
----
+
.Example output
[source,yaml]
----
apiVersion: config.openshift.io/v1
kind: DNS
metadata:
  creationTimestamp: "2019-10-25T18:27:09Z"
  generation: 2
  name: cluster
  resourceVersion: "37966"
  selfLink: /apis/config.openshift.io/v1/dnses/cluster
  uid: 0e714746-f755-11f9-9cb1-02ff55d8f976
spec:
  baseDomain: <base_domain>
  privateZone:
    tags:
      Name: <infrastructure_id>-int
      kubernetes.io/cluster/<infrastructure_id>-wfpg4: owned
status: {}
----

:leveloffset!:

[id="post-install-configuring_ingress_cluster_traffic"]
== Configuring ingress cluster traffic

// This section is sourced from networking/configuring_ingress_cluster_traffic/overview-traffic.adoc

{product-title} provides the following methods for communicating from outside the cluster with services running in the cluster:

* If you have HTTP/HTTPS, use an Ingress Controller.
* If you have a TLS-encrypted protocol other than HTTPS, such as TLS with the SNI header, use an Ingress Controller.
* Otherwise, use a load balancer, an external IP, or a node port.

[options="header"]
|===

|Method |Purpose

|xref:../networking/configuring_ingress_cluster_traffic/configuring-ingress-cluster-traffic-ingress-controller.adoc#configuring-ingress-cluster-traffic-ingress-controller[Use an Ingress Controller]
|Allows access to HTTP/HTTPS traffic and TLS-encrypted protocols other than HTTPS, such as TLS with the SNI header.

|xref:../networking/configuring_ingress_cluster_traffic/configuring-ingress-cluster-traffic-load-balancer.adoc#configuring-ingress-cluster-traffic-load-balancer[Automatically assign an external IP by using a load balancer service]
|Allows traffic to non-standard ports through an IP address assigned from a pool.

|xref:../networking/configuring_ingress_cluster_traffic/configuring-ingress-cluster-traffic-service-external-ip.adoc#configuring-ingress-cluster-traffic-service-external-ip[Manually assign an external IP to a service]
|Allows traffic to non-standard ports through a specific IP address.

|xref:../networking/configuring_ingress_cluster_traffic/configuring-ingress-cluster-traffic-nodeport.adoc#configuring-ingress-cluster-traffic-nodeport[Configure a `NodePort`]
|Expose a service on all nodes in the cluster.
|===

[id="post-install-configuring-node-port-service-range"]
== Configuring the node port service range

As a cluster administrator, you can expand the available node port range. If your cluster uses of a large number of node ports, you might need to increase the number of available ports.

The default port range is `30000-32767`. You can never reduce the port range, even if you first expand it beyond the default range.

[id="post-install-configuring-node-port-service-range-prerequisites"]
=== Prerequisites

- Your cluster infrastructure must allow access to the ports that you specify within the expanded range. For example, if you expand the node port range to `30000-32900`, the inclusive port range of `32768-32900` must be allowed by your firewall or packet filtering configuration.

:leveloffset: +3

// Module included in the following assemblies:
//
// * networking/configuring-node-port-service-range.adoc

:_mod-docs-content-type: PROCEDURE
[id="nw-nodeport-service-range-edit_{context}"]
= Expanding the node port range

You can expand the node port range for the cluster.

.Prerequisites

* Install the OpenShift CLI (`oc`).
* Log in to the cluster with a user with `cluster-admin` privileges.

.Procedure

. To expand the node port range, enter the following command. Replace `<port>` with the largest port number in the new range.
+
[source,terminal]
----
$ oc patch network.config.openshift.io cluster --type=merge -p \
  '{
    "spec":
      { "serviceNodePortRange": "30000-<port>" }
  }'
----
+
[TIP]
====
You can alternatively apply the following YAML to update the node port range:

[source,yaml]
----
apiVersion: config.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  serviceNodePortRange: "30000-<port>"
----
====
+
.Example output
[source,terminal]
----
network.config.openshift.io/cluster patched
----

. To confirm that the configuration is active, enter the following command. It can take several minutes for the update to apply.
+
[source,terminal]
----
$ oc get configmaps -n openshift-kube-apiserver config \
  -o jsonpath="{.data['config\.yaml']}" | \
  grep -Eo '"service-node-port-range":["[[:digit:]]+-[[:digit:]]+"]'
----
+
.Example output
[source,terminal]
----
"service-node-port-range":["30000-33000"]
----

:leveloffset!:

[id="post-install-configuring-ipsec-ovn"]
== Configuring IPsec encryption

With IPsec enabled, all network traffic between nodes on the OVN-Kubernetes network plugin travels through an encrypted tunnel.

IPsec is disabled by default.

[id="post-install-configuring-ipsec-ovn-prerequisites"]
=== Prerequisites

- Your cluster must use the OVN-Kubernetes network plugin.

:leveloffset: +3

// Module included in the following assemblies:
//
// * networking/ovn_kubernetes_network_provider/configuring-ipsec-ovn.adoc

:_mod-docs-content-type: PROCEDURE
[id="nw-ovn-ipsec-enable_{context}"]
= Enabling pod-to-pod IPsec encryption

As a cluster administrator, you can enable pod-to-pod IPsec encryption after cluster installation.

.Prerequisites

* Install the {oc-first}.
* You are logged in to the cluster as a user with `cluster-admin` privileges.
* You have reduced the size of your cluster MTU by `46` bytes to allow for the overhead of the IPsec ESP header.

.Procedure

* To enable IPsec encryption, enter the following command:
+
[source,terminal]
----
$ oc patch networks.operator.openshift.io cluster --type=merge \
-p '{"spec":{"defaultNetwork":{"ovnKubernetesConfig":{"ipsecConfig":{ }}}}}'
----

:leveloffset!:
:leveloffset: +3

// Module included in the following assemblies:
//
// * networking/ovn_kubernetes_network_provider/about-ipsec-ovn.adoc

:_mod-docs-content-type: PROCEDURE
[id="nw-ovn-ipsec-verification_{context}"]
= Verifying that IPsec is enabled

As a cluster administrator, you can verify that IPsec is enabled.

.Verification

. To find the names of the OVN-Kubernetes data plane pods, enter the following command:
+
[source,terminal]
----
$ oc get pods -n openshift-ovn-kubernetes -l=app=ovnkube-node
----
+
.Example output
[source,terminal]
----
ovnkube-node-5xqbf                       8/8     Running   0              28m
ovnkube-node-6mwcx                       8/8     Running   0              29m
ovnkube-node-ck5fr                       8/8     Running   0              31m
ovnkube-node-fr4ld                       8/8     Running   0              26m
ovnkube-node-wgs4l                       8/8     Running   0              33m
ovnkube-node-zfvcl                       8/8     Running   0              34m
----

. Verify that IPsec is enabled on your cluster:
+
[source,terminal]
----
$ oc -n openshift-ovn-kubernetes -c nbdb rsh ovnkube-node-<XXXXX> ovn-nbctl --no-leader-only get nb_global . ipsec
----
+
--
where:

`<XXXXX>`:: Specifies the random sequence of letters for a pod from the previous step.
--
+
.Example output
[source,text]
----
true
----

:leveloffset!:

[id="post-install-configuring-network-policy"]
== Configuring network policy

As a cluster administrator or project administrator, you can configure network policies for a project.

:leveloffset: +2

// Module included in the following assemblies:
//
// * networking/network_policy/about-network-policy.adoc
// * post_installation_configuration/network-configuration.adoc

:_mod-docs-content-type: CONCEPT
[id="nw-networkpolicy-about_{context}"]
= About network policy

In a cluster using a network plugin that supports Kubernetes network policy, network isolation is controlled entirely by `NetworkPolicy` objects.
In {product-title} {product-version}, OpenShift SDN supports using network policy in its default network isolation mode.

[WARNING]
====
Network policy does not apply to the host network namespace. Pods with host networking enabled are unaffected by network policy rules. However, pods connecting to the host-networked pods might be affected by the network policy rules.

Network policies cannot block traffic from localhost or from their resident nodes.
====

By default, all pods in a project are accessible from other pods and network endpoints. To isolate one or more pods in a project, you can create `NetworkPolicy` objects in that project to indicate the allowed incoming connections. Project administrators can create and delete `NetworkPolicy` objects within their own project.

If a pod is matched by selectors in one or more `NetworkPolicy` objects, then the pod will accept only connections that are allowed by at least one of those `NetworkPolicy` objects. A pod that is not selected by any `NetworkPolicy` objects is fully accessible.

A network policy applies to only the TCP, UDP, and SCTP protocols. Other protocols are not affected.

The following example `NetworkPolicy` objects demonstrate supporting different scenarios:

* Deny all traffic:
+
To make a project deny by default, add a `NetworkPolicy` object that matches all pods but accepts no traffic:
+
[source,yaml]
----
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: deny-by-default
spec:
  podSelector: {}
  ingress: []
----

* Only allow connections from the {product-title} Ingress Controller:
+
To make a project allow only connections from the {product-title} Ingress Controller, add the following `NetworkPolicy` object.
+
[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-openshift-ingress
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          network.openshift.io/policy-group: ingress
  podSelector: {}
  policyTypes:
  - Ingress
----

* Only accept connections from pods within a project:
+
To make pods accept connections from other pods in the same project, but reject all other connections from pods in other projects, add the following `NetworkPolicy` object:
+
[source,yaml]
----
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-same-namespace
spec:
  podSelector: {}
  ingress:
  - from:
    - podSelector: {}
----

* Only allow HTTP and HTTPS traffic based on pod labels:
+
To enable only HTTP and HTTPS access to the pods with a specific label (`role=frontend` in following example), add a `NetworkPolicy` object similar to the following:
+
[source,yaml]
----
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-http-and-https
spec:
  podSelector:
    matchLabels:
      role: frontend
  ingress:
  - ports:
    - protocol: TCP
      port: 80
    - protocol: TCP
      port: 443
----

* Accept connections by using both namespace and pod selectors:
+
To match network traffic by combining namespace and pod selectors, you can use a `NetworkPolicy` object similar to the following:
+
[source,yaml]
----
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-pod-and-namespace-both
spec:
  podSelector:
    matchLabels:
      name: test-pods
  ingress:
    - from:
      - namespaceSelector:
          matchLabels:
            project: project_name
        podSelector:
          matchLabels:
            name: test-pods
----

`NetworkPolicy` objects are additive, which means you can combine multiple `NetworkPolicy` objects together to satisfy complex network requirements.

For example, for the `NetworkPolicy` objects defined in previous samples, you can define both `allow-same-namespace` and `allow-http-and-https` policies within the same project. Thus allowing the pods with the label `role=frontend`, to accept any connection allowed by each policy. That is, connections on any port from pods in the same namespace, and connections on ports `80` and `443` from pods in any namespace.

[id="nw-networkpolicy-allow-from-router_{context}"]
== Using the allow-from-router network policy

Use the following `NetworkPolicy` to allow external traffic regardless of the router configuration:

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-router
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          policy-group.network.openshift.io/ingress: ""<1>
  podSelector: {}
  policyTypes:
  - Ingress
----
<1> `policy-group.network.openshift.io/ingress:""` label supports both OpenShift-SDN and OVN-Kubernetes.


[id="nw-networkpolicy-allow-from-hostnetwork_{context}"]
== Using the allow-from-hostnetwork network policy

Add the following `allow-from-hostnetwork` `NetworkPolicy` object to direct traffic from the host network pods:

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-hostnetwork
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          policy-group.network.openshift.io/host-network: ""
  podSelector: {}
  policyTypes:
  - Ingress
----

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * networking/network_policy/creating-network-policy.adoc
// * networking/network_policy/viewing-network-policy.adoc
// * networking/network_policy/editing-network-policy.adoc
// * post_installation_configuration/network-configuration.adoc

:_mod-docs-content-type: REFERENCE
[id="nw-networkpolicy-object_{context}"]
= Example NetworkPolicy object

The following annotates an example NetworkPolicy object:

[source,yaml]
----
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-27107 <1>
spec:
  podSelector: <2>
    matchLabels:
      app: mongodb
  ingress:
  - from:
    - podSelector: <3>
        matchLabels:
          app: app
    ports: <4>
    - protocol: TCP
      port: 27017
----
<1> The name of the NetworkPolicy object.
<2> A selector that describes the pods to which the policy applies. The policy object can
only select pods in the project that defines the NetworkPolicy object.
<3> A selector that matches the pods from which the policy object allows ingress traffic. The selector matches pods in the same namespace as the NetworkPolicy.
<4> A list of one or more destination ports on which to accept traffic.

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * networking/multiple_networks/configuring-multi-network-policy.adoc
// * networking/network_policy/creating-network-policy.adoc
// * post_installation_configuration/network-configuration.adoc

:name: network
:role: admin

:_mod-docs-content-type: PROCEDURE
[id="nw-networkpolicy-create-cli_{context}"]
= Creating a {name} policy using the CLI

To define granular rules describing ingress or egress network traffic allowed for namespaces in your cluster, you can create a {name} policy.

[NOTE]
====
If you log in with a user with the `cluster-admin` role, then you can create a network policy in any namespace in the cluster.
====

.Prerequisites

* Your cluster uses a network plugin that supports `NetworkPolicy` objects, such as the OVN-Kubernetes network plugin or the OpenShift SDN network plugin with `mode: NetworkPolicy` set. This mode is the default for OpenShift SDN.
* You installed the OpenShift CLI (`oc`).
* You are logged in to the cluster with a user with `{role}` privileges.
* You are working in the namespace that the {name} policy applies to.

.Procedure

. Create a policy rule:
.. Create a `<policy_name>.yaml` file:
+
[source,terminal]
----
$ touch <policy_name>.yaml
----
+
--
where:

`<policy_name>`:: Specifies the {name} policy file name.
--

.. Define a {name} policy in the file that you just created, such as in the following examples:
+
.Deny ingress from all pods in all namespaces
This is a fundamental policy, blocking all cross-pod networking other than cross-pod traffic allowed by the configuration of other Network Policies.
+
[source,yaml]
----
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: deny-by-default
spec:
  podSelector:
  ingress: []
----
+
+
.Allow ingress from all pods in the same namespace
+
[source,yaml]
----
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-same-namespace
spec:
  podSelector:
  ingress:
  - from:
    - podSelector: {}
----
+
.Allow ingress traffic to one pod from a particular namespace
+
This policy allows traffic to pods labelled `pod-a` from pods running in `namespace-y`.
+
[source,yaml]
----
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-traffic-pod
spec:
  podSelector:
   matchLabels:
      pod: pod-a
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
           kubernetes.io/metadata.name: namespace-y
----
+

. To create the {name} policy object, enter the following command:
+
[source,terminal]
----
$ oc apply -f <policy_name>.yaml -n <namespace>
----
+
--
where:

`<policy_name>`:: Specifies the {name} policy file name.
`<namespace>`:: Optional: Specifies the namespace if the object is defined in a different namespace than the current namespace.
--
+
.Example output
[source,terminal]
----
networkpolicy.networking.k8s.io/deny-by-default created
----

:!name:
:!role:

[NOTE]
====
If you log in to the web console with `cluster-admin` privileges, you have a choice of creating a network policy in any namespace in the cluster directly in YAML or from a form in the web console.
====

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * networking/network_policy/multitenant-network-policy.adoc
// * post_installation_configuration/network-configuration.adoc

:_mod-docs-content-type: PROCEDURE
[id="nw-networkpolicy-multitenant-isolation_{context}"]
= Configuring multitenant isolation by using network policy

You can configure your project to isolate it from pods and services in other
project namespaces.

.Prerequisites

* Your cluster uses a network plugin that supports `NetworkPolicy` objects, such as the OVN-Kubernetes network plugin or the OpenShift SDN network plugin with `mode: NetworkPolicy` set. This mode is the default for OpenShift SDN.
This mode is the default for OpenShift SDN.
* You installed the OpenShift CLI (`oc`).
* You are logged in to the cluster with a user with `admin` privileges.

.Procedure

. Create the following `NetworkPolicy` objects:
.. A policy named `allow-from-openshift-ingress`.
+
[source,terminal]
----
$ cat << EOF| oc create -f -
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-openshift-ingress
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          policy-group.network.openshift.io/ingress: ""
  podSelector: {}
  policyTypes:
  - Ingress
EOF
----
+
[NOTE]
====
`policy-group.network.openshift.io/ingress: ""` is the preferred namespace selector label for OpenShift SDN. You can use the `network.openshift.io/policy-group: ingress` namespace selector label, but this is a legacy label.
====
.. A policy named `allow-from-openshift-monitoring`:
+
[source,terminal]
----
$ cat << EOF| oc create -f -
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-openshift-monitoring
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          network.openshift.io/policy-group: monitoring
  podSelector: {}
  policyTypes:
  - Ingress
EOF
----

.. A policy named `allow-same-namespace`:
+
[source,terminal]
----
$ cat << EOF| oc create -f -
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-same-namespace
spec:
  podSelector:
  ingress:
  - from:
    - podSelector: {}
EOF
----

.. A policy named `allow-from-kube-apiserver-operator`:
+
[source,terminal]
----
$ cat << EOF| oc create -f -
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-kube-apiserver-operator
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: openshift-kube-apiserver-operator
      podSelector:
        matchLabels:
          app: kube-apiserver-operator
  policyTypes:
  - Ingress
EOF
----
+
For more details, see link:https://access.redhat.com/solutions/6964520[New `kube-apiserver-operator` webhook controller validating health of webhook].

. Optional: To confirm that the network policies exist in your current project, enter the following command:
+
[source,terminal]
----
$ oc describe networkpolicy
----
+
.Example output
[source,text]
----
Name:         allow-from-openshift-ingress
Namespace:    example1
Created on:   2020-06-09 00:28:17 -0400 EDT
Labels:       <none>
Annotations:  <none>
Spec:
  PodSelector:     <none> (Allowing the specific traffic to all pods in this namespace)
  Allowing ingress traffic:
    To Port: <any> (traffic allowed to all ports)
    From:
      NamespaceSelector: network.openshift.io/policy-group: ingress
  Not affecting egress traffic
  Policy Types: Ingress


Name:         allow-from-openshift-monitoring
Namespace:    example1
Created on:   2020-06-09 00:29:57 -0400 EDT
Labels:       <none>
Annotations:  <none>
Spec:
  PodSelector:     <none> (Allowing the specific traffic to all pods in this namespace)
  Allowing ingress traffic:
    To Port: <any> (traffic allowed to all ports)
    From:
      NamespaceSelector: network.openshift.io/policy-group: monitoring
  Not affecting egress traffic
  Policy Types: Ingress
----

:leveloffset!:


:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v2x/preparing-ossm-install.adoc
// * service_mesh/v2x/servicemesh-release-notes.adoc
// * post_installation_configuration/network-configuration.adoc

:_mod-docs-content-type: REFERENCE
[id="ossm-supported-configurations_{context}"]
= Supported configurations

The following configurations are supported for the current release of {SMProductName}.

[id="ossm-supported-platforms_{context}"]
== Supported platforms

The {SMProductName} Operator supports multiple versions of the `ServiceMeshControlPlane` resource. Version {MaistraVersion} {SMProductShortName} control planes are supported on the following platform versions:

* Red Hat {product-title} version 4.10 or later.
* {product-dedicated} version 4.
* Azure Red Hat OpenShift (ARO) version 4.
* Red Hat OpenShift Service on AWS (ROSA).

[id="ossm-unsupported-configurations_{context}"]
== Unsupported configurations

Explicitly unsupported cases include:

* OpenShift Online is not supported for {SMProductName}.
* {SMProductName} does not support the management of microservices outside the cluster where {SMProductShortName} is running.

[id="ossm-supported-configurations-networks_{context}"]
== Supported network configurations

{SMProductName} supports the following network configurations.

* OpenShift-SDN
* OVN-Kubernetes is available on all supported versions of {product-title}.
* Third-Party Container Network Interface (CNI) plugins that have been certified on {product-title} and passed {SMProductShortName} conformance testing. See link:https://access.redhat.com/articles/5436171[Certified OpenShift CNI Plug-ins] for more information.

[id="ossm-supported-configurations-sm_{context}"]
== Supported configurations for {SMProductShortName}

* This release of {SMProductName} is only available on {product-title} x86_64, {ibmzProductName}, and {ibmpowerProductName}.
** {ibmzProductName} is only supported on {product-title} 4.10 and later.
** {ibmpowerProductName} is only supported on {product-title} 4.10 and later.
* Configurations where all {SMProductShortName} components are contained within a single {product-title} cluster.
* Configurations that do not integrate external services such as virtual machines.
* {SMProductName} does not support `EnvoyFilter` configuration except where explicitly documented.

[id="ossm-supported-configurations-kiali_{context}"]
== Supported configurations for Kiali

* The Kiali console is only supported on the two most recent releases of the Google Chrome, Microsoft Edge, Mozilla Firefox, or Apple Safari browsers.
* The `openshift` authentication strategy is the only supported authentication configuration when Kiali is deployed with {SMProductName} (OSSM). The `openshift` strategy controls access based on the individual's role-based access control (RBAC) roles of the {product-title}.

[id="ossm-supported-configurations-jaeger_{context}"]
== Supported configurations for Distributed Tracing

* Jaeger agent as a sidecar is the only supported configuration for Jaeger. Jaeger as a daemonset is not supported for multitenant installations or OpenShift Dedicated.

[id="ossm-supported-configurations-webassembly_{context}"]
== Supported WebAssembly module

* 3scale WebAssembly is the only provided WebAssembly module. You can create custom WebAssembly modules.

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v1x/preparing-ossm-installation.adoc
// * service_mesh/v2x/preparing-ossm-installation.adoc
// * post_installation_configuration/network-configuration.adoc

:_mod-docs-content-type: CONCEPT
[id="ossm-installation-activities_{context}"]
= Operator overview

{SMProductName} requires the following four Operators:

* *OpenShift Elasticsearch* - (Optional) Provides database storage for tracing and logging with the {JaegerShortName}. It is based on the open source link:https://www.elastic.co/[Elasticsearch] project.
* *{JaegerName}* - Provides distributed tracing to monitor and troubleshoot transactions in complex distributed systems. It is based on the open source link:https://www.jaegertracing.io/[Jaeger] project.
* *Kiali Operator provided by Red Hat* - Provides observability for your service mesh. You can view configurations, monitor traffic, and analyze traces in a single console. It is based on the open source link:https://www.kiali.io/[Kiali] project.
* *{SMProductName}* - Allows you to connect, secure, control, and observe the microservices that comprise your applications. The {SMProductShortName} Operator defines and monitors the `ServiceMeshControlPlane` resources that manage the deployment, updating, and deletion of the {SMProductShortName} components. It is based on the open source link:https://istio.io/[Istio] project.

:leveloffset!:

.Next steps

* xref:../service_mesh/v2x/installing-ossm.adoc#installing-ossm[Install {SMProductName}] in your {product-title} environment.

[id="post-installationrouting-optimization"]
== Optimizing routing

The {product-title} HAProxy router can be scaled or configured to optimize performance.

:leveloffset: +2

// Module included in the following assemblies:
// * scalability_and_performance/optimization/routing-optimization.adoc
// * post_installation_configuration/network-configuration.adoc

[id="baseline-router-performance_{context}"]
= Baseline Ingress Controller (router) performance

The {product-title} Ingress Controller, or router, is the ingress point for ingress traffic for applications and services that are configured using routes and ingresses.

When evaluating a single HAProxy router performance in terms of HTTP requests handled per second, the performance varies depending on many factors. In particular:

* HTTP keep-alive/close mode

* Route type

* TLS session resumption client support

* Number of concurrent connections per target route

* Number of target routes

* Back end server page size

* Underlying infrastructure (network/SDN solution, CPU, and so on)

While performance in your specific environment will vary, Red Hat lab tests on a public cloud instance of size 4 vCPU/16GB RAM. A single HAProxy router handling 100 routes terminated by backends serving 1kB static pages is able to handle the following number of transactions per second.

In HTTP keep-alive mode scenarios:

[cols="3",options="header"]
|===
|*Encryption* |*LoadBalancerService*|*HostNetwork*
|none |21515|29622
|edge |16743|22913
|passthrough |36786|53295
|re-encrypt |21583|25198
|===

In HTTP close (no keep-alive) scenarios:

[cols="3",options="header"]
|===
|*Encryption* |*LoadBalancerService*|*HostNetwork*
|none |5719|8273
|edge |2729|4069
|passthrough |4121|5344
|re-encrypt |2320|2941
|===

The default Ingress Controller configuration was used with the `spec.tuningOptions.threadCount` field set to `4`. Two different endpoint publishing strategies were tested: Load Balancer Service and Host Network. TLS session resumption was used for encrypted routes. With HTTP keep-alive, a single HAProxy router is capable of saturating a 1 Gbit NIC at page sizes as small as 8 kB.

When running on bare metal with modern processors, you can expect roughly twice the performance of the public cloud instance above. This overhead is introduced by the virtualization layer in place on public clouds and holds mostly true for private cloud-based virtualization as well. The following table is a guide to how many applications to use behind the router:

[cols="2,4",options="header"]
|===
|*Number of applications* |*Application type*
|5-10 |static file/web server or caching proxy
|100-1000 |applications generating dynamic content

|===

In general, HAProxy can support routes for up to 1000 applications, depending on the technology in use. Ingress Controller performance might be limited by the
capabilities and performance of the applications behind it, such as language or static versus dynamic content.

Ingress, or router, sharding should be used to serve more routes towards applications and help horizontally scale the routing tier.

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
// * scalability_and_performance/optimization/routing-optimization.adoc
// * post_installation_configuration/network-configuration.adoc

:_mod-docs-content-type: REFERENCE
[id="ingress-liveness-readiness-startup-probes_{context}"]
= Configuring Ingress Controller liveness, readiness, and startup probes

Cluster administrators can configure the timeout values for the kubelet's liveness, readiness, and startup probes for router deployments that are managed by the {product-title} Ingress Controller (router). The liveness and readiness probes of the router use the default timeout value
of 1 second, which is too brief when networking or runtime performance is severely degraded. Probe timeouts can cause unwanted router restarts that interrupt application connections. The ability to set larger timeout values can reduce the risk of unnecessary and unwanted restarts.

You can update the `timeoutSeconds` value on the `livenessProbe`, `readinessProbe`, and `startupProbe` parameters of the router container.

[cols="3a,8a",options="header"]
|===
 |Parameter |Description

 |`livenessProbe`
 |The `livenessProbe` reports to the kubelet whether a pod is dead and needs to be restarted.

 |`readinessProbe`
 |The `readinessProbe` reports whether a pod is healthy or unhealthy. When the readiness probe reports an unhealthy pod, then the kubelet marks the pod as not ready to accept traffic. Subsequently, the endpoints for that pod are marked as not ready, and this status propagates to the kube-proxy. On cloud platforms with a configured load balancer, the kube-proxy communicates to the cloud load-balancer not to send traffic to the node with that pod.

 |`startupProbe`
 |The `startupProbe` gives the router pod up to 2 minutes to initialize before the kubelet begins sending the router liveness and readiness probes.  This initialization time can prevent routers with many routes or endpoints from prematurely restarting.
|===


[IMPORTANT]
====
The timeout configuration option is an advanced tuning technique that can be used to work around issues. However, these issues should eventually be diagnosed and possibly a support case or https://issues.redhat.com/secure/CreateIssueDetails!init.jspa?pid=12332330&summary=Summary&issuetype=1&priority=10200&versions=12385624[Jira issue] opened for any issues that causes probes to time out.
====

The following example demonstrates how you can directly patch the default router deployment to set a 5-second timeout for the liveness and readiness probes:


[source,terminal]
----
$ oc -n openshift-ingress patch deploy/router-default --type=strategic --patch='{"spec":{"template":{"spec":{"containers":[{"name":"router","livenessProbe":{"timeoutSeconds":5},"readinessProbe":{"timeoutSeconds":5}}]}}}}'
----

.Verification
[source,terminal]
----
$ oc -n openshift-ingress describe deploy/router-default | grep -e Liveness: -e Readiness:
    Liveness:   http-get http://:1936/healthz delay=0s timeout=5s period=10s #success=1 #failure=3
    Readiness:  http-get http://:1936/healthz/ready delay=0s timeout=5s period=10s #success=1 #failure=3
----

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
// * scalability_and_performance/optimization/routing-optimization.adoc
// * post_installation_configuration/network-configuration.adoc

:_mod-docs-content-type: PROCEDURE
[id="configuring-haproxy-interval_{context}"]
= Configuring HAProxy reload interval

When you update a route or an endpoint associated with a route, {product-title} router updates the configuration for HAProxy. Then, HAProxy reloads the updated configuration for those changes to take effect. When HAProxy reloads, it generates a new process that handles new connections using the updated configuration.

HAProxy keeps the old process running to handle existing connections until those connections are all closed. When old processes have long-lived connections, these processes can accumulate and consume resources.

The default minimum HAProxy reload interval is five seconds. You can configure an Ingress Controller using its `spec.tuningOptions.reloadInterval` field to set a longer minimum reload interval.

[WARNING]
====
Setting a large value for the minimum HAProxy reload interval can cause latency in observing updates to routes and their endpoints. To lessen the risk, avoid setting a value larger than the tolerable latency for updates.
====

.Procedure

* Change the minimum HAProxy reload interval of the default Ingress Controller to 15 seconds by running the following command:
+
[source,terminal]
----
$ oc -n openshift-ingress-operator patch ingresscontrollers/default --type=merge --patch='{"spec":{"tuningOptions":{"reloadInterval":"15s"}}}'
----

:leveloffset!:

[id="post-installation-osp-fips"]
== Postinstallation {rh-openstack} network configuration

You can configure some aspects of an {product-title} on {rh-openstack-first} cluster after installation.

:leveloffset: +2

// Module included in the following assemblies:
//
// * post_installation_configuration/network-configuration.adoc

:_mod-docs-content-type: PROCEDURE
[id="installation-osp-configuring-api-floating-ip_{context}"]
= Configuring application access with floating IP addresses

After you install {product-title}, configure {rh-openstack-first} to allow application network traffic.

[NOTE]
====
You do not need to perform this procedure if you provided values for `platform.openstack.apiFloatingIP` and `platform.openstack.ingressFloatingIP` in the `install-config.yaml` file, or `os_api_fip` and `os_ingress_fip` in the `inventory.yaml` playbook, during installation. The floating IP addresses are already set.
====

.Prerequisites

* {product-title} cluster must be installed
* Floating IP addresses are enabled as described in the {product-title} on {rh-openstack} installation documentation.

.Procedure

After you install the {product-title} cluster, attach a floating IP address to the ingress port:

. Show the port:
+
[source,terminal]
----
$ openstack port show <cluster_name>-<cluster_ID>-ingress-port
----

. Attach the port to the IP address:
+
[source,terminal]
----
$ openstack floating ip set --port <ingress_port_ID> <apps_FIP>
----

. Add a wildcard `A` record for `*apps.` to your DNS file:
+
[source,dns]
----
*.apps.<cluster_name>.<base_domain>  IN  A  <apps_FIP>
----

[NOTE]
====
If you do not control the DNS server but want to enable application access for non-production purposes, you can add these hostnames to `/etc/hosts`:

[source,dns]
----
<apps_FIP> console-openshift-console.apps.<cluster name>.<base domain>
<apps_FIP> integrated-oauth-server-openshift-authentication.apps.<cluster name>.<base domain>
<apps_FIP> oauth-openshift.apps.<cluster name>.<base domain>
<apps_FIP> prometheus-k8s-openshift-monitoring.apps.<cluster name>.<base domain>
<apps_FIP> <app name>.apps.<cluster name>.<base domain>
----
====

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * installing/installing_openstack/installing-openstack-installer-kuryr.adoc
// * installing/installing_openstack/installing-openstack-user-kuryr.adoc
// * post_installation_configuration/network-configuration.adoc

[id="installation-osp-kuryr-port-pools_{context}"]
= Kuryr ports pools

A Kuryr ports pool maintains a number of ports on standby for pod creation.

Keeping ports on standby minimizes pod creation time. Without ports pools, Kuryr must explicitly request port creation or deletion whenever a pod is created or deleted.

The Neutron ports that Kuryr uses are created in subnets that are tied to namespaces. These pod ports are also added as subports to the primary port of {product-title} cluster nodes.

Because Kuryr keeps each namespace in a separate subnet, a separate ports pool is maintained for each namespace-worker pair.

Prior to installing a cluster, you can set the following parameters in the `cluster-network-03-config.yml` manifest file to configure ports pool behavior:

* The `enablePortPoolsPrepopulation` parameter controls pool prepopulation, which forces Kuryr to add Neutron ports to the pools when the first pod that is configured to use the dedicated network for pods is created in a namespace. The default value is `false`.
* The `poolMinPorts` parameter is the minimum number of free ports that are kept in the pool. The default value is `1`.
* The `poolMaxPorts` parameter is the maximum number of free ports that are kept in the pool. A value of `0` disables that upper bound. This is the default setting.
+
If your OpenStack port quota is low, or you have a limited number of IP addresses on the pod network, consider setting this option to ensure that unneeded ports are deleted.
* The `poolBatchPorts` parameter defines the maximum number of Neutron ports that can be created at once. The default value is `3`.

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * post_installation_configuration/network-configuration.adoc

:_mod-docs-content-type: PROCEDURE
[id="installation-osp-kuryr-settings-active_{context}"]
= Adjusting Kuryr ports pool settings in active deployments on {rh-openstack}

You can use a custom resource (CR) to configure how Kuryr manages {rh-openstack-first} Neutron ports to control the speed and efficiency of pod creation on a deployed cluster.

.Procedure

. From a command line, open the Cluster Network Operator (CNO) CR for editing:
+
[source,terminal]
----
$ oc edit networks.operator.openshift.io cluster
----

. Edit the settings to meet your requirements. The following file is provided as an example:
+
[source,yaml]
----
apiVersion: operator.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  clusterNetwork:
  - cidr: 10.128.0.0/14
    hostPrefix: 23
  serviceNetwork:
  - 172.30.0.0/16
  defaultNetwork:
    type: Kuryr
    kuryrConfig:
      enablePortPoolsPrepopulation: false <1>
      poolMinPorts: 1 <2>
      poolBatchPorts: 3 <3>
      poolMaxPorts: 5 <4>
----
<1> Set `enablePortPoolsPrepopulation` to `true` to make Kuryr create Neutron ports when the first pod that is configured to use the dedicated network for pods is created in a namespace. This setting raises the Neutron ports quota but can reduce the time that is required to spawn pods. The default value is `false`.
<2> Kuryr creates new ports for a pool if the number of free ports in that pool is lower than the value of `poolMinPorts`. The default value is `1`.
<3> `poolBatchPorts` controls the number of new ports that are created if the number of free ports is lower than the value of `poolMinPorts`. The default value is `3`.
<4> If the number of free ports in a pool is higher than the value of `poolMaxPorts`, Kuryr deletes them until the number matches that value. Setting the value to `0` disables this upper bound, preventing pools from shrinking. The default value is `0`.

. Save your changes and quit the text editor to commit your changes.

[IMPORTANT]
====
Modifying these options on a running cluster forces the kuryr-controller and kuryr-cni pods to restart. As a result, the creation of new pods and services will be delayed.
====

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * post_installation_configuration/network-configuration.adoc

:_mod-docs-content-type: PROCEDURE
[id="nw-osp-enabling-ovs-offload_{context}"]
= Enabling OVS hardware offloading

For clusters that run on {rh-openstack-first}, you can enable link:https://www.openvswitch.org/[Open vSwitch (OVS)] hardware offloading.

OVS is a multi-layer virtual switch that enables large-scale, multi-server network virtualization.

.Prerequisites

* You installed a cluster on {rh-openstack} that is configured for single-root input/output virtualization (SR-IOV).
* You installed the SR-IOV Network Operator on your cluster.
* You created two `hw-offload` type virtual function (VF) interfaces on your cluster.

.Procedure

. Create an `SriovNetworkNodePolicy` policy for the two `hw-offload` type VF interfaces that are on your cluster:
+
.The first virtual function interface
[source,yaml]
----
apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy <1>
metadata:
  name: "hwoffload9"
  namespace: openshift-sriov-network-operator
spec:
  deviceType: netdevice
  isRdma: true
  nicSelector:
    pfNames: <2>
    - ens6
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: 'true'
  numVfs: 1
  priority: 99
  resourceName: "hwoffload9"
----
<1> Insert the `SriovNetworkNodePolicy` value here.
<2> Both interfaces must include physical function (PF) names.
+
.The second virtual function interface
[source,yaml]
----
apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy <1>
metadata:
  name: "hwoffload10"
  namespace: openshift-sriov-network-operator
spec:
  deviceType: netdevice
  isRdma: true
  nicSelector:
    pfNames: <2>
    - ens5
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: 'true'
  numVfs: 1
  priority: 99
  resourceName: "hwoffload10"
----
<1> Insert the `SriovNetworkNodePolicy` value here.
<2> Both interfaces must include physical function (PF) names.

. Create `NetworkAttachmentDefinition` resources for the two interfaces:
+
.A `NetworkAttachmentDefinition` resource for the first interface
[source,yaml]
----
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  annotations:
    k8s.v1.cni.cncf.io/resourceName: openshift.io/hwoffload9
  name: hwoffload9
  namespace: default
spec:
    config: '{ "cniVersion":"0.3.1", "name":"hwoffload9","type":"host-device","device":"ens6"
    }'
----
+
.A `NetworkAttachmentDefinition` resource for the second interface
[source,yaml]
----
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  annotations:
    k8s.v1.cni.cncf.io/resourceName: openshift.io/hwoffload10
  name: hwoffload10
  namespace: default
spec:
    config: '{ "cniVersion":"0.3.1", "name":"hwoffload10","type":"host-device","device":"ens5"
    }'
----

. Use the interfaces that you created with a pod. For example:
+
.A pod that uses the two OVS offload interfaces
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: dpdk-testpmd
  namespace: default
  annotations:
    irq-load-balancing.crio.io: disable
    cpu-quota.crio.io: disable
    k8s.v1.cni.cncf.io/resourceName: openshift.io/hwoffload9
    k8s.v1.cni.cncf.io/resourceName: openshift.io/hwoffload10
spec:
  restartPolicy: Never
  containers:
  - name: dpdk-testpmd
    image: quay.io/krister/centos8_nfv-container-dpdk-testpmd:latest
----

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * post_installation_configuration/network-configuration.adoc

:_mod-docs-content-type: PROCEDURE
[id="nw-osp-hardware-offload-attaching-network_{context}"]
= Attaching an OVS hardware offloading network

You can attach an Open vSwitch (OVS) hardware offloading network to your cluster.

.Prerequisites

* Your cluster is installed and running.
* You provisioned an OVS hardware offloading network on {rh-openstack-first} to use with your cluster.

.Procedure

. Create a file named `network.yaml` from the following template:
+
[source,yaml]
----
spec:
  additionalNetworks:
  - name: hwoffload1
    namespace: cnf
    rawCNIConfig: '{ "cniVersion": "0.3.1", "name": "hwoffload1", "type": "host-device","pciBusId": "0000:00:05.0", "ipam": {}}' <1>
    type: Raw
----
+
where:
+
`pciBusId`:: Specifies the device that is connected to the offloading network. If you do not have it, you can find this value by running the following command:
+
[source,terminal]
----
$ oc describe SriovNetworkNodeState -n openshift-sriov-network-operator
----

. From a command line, enter the following command to patch your cluster with the file:
+
[source,terminal]
----
$ oc apply -f network.yaml
----

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * post_installation_configuration/network-configuration.adoc

:_mod-docs-content-type: PROCEDURE
[id="nw-osp-pod-connections-ipv6_{context}"]
= Enabling IPv6 connectivity to pods on {rh-openstack}

To enable IPv6 connectivity between pods that have additional networks that are on different nodes, disable port security for the IPv6 port of the server. Disabling port security obviates the need to create allowed address pairs for each IPv6 address that is assigned to pods and enables traffic on the security group.

[IMPORTANT]
====
Only the following IPv6 additional network configurations are supported:

* SLAAC and host-device
* SLAAC and MACVLAN
* DHCP stateless and host-device
* DHCP stateless and MACVLAN
====

.Procedure

* On a command line, enter the following command:
+
[source,terminal]
----
$ openstack port set --no-security-group --disable-port-security <compute_ipv6_port>
----
+
IMPORTANT: This command removes security groups from the port and disables port security. Traffic restrictions are removed entirely from the port.

where:

<compute_ipv6_port>:: Specifies the IPv6 port of the compute server.

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * post_installation_configuration/network-configuration.adoc

:_mod-docs-content-type: PROCEDURE
[id="nw-osp-pod-adding-connections-ipv6_{context}"]
= Adding IPv6 connectivity to pods on {rh-openstack}

After you enable IPv6 connectivity in pods, add connectivity to them by using a Container Network Interface (CNI) configuration.

.Procedure

. To edit the Cluster Network Operator (CNO), enter the following command:
+
[source,terminal]
----
$ oc edit networks.operator.openshift.io cluster
----

. Specify your CNI configuration under the `spec` field. For example, the following configuration uses a SLAAC address mode with MACVLAN:
+
[source,yaml]
----
...
spec:
  additionalNetworks:
  - name: ipv6
    namespace: ipv6 <1>
    rawCNIConfig: '{ "cniVersion": "0.3.1", "name": "ipv6", "type": "macvlan", "master": "ens4"}' <2>
    type: Raw
----
<1> Be sure to create pods in the same namespace.
<2> The interface in the network attachment `"master"` field can differ from `"ens4"` when more networks are configured or when a different kernel driver is used.
+
[NOTE]
====
If you are using stateful address mode, include the IP Address Management (IPAM) in the CNI configuration.

DHCPv6 is not supported by Multus.
====

. Save your changes and quit the text editor to commit your changes.

.Verification

* On a command line, enter the following command:
+
[source,terminal]
----
$ oc get network-attachment-definitions -A
----
+
.Example output
[source,terminal]
----
NAMESPACE       NAME            AGE
ipv6            ipv6            21h
----

You can now create pods that have secondary IPv6 connections.

:leveloffset!:

[role="_additional-resources"]
.Additional resources

* xref:../networking/multiple_networks/configuring-additional-network.adoc#configuring-additional-network_configuration-additional-network-attachment[Configuration for an additional network attachment]

:leveloffset: +2

// Module included in the following assemblies:
//
// * post_installation_configuration/network-configuration.adoc

:_mod-docs-content-type: PROCEDURE
[id="nw-osp-pod-creating-ipv6_{context}"]
= Create pods that have IPv6 connectivity on {rh-openstack}

After you enable IPv6 connectivty for pods and add it to them, create pods that have secondary IPv6 connections.

.Procedure

. Define pods that use your IPv6 namespace and the annotation `k8s.v1.cni.cncf.io/networks: <additional_network_name>`, where `<additional_network_name` is the name of the additional network. For example, as part of a `Deployment` object:
+
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hello-openshift
  namespace: ipv6
spec:
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
         - labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - hello-openshift
  replicas: 2
  selector:
    matchLabels:
      app: hello-openshift
  template:
    metadata:
      labels:
        app: hello-openshift
      annotations:
        k8s.v1.cni.cncf.io/networks: ipv6
    spec:
      securityContext:
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      containers:
      - name: hello-openshift
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
        image: quay.io/openshift/origin-hello-openshift
        ports:
        - containerPort: 8080
----

. Create the pod. For example, on a command line, enter the following command:
+
[source,terminal]
----
$ oc create -f <ipv6_enabled_resource>
----

where:

<ipv6_enabled_resource>:: Specifies the file that contains your resource definition.

:leveloffset!:

//# includes=_attributes/common-attributes,modules/nw-operator-cr,modules/nw-proxy-configure-object,modules/private-clusters-setting-dns-private,modules/nw-nodeport-service-range-edit,modules/nw-ovn-ipsec-enable,modules/nw-ovn-ipsec-verification,modules/nw-networkpolicy-about,modules/nw-networkpolicy-object,modules/nw-networkpolicy-create-cli,modules/nw-networkpolicy-multitenant-isolation,modules/ossm-supported-configurations,modules/ossm-installation-activities,modules/baseline-router-performance,modules/ingress-liveness-readiness-startup-probes,modules/configuring-haproxy-interval,modules/installation-osp-configuring-api-floating-ip,modules/installation-osp-kuryr-port-pools,modules/installation-osp-kuryr-settings-active,modules/nw-osp-enabling-ovs-offload,modules/nw-osp-hardware-offload-attaching-network,modules/nw-osp-pod-connections-ipv6,modules/nw-osp-pod-adding-connections-ipv6,modules/nw-osp-pod-creating-ipv6

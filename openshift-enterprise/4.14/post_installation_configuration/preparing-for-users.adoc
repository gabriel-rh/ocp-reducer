:_mod-docs-content-type: ASSEMBLY
:context: post-install-preparing-for-users
[id="post-install-preparing-for-users"]
= Preparing for users
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS

toc::[]

After installing {product-title}, you can further expand and customize your
cluster to your requirements, including taking steps to prepare for users.

[id="post-install-understanding-identity-provider"]
== Understanding identity provider configuration

The {product-title} control plane includes a built-in OAuth server. Developers and
administrators obtain OAuth access tokens to authenticate themselves to the API.

As an administrator, you can configure OAuth to specify an identity provider
after you install your cluster.

:leveloffset: +2

// Module included in the following assemblies:
//
// * authentication/configuring-identity-provider.adoc
// * authentication/identity_providers/configuring-allow-all-identity-provider.adoc
// * authentication/identity_providers/configuring-deny-all-identity-provider.adoc
// * authentication/identity_providers/configuring-htpasswd-identity-provider.adoc
// * authentication/identity_providers/configuring-keystone-identity-provider.adoc
// * authentication/identity_providers/configuring-ldap-identity-provider.adoc
// * authentication/identity_providers/configuring-basic-authentication-identity-provider.adoc
// * authentication/identity_providers/configuring-request-header-identity-provider.adoc
// * authentication/identity_providers/configuring-github-identity-provider.adoc
// * authentication/identity_providers/configuring-gitlab-identity-provider.adoc
// * authentication/identity_providers/configuring-google-identity-provider.adoc
// * authentication/identity_providers/configuring-oidc-identity-provider.adoc
// * post_installation_configuration/preparing-for-users.adoc

:_mod-docs-content-type: CONCEPT
[id="identity-provider-overview_{context}"]
= About identity providers in {product-title}

By default, only a `kubeadmin` user exists on your cluster. To specify an
identity provider, you must create a custom resource (CR) that describes
that identity provider and add it to the cluster.

[NOTE]
====
{product-title} user names containing `/`, `:`, and `%` are not supported.
====

:leveloffset!:

[id="post-install-supported-identity-providers"]
=== Supported identity providers
// This section is sourced from authentication/understanding-identity-provider.adoc
You can configure the following types of identity providers:

[cols="2a,8a",options="header"]
|===

|Identity provider
|Description

|xref:../authentication/identity_providers/configuring-htpasswd-identity-provider.adoc#configuring-htpasswd-identity-provider[htpasswd]
|Configure the `htpasswd` identity provider to validate user names and passwords
against a flat file generated using
link:http://httpd.apache.org/docs/2.4/programs/htpasswd.html[`htpasswd`].

|xref:../authentication/identity_providers/configuring-keystone-identity-provider.adoc#configuring-keystone-identity-provider[Keystone]
|Configure the `keystone` identity provider to integrate
your {product-title} cluster with Keystone to enable shared authentication with
an OpenStack Keystone v3 server configured to store users in an internal
database.

|xref:../authentication/identity_providers/configuring-ldap-identity-provider.adoc#configuring-ldap-identity-provider[LDAP]
|Configure the `ldap` identity provider to validate user names and passwords
against an LDAPv3 server, using simple bind authentication.

|xref:../authentication/identity_providers/configuring-basic-authentication-identity-provider.adoc#configuring-basic-authentication-identity-provider[Basic authentication]
|Configure a `basic-authentication` identity provider for users to log in to
{product-title} with credentials validated against a remote identity provider.
Basic authentication is a generic backend integration mechanism.

|xref:../authentication/identity_providers/configuring-request-header-identity-provider.adoc#configuring-request-header-identity-provider[Request header]
|Configure a `request-header` identity provider to identify users from request
header values, such as `X-Remote-User`. It is typically used in combination with
an authenticating proxy, which sets the request header value.

|xref:../authentication/identity_providers/configuring-github-identity-provider.adoc#configuring-github-identity-provider[GitHub or GitHub Enterprise]
|Configure a `github` identity provider to validate user names and passwords
against GitHub or GitHub Enterprise's OAuth authentication server.

|xref:../authentication/identity_providers/configuring-gitlab-identity-provider.adoc#configuring-gitlab-identity-provider[GitLab]
|Configure a `gitlab` identity provider to use
link:https://gitlab.com/[GitLab.com] or any other GitLab instance as an identity
provider.

|xref:../authentication/identity_providers/configuring-google-identity-provider.adoc#configuring-google-identity-provider[Google]
|Configure a `google` identity provider using
link:https://developers.google.com/identity/protocols/OpenIDConnect[Google's OpenID Connect integration].

|xref:../authentication/identity_providers/configuring-oidc-identity-provider.adoc#configuring-oidc-identity-provider[OpenID Connect]
|Configure an `oidc` identity provider to integrate with an OpenID Connect
identity provider using an
link:http://openid.net/specs/openid-connect-core-1_0.html#CodeFlowAuth[Authorization Code Flow].

|===

After you define an identity provider, you can
xref:../authentication/using-rbac.adoc#authorization-overview_using-rbac[use
RBAC to define and apply permissions].

:leveloffset: +2

// Module included in the following assemblies:
//
// * authentication/understanding-identity-provider.adoc
// * post_installation_configuration/preparing-for-users.adoc

[id="identity-provider-parameters_{context}"]
= Identity provider parameters

The following parameters are common to all identity providers:

[cols="2a,8a",options="header"]
|===
|Parameter     | Description
|`name`      | The provider name is prefixed to provider user names to form an
identity name.

|`mappingMethod`  | Defines how new identities are mapped to users when they log in.
Enter one of the following values:

claim:: The default value. Provisions a user with the identity's preferred
user name. Fails if a user with that user name is already mapped to another
identity.
lookup:: Looks up an existing identity, user identity mapping, and user,
but does not automatically provision users or identities. This allows cluster
administrators to set up identities and users manually, or using an external
process. Using this method requires you to manually provision users.
generate:: Provisions a user with the identity's preferred user name. If a
user with the preferred user name is already mapped to an existing identity, a
unique user name is generated. For example, `myuser2`. This method should not be
used in combination with external processes that require exact matches between
{product-title} user names and identity provider user names, such as LDAP group
sync.
add:: Provisions a user with the identity's preferred user name. If a user
with that user name already exists, the identity is mapped to the existing user,
adding to any existing identity mappings for the user. Required when multiple
identity providers are configured that identify the same set of users and map to
the same user names.
|===

[NOTE]
When adding or changing identity providers, you can map identities from the new
provider to existing users by setting the `mappingMethod` parameter to
`add`.

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * authentication/understanding-identity-provider.adoc
// * post_installation_configuration/preparing-for-users.adoc

[id="identity-provider-default-CR_{context}"]
= Sample identity provider CR

The following custom resource (CR) shows the parameters and default
values that you use to configure an identity provider. This example
uses the htpasswd identity provider.

.Sample identity provider CR

[source,yaml]
----
apiVersion: config.openshift.io/v1
kind: OAuth
metadata:
  name: cluster
spec:
  identityProviders:
  - name: my_identity_provider <1>
    mappingMethod: claim <2>
    type: HTPasswd
    htpasswd:
      fileData:
        name: htpass-secret <3>
----
<1> This provider name is prefixed to provider user names to form an
identity name.
<2> Controls how mappings are established between this provider's
identities and `User` objects.
<3> An existing secret containing a file generated using
link:http://httpd.apache.org/docs/2.4/programs/htpasswd.html[`htpasswd`].

:leveloffset!:

[id="post-install-using-rbac-to-define-and-apply-permissions"]
== Using RBAC to define and apply permissions
Understand and apply role-based access control.

:leveloffset: +2

// Module included in the following assemblies:
//
// * authentication/using-rbac.adoc
// * post_installation_configuration/preparing-for-users.adoc

:_mod-docs-content-type: CONCEPT
[id="authorization-overview_{context}"]
= RBAC overview

Role-based access control (RBAC) objects determine whether a user is allowed to
perform a given action within a project.

administrators can use the cluster roles and
bindings to control who has various access levels to the {product-title}
platform itself and all projects.

Developers can use local roles and bindings to control who has access
to their projects. Note that authorization is a separate step from
authentication, which is more about determining the identity of who is taking the action.

Authorization is managed using:

[cols="1,4",options="header"]
|===

|Authorization object |Description

|Rules |Sets of permitted verbs on a set of objects. For example,
whether a user or service account can `create` pods.

|Roles |Collections of rules. You can associate, or bind, users and groups
to multiple roles.

|Bindings |Associations between users and/or groups with a role.
|===


Be mindful of the difference between local and cluster bindings. For example,
if you bind the `cluster-admin` role to a user by using a local role binding,
it might appear that this user has the privileges of a cluster administrator.
This is not the case. Binding the `cluster-admin` to a user in a project
grants super administrator privileges for only that
project to the user. That user has the permissions of the cluster role
`admin`, plus a few additional permissions like the ability to edit rate limits,
for that project.
This binding can be confusing via the web console UI, which does not list
cluster role bindings that are bound to true cluster administrators. However, it
does list local role bindings that you can use to locally bind `cluster-admin`.

////
If you do, when you upgrade
your cluster, the default roles are updated and
automatically reconciled when the server is started. During reconciliation, any
permissions that are missing from
the default roles are added. If you added more permissions to the role, they are
not removed.

If you customized the default roles and configured them to prevent automatic
role reconciliation, you must manually update policy definitions
when you upgrade {product-title}.
////


The relationships between cluster roles, local roles, cluster role bindings,
local role bindings, users, groups and service accounts are illustrated below.

image::rbac.png[{product-title} RBAC]

[WARNING]
====
The `get pods/exec`, `get pods/*`, and `get *` rules grant execution privileges when they are applied to a role. Apply the principle of least privilege and assign only the minimal RBAC rights required for users and agents. For more information, see link:https://access.redhat.com/solutions/6989997[RBAC rules allow execution privileges].
====

[id="evaluating-authorization_{context}"]
== Evaluating authorization

{product-title} evaluates authorization by using:

Identity:: The user name and list of groups that the user belongs to.

Action:: The action you perform. In most cases, this consists of:
* *Project*: The project you access. A project is a Kubernetes namespace with
additional annotations that allows a community of users to organize and manage
their content in isolation from other communities.
* *Verb* : The action itself:  `get`, `list`, `create`, `update`, `delete`, `deletecollection`, or `watch`.
* *Resource name*: The API endpoint that you access.
Bindings:: The full list of bindings, the associations between users or groups
with a role.

{product-title} evaluates authorization by using the following steps:

. The identity and the project-scoped action is used to find all bindings that
apply to the user or their groups.
. Bindings are used to locate all the roles that apply.
. Roles are used to find all the rules that apply.
. The action is checked against each rule to find a match.
. If no matching rule is found, the action is then denied by default.


including a matrix of the verbs and resources each are associated with.



:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * authentication/using-rbac.adoc
// * post_installation_configuration/preparing-for-users.adoc

[id="rbac-projects-namespaces_{context}"]
= Projects and namespaces

A Kubernetes _namespace_ provides a mechanism to scope resources in a cluster.
The
https://kubernetes.io/docs/tasks/administer-cluster/namespaces/[Kubernetes documentation]
has more information on namespaces.

Namespaces provide a unique scope for:

* Named resources to avoid basic naming collisions.
* Delegated management authority to trusted users.
* The ability to limit community resource consumption.

Most objects in the system are scoped by namespace, but some are
excepted and have no namespace, including nodes and users.

A _project_ is a Kubernetes namespace with additional annotations and is the central vehicle
by which access to resources for regular users is managed.
A project allows a community of users to organize and manage their content in
isolation from other communities. Users must be given access to projects by administrators,
or if allowed to create projects, automatically have access to their own projects.

Projects can have a separate `name`, `displayName`, and `description`.

- The mandatory `name` is a unique identifier for the project and is most visible when using the CLI tools or API. The maximum name length is 63 characters.
- The optional `displayName` is how the project is displayed in the web console (defaults to `name`).
- The optional `description` can be a more detailed description of the project and is also visible in the web console.

Each project scopes its own set of:

[cols="1,4",options="header"]
|===

|Object
|Description

|`Objects`
|Pods, services, replication controllers, etc.

|`Policies`
|Rules for which users can or cannot perform actions on objects.

|`Constraints`
|Quotas for each kind of object that can be limited.

|`Service accounts`
|Service accounts act automatically with designated access to objects in the project.

|===

Cluster administrators can create projects and delegate administrative rights
for the project to any member of the user community. Cluster administrators can
also allow developers to create their own projects.

Developers and administrators can interact with projects by using the CLI or the
web console.

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * authentication/using-rbac.adoc
// * post_installation_configuration/preparing-for-users.adoc

[id="rbac-default-projects_{context}"]
= Default projects

{product-title} comes with a number of default projects, and projects
starting with `openshift-` are the most essential to users.
These projects host master components that run as pods and other infrastructure
components. The pods created in these namespaces that have a
link:https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#rescheduler-guaranteed-scheduling-of-critical-add-ons[critical pod annotation]
are considered critical, and the have guaranteed admission by kubelet.
Pods created for master components in these namespaces are already marked as
critical.

// Text snippet included in the following assemblies:
//
// * applications/projects/working-with-projects.adoc
// * applications/quotas/quotas-setting-across-multiple-projects.adoc
// * openshift_images/image-streams-manage.adoc
//
// Text snippet included in the following modules:
//
// * modules/admission-plug-ins-about.adoc
// * modules/creating-a-project-using-the-CLI.adoc
// * modules/creating-a-project-using-the-web-console.adoc
// * modules/images-managing-images-enabling-imagestreams-kube.adoc
// * modules/odc-creating-projects-using-developer-perspective.adoc
// * modules/rbac-default-projects.adoc
// * modules/security-context-constraints-psa-about.adoc
// * modules/security-context-constraints-rbac.adoc

:_mod-docs-content-type: SNIPPET

[IMPORTANT]
====
Do not run workloads in or share access to default projects. Default projects are reserved for running core cluster components.

The following default projects are considered highly privileged: `default`, `kube-public`, `kube-system`, `openshift`, `openshift-infra`, `openshift-node`, and other system-created projects that have the `openshift.io/run-level` label set to `0` or `1`. Functionality that relies on admission plugins, such as pod security admission, security context constraints, cluster resource quotas, and image reference resolution, does not work in highly privileged projects.
====

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * authentication/using-rbac.adoc
// * post_installation_configuration/preparing-for-users.adoc

:_mod-docs-content-type: PROCEDURE
[id="viewing-cluster-roles_{context}"]
= Viewing cluster roles and bindings

You can use the `oc` CLI to view cluster roles and bindings by using the
`oc describe` command.

.Prerequisites

* Install the `oc` CLI.
* Obtain permission to view the cluster roles and bindings.


.Procedure

. To view the cluster roles and their associated rule sets:
+

. To view the current set of cluster role bindings, which shows the users and
groups that are bound to various roles:
+

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * authentication/using-rbac.adoc
// * post_installation_configuration/preparing-for-users.adoc

:_mod-docs-content-type: PROCEDURE
[id="viewing-local-roles_{context}"]
= Viewing local roles and bindings

You can use the `oc` CLI to view local roles and bindings by using the
`oc describe` command.

.Prerequisites

* Install the `oc` CLI.
* Obtain permission to view the local roles and bindings:


** Users with the `admin` default cluster role bound locally can view and manage
roles and bindings in that project.

.Procedure

. To view the current set of local role bindings, which show the users and groups
that are bound to various roles for the current project:
+
[source,terminal]
----
$ oc describe rolebinding.rbac
----

. To view the local role bindings for a different project, add the `-n` flag
to the command:
+
[source,terminal]
----
$ oc describe rolebinding.rbac -n joe-project
----
+
.Example output
[source,terminal]
----
Name:         admin
Labels:       <none>
Annotations:  <none>
Role:
  Kind:  ClusterRole
  Name:  admin
Subjects:
  Kind  Name        Namespace
  ----  ----        ---------
  User  kube:admin


Name:         system:deployers
Labels:       <none>
Annotations:  openshift.io/description:
                Allows deploymentconfigs in this namespace to rollout pods in
                this namespace.  It is auto-managed by a controller; remove
                subjects to disa...
Role:
  Kind:  ClusterRole
  Name:  system:deployer
Subjects:
  Kind            Name      Namespace
  ----            ----      ---------
  ServiceAccount  deployer  joe-project


Name:         system:image-builders
Labels:       <none>
Annotations:  openshift.io/description:
                Allows builds in this namespace to push images to this
                namespace.  It is auto-managed by a controller; remove subjects
                to disable.
Role:
  Kind:  ClusterRole
  Name:  system:image-builder
Subjects:
  Kind            Name     Namespace
  ----            ----     ---------
  ServiceAccount  builder  joe-project


Name:         system:image-pullers
Labels:       <none>
Annotations:  openshift.io/description:
                Allows all pods in this namespace to pull images from this
                namespace.  It is auto-managed by a controller; remove subjects
                to disable.
Role:
  Kind:  ClusterRole
  Name:  system:image-puller
Subjects:
  Kind   Name                                Namespace
  ----   ----                                ---------
  Group  system:serviceaccounts:joe-project
----

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * authentication/using-rbac.adoc
// * post_installation_configuration/preparing-for-users.adoc

:_mod-docs-content-type: PROCEDURE
[id="adding-roles_{context}"]
= Adding roles to users

You can use  the `oc adm` administrator CLI to manage the roles and bindings.

Binding, or adding, a role to users or groups gives the user or group the access
that is granted by the role. You can add and remove roles to and from users and
groups using `oc adm policy` commands.

You can bind any of the default cluster roles to local users or groups in your
project.

.Procedure

. Add a role to a user in a specific project:
+
[source,terminal]
----
$ oc adm policy add-role-to-user <role> <user> -n <project>
----
+
For example, you can add the `admin` role to the `alice` user in `joe` project
by running:
+
[source,terminal]
----
$ oc adm policy add-role-to-user admin alice -n joe
----
+
[TIP]
====
You can alternatively apply the following YAML to add the role to the user:

[source,yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: admin-0
  namespace: joe
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: admin
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: User
  name: alice
----
====

. View the local role bindings and verify the addition in the output:
+
[source,terminal]
----
$ oc describe rolebinding.rbac -n <project>
----
+
For example, to view the local role bindings for the `joe` project:
+
[source,terminal]
----
$ oc describe rolebinding.rbac -n joe
----
+
.Example output
[source,terminal]
----

Name:         admin
Labels:       <none>
Annotations:  <none>
Role:
  Kind:  ClusterRole
  Name:  admin
Subjects:
  Kind  Name        Namespace
  ----  ----        ---------
  User  kube:admin


Name:         admin-0
Labels:       <none>
Annotations:  <none>
Role:
  Kind:  ClusterRole
  Name:  admin
Subjects:
  Kind  Name   Namespace
  ----  ----   ---------
  User  alice <1>


Name:         system:deployers
Labels:       <none>
Annotations:  openshift.io/description:
                Allows deploymentconfigs in this namespace to rollout pods in
                this namespace.  It is auto-managed by a controller; remove
                subjects to disa...
Role:
  Kind:  ClusterRole
  Name:  system:deployer
Subjects:
  Kind            Name      Namespace
  ----            ----      ---------
  ServiceAccount  deployer  joe


Name:         system:image-builders
Labels:       <none>
Annotations:  openshift.io/description:
                Allows builds in this namespace to push images to this
                namespace.  It is auto-managed by a controller; remove subjects
                to disable.
Role:
  Kind:  ClusterRole
  Name:  system:image-builder
Subjects:
  Kind            Name     Namespace
  ----            ----     ---------
  ServiceAccount  builder  joe


Name:         system:image-pullers
Labels:       <none>
Annotations:  openshift.io/description:
                Allows all pods in this namespace to pull images from this
                namespace.  It is auto-managed by a controller; remove subjects
                to disable.
Role:
  Kind:  ClusterRole
  Name:  system:image-puller
Subjects:
  Kind   Name                                Namespace
  ----   ----                                ---------
  Group  system:serviceaccounts:joe
----
<1> The `alice` user has been added to the `admins` `RoleBinding`.

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * authentication/using-rbac.adoc
// * post_installation_configuration/preparing-for-users.adoc

:_mod-docs-content-type: PROCEDURE

:leveloffset!:


:leveloffset: +2

// Module included in the following assemblies:
//
// * authentication/using-rbac.adoc
// * post_installation_configuration/preparing-for-users.adoc

[id="local-role-binding-commands_{context}"]
= Local role binding commands

When you manage a user or group's associated roles for local role bindings using the
following operations, a project may be specified with the `-n` flag. If it is
not specified, then the current project is used.

You can use the following commands for local RBAC management.

.Local role binding operations
[options="header"]
|===

|Command |Description

|`$ oc adm policy who-can _<verb>_ _<resource>_`
|Indicates which users can perform an action on a resource.

|`$ oc adm policy add-role-to-user _<role>_ _<username>_`
|Binds a specified role to specified users in the current project.

|`$ oc adm policy remove-role-from-user _<role>_ _<username>_`
|Removes a given role from specified users in the current project.

|`$ oc adm policy remove-user _<username>_`
|Removes specified users and all of their roles in the current project.

|`$ oc adm policy add-role-to-group _<role>_ _<groupname>_`
|Binds a given role to specified groups in the current project.

|`$ oc adm policy remove-role-from-group _<role>_ _<groupname>_`
|Removes a given role from specified groups in the current project.

|`$ oc adm policy remove-group _<groupname>_`
|Removes specified groups and all of their roles in the current project.

|===

:leveloffset!:


:leveloffset: +1

// Module included in the following assemblies:
//
// * authentication/removing-kubeadmin.adoc
// * post_installation_configuration/preparing-for-users.adoc

[id="understanding-kubeadmin_{context}"]
= The kubeadmin user

{product-title} creates a cluster administrator, `kubeadmin`, after the
installation process completes.

This user has the `cluster-admin` role automatically applied and is treated
as the root user for the cluster. The password is dynamically generated
and unique to your {product-title} environment. After installation
completes the password is provided in the installation program's output.
For example:

[source,terminal]
----
INFO Install complete!
INFO Run 'export KUBECONFIG=<your working directory>/auth/kubeconfig' to manage the cluster with 'oc', the OpenShift CLI.
INFO The cluster is ready when 'oc login -u kubeadmin -p <provided>' succeeds (wait a few minutes).
INFO Access the OpenShift web-console here: https://console-openshift-console.apps.demo1.openshift4-beta-abcorp.com
INFO Login to the console with user: kubeadmin, password: <provided>
----

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * authentication/understanding-authentication.adoc
// * authentication/understanding-identity-provider.adoc
// * post_installation_configuration/preparing-for-users.adoc

:_mod-docs-content-type: PROCEDURE
[id="removing-kubeadmin_{context}"]
= Removing the kubeadmin user

After you define an identity provider and create a new `cluster-admin`
user, you can remove the `kubeadmin` to improve cluster security.

[WARNING]
====
If you follow this procedure before another user is a `cluster-admin`,
then {product-title} must be reinstalled. It is not possible to undo
this command.
====

.Prerequisites

* You must have configured at least one identity provider.
* You must have added the `cluster-admin` role to a user.
* You must be logged in as an administrator.

.Procedure

* Remove the `kubeadmin` secrets:
+
[source,terminal]
----
$ oc delete secrets kubeadmin -n kube-system
----

:leveloffset!:

[id="post-install-image-configuration-resources"]
== Image configuration
Understand and configure image registry settings.

:leveloffset: +2

// Module included in the following assemblies:
//
// * openshift_images/image-configuration.adoc
// * post_installation_configuration/preparing-for-users.adoc

[id="images-configuration-parameters_{context}"]
= Image controller configuration parameters

The `image.config.openshift.io/cluster` resource holds cluster-wide information about how to handle images. The canonical, and only valid name is `cluster`. Its `spec` offers the following configuration parameters.

[NOTE]
====
Parameters such as `DisableScheduledImport`, `MaxImagesBulkImportedPerRepository`, `MaxScheduledImportsPerMinute`, `ScheduledImageImportMinimumIntervalSeconds`, `InternalRegistryHostname` are not configurable.
====

[cols="3a,8a",options="header"]
|===
|Parameter |Description

|`allowedRegistriesForImport`
|Limits the container image registries from which normal users can import images. Set this list to the registries that you trust to contain valid images, and that you want applications to be able to import from. Users with permission to create images or `ImageStreamMappings` from the API are not affected by this policy. Typically only cluster administrators have the appropriate permissions.

Every element of this list contains a location of the registry specified by the registry domain name.

`domainName`: Specifies a domain name for the registry. If the registry uses a non-standard `80` or `443` port, the port should be included in the domain name as well.

`insecure`: Insecure indicates whether the registry is secure or insecure. By default, if not otherwise specified, the registry is assumed to be secure.

|`additionalTrustedCA`
|A reference to a config map containing additional CAs that should be trusted during `image stream import`, `pod image pull`, `openshift-image-registry pullthrough`, and builds.

The namespace for this config map is `openshift-config`. The format of the config map is to use the registry hostname as the key, and the PEM-encoded certificate as the value, for each additional registry CA to trust.

|`externalRegistryHostnames`
|Provides the hostnames for the default external image registry. The external hostname should be set only when the image registry is exposed externally. The first value is used in `publicDockerImageRepository` field in image streams. The value must be in `hostname[:port]` format.

|`registrySources`
|Contains configuration that determines how the container runtime should treat individual registries when accessing images for builds and
pods. For instance, whether or not to allow insecure access. It does not contain configuration for the internal cluster registry.

`insecureRegistries`: Registries which do not have a valid TLS certificate or only support HTTP connections. To specify all subdomains, add the asterisk (`\*`) wildcard character as a prefix to the domain name. For example, `*.example.com`. You can specify an individual repository within a registry. For example: `reg1.io/myrepo/myapp:latest`.

`blockedRegistries`: Registries for which image pull and push actions are denied. To specify all subdomains, add the asterisk (`\*`) wildcard character as a prefix to the domain name. For example, `*.example.com`. You can specify an individual repository within a registry. For example: `reg1.io/myrepo/myapp:latest`. All other registries are allowed.

`allowedRegistries`: Registries for which image pull and push actions are allowed. To specify all subdomains, add the asterisk (`\*`) wildcard character as a prefix to the domain name. For example, `*.example.com`. You can specify an individual repository within a registry. For example: `reg1.io/myrepo/myapp:latest`. All other registries are blocked.

`containerRuntimeSearchRegistries`: Registries for which image pull and push actions are allowed using image short names. All other registries are blocked.

Either `blockedRegistries` or `allowedRegistries` can be set, but not both.

|===

[WARNING]
====
When the `allowedRegistries` parameter is defined, all registries, including `registry.redhat.io` and `quay.io` registries and the default {product-registry}, are blocked unless explicitly listed. When using the parameter, to prevent pod failure, add all registries including the `registry.redhat.io` and `quay.io` registries and the `internalRegistryHostname` to the `allowedRegistries` list, as they are required by payload images within your environment. For disconnected clusters, mirror registries should also be added.
====

The `status` field of the `image.config.openshift.io/cluster` resource holds observed values from the cluster.

[cols="3a,8a",options="header"]
|===
|Parameter |Description

|`internalRegistryHostname`
|Set by the Image Registry Operator, which controls the `internalRegistryHostname`. It sets the hostname for the default {product-registry}. The value must be in `hostname[:port]` format. For backward compatibility, you can still use the `OPENSHIFT_DEFAULT_REGISTRY` environment variable, but this setting overrides the environment variable.

|`externalRegistryHostnames`
|Set by the Image Registry Operator, provides the external hostnames for the image registry when it is exposed externally. The first value is used in `publicDockerImageRepository` field in image streams. The values must be in `hostname[:port]` format.

|===

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * openshift_images/image-configuration.adoc
// * post_installation_configuration/preparing-for-users.adoc

:_mod-docs-content-type: PROCEDURE
[id="images-configuration-file_{context}"]
= Configuring image registry settings

You can configure image registry settings by editing the `image.config.openshift.io/cluster` custom resource (CR).
When changes to the registry are applied to the `image.config.openshift.io/cluster` CR, the Machine Config Operator (MCO) performs the following sequential actions:

. Cordons the node
. Applies changes by restarting CRI-O
. Uncordons the node
+
[NOTE]
====
The MCO does not restart nodes when it detects changes.
====

.Procedure

. Edit the `image.config.openshift.io/cluster` custom resource:
+
[source,terminal]
----
$ oc edit image.config.openshift.io/cluster
----
+
The following is an example `image.config.openshift.io/cluster` CR:
+
[source,yaml]
----
apiVersion: config.openshift.io/v1
kind: Image <1>
metadata:
  annotations:
    release.openshift.io/create-only: "true"
  creationTimestamp: "2019-05-17T13:44:26Z"
  generation: 1
  name: cluster
  resourceVersion: "8302"
  selfLink: /apis/config.openshift.io/v1/images/cluster
  uid: e34555da-78a9-11e9-b92b-06d6c7da38dc
spec:
  allowedRegistriesForImport: <2>
    - domainName: quay.io
      insecure: false
  additionalTrustedCA: <3>
    name: myconfigmap
  registrySources: <4>
    allowedRegistries:
    - example.com
    - quay.io
    - registry.redhat.io
    - image-registry.openshift-image-registry.svc:5000
    - reg1.io/myrepo/myapp:latest
    insecureRegistries:
    - insecure.com
status:
  internalRegistryHostname: image-registry.openshift-image-registry.svc:5000
----
<1> `Image`: Holds cluster-wide information about how to handle images. The canonical, and only valid name is `cluster`.
<2> `allowedRegistriesForImport`: Limits the container image registries from which normal users may import images. Set this list to the registries that you trust to contain valid images, and that you want applications to be able to import from. Users with permission to create images or `ImageStreamMappings` from the API are not affected by this policy. Typically only cluster administrators have the appropriate permissions.
<3> `additionalTrustedCA`: A reference to a config map containing additional certificate authorities (CA) that are trusted during image stream import, pod image pull, `openshift-image-registry` pullthrough, and builds. The namespace for this config map is `openshift-config`. The format of the config map is to use the registry hostname as the key, and the PEM certificate as the value, for each additional registry CA to trust.
<4> `registrySources`: Contains configuration that determines whether the container runtime allows or blocks individual registries when accessing images for builds and pods.  Either the `allowedRegistries` parameter or the `blockedRegistries` parameter can be set, but not both. You can also define whether or not to allow access to insecure registries or registries that allow registries that use image short names. This example uses the `allowedRegistries` parameter, which defines the registries that are allowed to be used. The insecure registry `insecure.com` is also allowed. The `registrySources` parameter does not contain configuration for the internal cluster registry.
+
[NOTE]
====
When the `allowedRegistries` parameter is defined, all registries, including the registry.redhat.io and quay.io registries and the default {product-registry}, are blocked unless explicitly listed. If you use the parameter, to prevent pod failure, you must add the `registry.redhat.io` and `quay.io` registries and the `internalRegistryHostname` to the `allowedRegistries` list, as they are required by payload images within your environment. Do not add the `registry.redhat.io` and `quay.io` registries to the `blockedRegistries` list.

When using the `allowedRegistries`, `blockedRegistries`, or `insecureRegistries` parameter, you can specify an individual repository within a registry. For example: `reg1.io/myrepo/myapp:latest`.

Insecure external registries should be avoided to reduce possible security risks.
====

. To check that the changes are applied, list your nodes:
+
[source,terminal]
----
$ oc get nodes
----
+
.Example output
[source,terminal]
----
NAME                                         STATUS                     ROLES                  AGE   VERSION
ip-10-0-137-182.us-east-2.compute.internal   Ready,SchedulingDisabled   worker                 65m   v1.27.3
ip-10-0-139-120.us-east-2.compute.internal   Ready,SchedulingDisabled   control-plane          74m   v1.27.3
ip-10-0-176-102.us-east-2.compute.internal   Ready                      control-plane          75m   v1.27.3
ip-10-0-188-96.us-east-2.compute.internal    Ready                      worker                 65m   v1.27.3
ip-10-0-200-59.us-east-2.compute.internal    Ready                      worker                 63m   v1.27.3
ip-10-0-223-123.us-east-2.compute.internal   Ready                      control-plane          73m   v1.27.3
----

:leveloffset!:

For more information on the allowed, blocked, and insecure registry parameters, see xref:../openshift_images/image-configuration.adoc#images-configuration-file_image-configuration[Configuring image registry settings].

:leveloffset: +2

// Module included in the following assemblies:
//
// * registry/configuring-registry-operator.adoc
// * openshift_images/image-configuration.adoc
// * post_installation_configuration/preparing-for-users.adoc

:_mod-docs-content-type: PROCEDURE
[id="images-configuration-cas_{context}"]
= Configuring additional trust stores for image registry access

The `image.config.openshift.io/cluster` custom resource can contain a reference to a config map that contains additional certificate authorities to be trusted during image registry access.

.Prerequisites
* The certificate authorities (CA) must be PEM-encoded.

.Procedure

You can create a config map in the `openshift-config` namespace and use its name in `AdditionalTrustedCA` in the `image.config.openshift.io` custom resource to provide additional CAs that should be trusted when contacting external registries.

The config map key is the hostname of a registry with the port for which this CA is to be trusted, and the PEM certificate content is the value, for each additional registry CA to trust.

.Image registry CA config map example
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-registry-ca
data:
  registry.example.com: |
    -----BEGIN CERTIFICATE-----
    ...
    -----END CERTIFICATE-----
  registry-with-port.example.com..5000: | <1>
    -----BEGIN CERTIFICATE-----
    ...
    -----END CERTIFICATE-----
----
<1>  If the registry has the port, such as `registry-with-port.example.com:5000`, `:` should be replaced with `..`.

You can configure additional CAs with the following procedure.

* To configure an additional CA:
+
[source,terminal]
----
$ oc create configmap registry-config --from-file=<external_registry_address>=ca.crt -n openshift-config
----
+
[source,terminal]
----
$ oc edit image.config.openshift.io cluster
----
+
[source,yaml]
----
spec:
  additionalTrustedCA:
    name: registry-config
----

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * openshift_images/image-configuration.adoc
// * post_installation_configuration/preparing-for-users.adoc
// * updating/updating_a_cluster/updating_disconnected_cluster/disconnected-update.adoc

:_mod-docs-content-type: PROCEDURE
[id="images-configuration-registry-mirror_{context}"]
= Configuring image registry repository mirroring

Setting up container registry repository mirroring enables you to perform the following tasks:

* Configure your {product-title} cluster to redirect requests to pull images from a repository on a source image registry and have it resolved by a repository on a mirrored image registry.
* Identify multiple mirrored repositories for each target repository, to make sure that if one mirror is down, another can be used.

Repository mirroring in {product-title} includes the following attributes:

* Image pulls are resilient to registry downtimes.
* Clusters in disconnected environments can pull images from critical locations, such as quay.io, and have registries behind a company firewall provide the requested images.
* A particular order of registries is tried when an image pull request is made, with the permanent registry typically being the last one tried.
* The mirror information you enter is added to the `/etc/containers/registries.conf` file on every node in the {product-title} cluster.
* When a node makes a request for an image from the source repository, it tries each mirrored repository in turn until it finds the requested content. If all mirrors fail, the cluster tries the source repository. If successful, the image is pulled to the node.

Setting up repository mirroring can be done in the following ways:

* At {product-title} installation:
+
By pulling container images needed by {product-title} and then bringing those images behind your company's firewall, you can install {product-title} into a datacenter that is in a disconnected environment.

* After {product-title} installation:
+
If you did not configure mirroring during {product-title} installation, you can do so postinstallation by using one of the following custom resource (CR) objects:
+
--
** `ImageDigestMirrorSet`. This CR allows you to pull images from a mirrored registry by using digest specifications.
+
** `ImageTagMirrorSet`. This CR allows you to pull images from a mirrored registry by using image tags.
--
+
[IMPORTANT]
====
Using an `ImageContentSourcePolicy` (ICSP) object to configure repository mirroring is a deprecated feature. Deprecated functionality is still included in {product-title} and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments. If you have existing YAML files that you used to create `ImageContentSourcePolicy` objects, you can use the `oc adm migrate icsp` command to convert those files to an `ImageDigestMirrorSet` YAML file. For more information, see "Converting ImageContentSourcePolicy (ICSP) files for image registry repository mirroring" in the following section.
====

Both of these custom resource objects identify the following information:
--
* The source of the container image repository you want to mirror.
* A separate entry for each mirror repository you want to offer the content
requested from the source repository.
--

[NOTE]
====
If your cluster uses an `ImageDigestMirrorSet` or `ImageTagMirrorSet` object to configure repository mirroring, you can use only global pull secrets for mirrored registries. You cannot add a pull secret to a project.
====

The following procedure creates a postinstallation mirror configuration, where you create an `ImageDigestMirrorSet` object.

.Prerequisites
* Access to the cluster as a user with the `cluster-admin` role.

* Ensure that there are no `ImageContentSourcePolicy` objects on your cluster. For example, you can use the following command:
+
[source,terminal]
----
$ oc get ImageContentSourcePolicy
----
+
.Example output
[source,terminal]
----
No resources found
----

.Procedure

. Configure mirrored repositories, by either:
+
* Setting up a mirrored repository with Red Hat Quay, as described in link:https://access.redhat.com/documentation/en-us/red_hat_quay/3/html/manage_red_hat_quay/repo-mirroring-in-red-hat-quay[Red Hat Quay Repository Mirroring]. Using Red Hat Quay allows you to copy images from one repository to another and also automatically sync those repositories repeatedly over time.
* Using a tool such as `skopeo` to copy images manually from the source directory to the mirrored repository.
+
For example, after installing the skopeo RPM package on a Red Hat Enterprise Linux (RHEL) 7 or RHEL 8 system, use the `skopeo` command as shown in this example:
+
[source,terminal]
----
$ skopeo copy \
docker://registry.access.redhat.com/ubi9/ubi-minimal:latest@sha256:5cf... \
docker://example.io/example/ubi-minimal
----
+
In this example, you have a container image registry that is named `example.io` with an image repository named `example` to which you want to copy the `ubi9/ubi-minimal` image from `registry.access.redhat.com`. After you create the registry, you can configure your {product-title} cluster to redirect requests made of the source repository to the mirrored repository.

. Log in to your {product-title} cluster.

. Create an `ImageDigestMirrorSet` or `ImageTagMirrorSet` CR, as needed, replacing the source and mirrors with your own registry and repository pairs and images:
+
[source,yaml]
----
apiVersion: config.openshift.io/v1 <1>
kind: ImageDigestMirrorSet <2>
metadata:
  name: ubi9repo
spec:
  imageDigestMirrors: <3>
  - mirrors:
    - example.io/example/ubi-minimal <4>
    - example.com/example/ubi-minimal <5>
    source: registry.access.redhat.com/ubi9/ubi-minimal <6>
    mirrorSourcePolicy: AllowContactingSource <7>
  - mirrors:
    - mirror.example.com/redhat
    source: registry.redhat.io/openshift4 <8>
    mirrorSourcePolicy: AllowContactingSource
  - mirrors:
    - mirror.example.com
    source: registry.redhat.io <9>
    mirrorSourcePolicy: AllowContactingSource
  - mirrors:
    - mirror.example.net/image
    source: registry.example.com/example/myimage <10>
    mirrorSourcePolicy: AllowContactingSource
  - mirrors:
    - mirror.example.net
    source: registry.example.com/example <11>
    mirrorSourcePolicy: AllowContactingSource
  - mirrors:
    - mirror.example.net/registry-example-com
    source: registry.example.com <12>
    mirrorSourcePolicy: AllowContactingSource
----
<1> Indicates the API to use with this CR. This must be `config.openshift.io/v1`.
<2> Indicates the kind of object according to the pull type:
** `ImageDigestMirrorSet`: Pulls a digest reference image.
** `ImageTagMirrorSet`: Pulls a tag reference image.
<3> Indicates the type of image pull method, either:
** `imageDigestMirrors`: Use for an `ImageDigestMirrorSet` CR.
** `imageTagMirrors`: Use for an `ImageTagMirrorSet` CR.
<4> Indicates the name of the mirrored image registry and repository.
<5> Optional: Indicates a secondary mirror repository for each target repository. If one mirror is down, the target repository can use another mirror.
<6> Indicates the registry and repository source, which is the repository that is referred to in image pull specifications.
<7> Optional: Indicates the fallback policy if the image pull fails:
** `AllowContactingSource`: Allows continued attempts to pull the image from the source repository. This is the default.
** `NeverContactSource`: Prevents continued attempts to pull the image from the source repository.
<8> Optional: Indicates a namespace inside a registry, which allows you to use any image in that namespace. If you use a registry domain as a source, the object is applied to all repositories from the registry.
<9> Optional: Indicates a registry, which allows you to use any image in that registry. If you specify a registry name, the object is applied to all repositories from a source registry to a mirror registry.
<10> Pulls the image `registry.example.com/example/myimage@sha256:...` from the mirror `mirror.example.net/image@sha256:..`.
<11> Pulls the image `registry.example.com/example/image@sha256:...` in the source registry namespace from the mirror `mirror.example.net/image@sha256:...`.
<12> Pulls the image `registry.example.com/myimage@sha256` from the mirror registry `example.net/registry-example-com/myimage@sha256:...`. The `ImageContentSourcePolicy` resource is applied to all repositories from a source registry to a mirror registry `mirror.example.net/registry-example-com`.

. Create the new object:
+
[source,terminal]
----
$ oc create -f registryrepomirror.yaml
----
+
After the object is created, the Machine Config Operator (MCO) cordons the nodes as the new settings are deployed to each node. The MCO restarts the nodes for an `ImageTagMirrorSet` object only. The MCO does not restart the nodes for `ImageDigestMirrorSet` objects. When the nodes are uncordoned, the cluster starts using the mirrored repository for requests to the source repository.

. To check that the mirrored configuration settings are applied, do the following on one of the nodes.

.. List your nodes:
+
[source,terminal]
----
$ oc get node
----
+
.Example output
[source,terminal]
----
NAME                           STATUS                     ROLES    AGE  VERSION
ip-10-0-137-44.ec2.internal    Ready                      worker   7m   v1.27.3
ip-10-0-138-148.ec2.internal   Ready                      master   11m  v1.27.3
ip-10-0-139-122.ec2.internal   Ready                      master   11m  v1.27.3
ip-10-0-147-35.ec2.internal    Ready                      worker   7m   v1.27.3
ip-10-0-153-12.ec2.internal    Ready                      worker   7m   v1.27.3
ip-10-0-154-10.ec2.internal    Ready                      master   11m  v1.27.3
----

.. Start the debugging process to access the node:
+
[source,terminal]
----
$ oc debug node/ip-10-0-147-35.ec2.internal
----
+
.Example output
[source,terminal]
----
Starting pod/ip-10-0-147-35ec2internal-debug ...
To use host binaries, run `chroot /host`
----

.. Change your root directory to `/host`:
+
[source,terminal]
----
sh-4.2# chroot /host
----

.. Check the `/etc/containers/registries.conf` file to make sure
the changes were made:
+
[source,terminal]
----
sh-4.2# cat /etc/containers/registries.conf
----
+
The following output represents a `registries.conf` file where an `ImageDigestMirrorSet` object and an `ImageTagMirrorSet` object were applied. The final two entries are marked `digest-only` and `tag-only` respectively.
+
.Example output
[source,terminal]
----
unqualified-search-registries = ["registry.access.redhat.com", "docker.io"]
short-name-mode = ""

[[registry]]
  prefix = ""
  location = "registry.access.redhat.com/ubi9/ubi-minimal" <1>

  [[registry.mirror]]
    location = "example.io/example/ubi-minimal" <2>
    pull-from-mirror = "digest-only" <3>

  [[registry.mirror]]
    location = "example.com/example/ubi-minimal"
    pull-from-mirror = "digest-only"

[[registry]]
  prefix = ""
  location = "registry.example.com"

  [[registry.mirror]]
    location = "mirror.example.net/registry-example-com"
    pull-from-mirror = "digest-only"

[[registry]]
  prefix = ""
  location = "registry.example.com/example"

  [[registry.mirror]]
    location = "mirror.example.net"
    pull-from-mirror = "digest-only"

[[registry]]
  prefix = ""
  location = "registry.example.com/example/myimage"

  [[registry.mirror]]
    location = "mirror.example.net/image"
    pull-from-mirror = "digest-only"

[[registry]]
  prefix = ""
  location = "registry.redhat.io"

  [[registry.mirror]]
    location = "mirror.example.com"
    pull-from-mirror = "digest-only"

[[registry]]
  prefix = ""
  location = "registry.redhat.io/openshift4"

  [[registry.mirror]]
    location = "mirror.example.com/redhat"
    pull-from-mirror = "digest-only"
[[registry]]
  prefix = ""
  location = "registry.access.redhat.com/ubi9/ubi-minimal"
  blocked = true <4>

  [[registry.mirror]]
    location = "example.io/example/ubi-minimal-tag"
    pull-from-mirror = "tag-only" <5>
----
<1> Indicates the repository that is referred to in a pull spec.
<2> Indicates the mirror for that repository.
<3> Indicates that the image pull from the mirror is a digest reference image.
<4> Indicates that the `NeverContactSource` parameter is set for this repository.
<5> Indicates that the image pull from the mirror is a tag reference image.

.. Pull an image to the node from the source and check if it is resolved by the mirror.
+
[source,terminal]
----
sh-4.2# podman pull --log-level=debug registry.access.redhat.com/ubi9/ubi-minimal@sha256:5cf...
----

.Troubleshooting repository mirroring

If the repository mirroring procedure does not work as described, use the following information about how repository mirroring works to help troubleshoot the problem.

* The first working mirror is used to supply the pulled image.
* The main registry is only used if no other mirror works.
* From the system context, the `Insecure` flags are used as fallback.
* The format of the `/etc/containers/registries.conf` file has changed recently. It is now version 2 and in TOML format.
* You cannot add the same repository to both an `ImageDigestMirrorSet` and an `ImageTagMirrorSet` object.


:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * openshift_images/image-configuration.adoc
// * post_installation_configuration/preparing-for-users.adoc
// * updating/updating_a_cluster/updating_disconnected_cluster/disconnected-update.adoc

:_mod-docs-content-type: PROCEDURE
[id="images-configuration-registry-mirror-convert_{context}"]
= Converting ImageContentSourcePolicy (ICSP) files for image registry repository mirroring

Using an `ImageContentSourcePolicy` (ICSP) object to configure repository mirroring is a deprecated feature. This functionality is still included in {product-title} and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments.

ICSP objects are being replaced by `ImageDigestMirrorSet` and `ImageTagMirrorSet` objects to configure repository mirroring. If you have existing YAML files that you used to create `ImageContentSourcePolicy` objects, you can use the `oc adm migrate icsp` command to convert those files to an `ImageDigestMirrorSet` YAML file. The command updates the API to the current version, changes the `kind` value to `ImageDigestMirrorSet`, and changes `spec.repositoryDigestMirrors` to `spec.imageDigestMirrors`. The rest of the file is not changed.

For more information about `ImageDigestMirrorSet` or `ImageTagMirrorSet` objects, see "Configuring image registry repository mirroring" in the previous section.

.Prerequisites

* Access to the cluster as a user with the `cluster-admin` role.

* Ensure that you have `ImageContentSourcePolicy` objects on your cluster.

.Procedure

. Use the following command to convert one or more `ImageContentSourcePolicy` YAML files to an `ImageDigestMirrorSet` YAML file:
+
[source,terminal]
----
$ oc adm migrate icsp <file_name>.yaml <file_name>.yaml <file_name>.yaml --dest-dir <path_to_the_directory>
----
+
--
where:

`<file_name>`:: Specifies the name of the source `ImageContentSourcePolicy` YAML. You can list multiple file names.
`--dest-dir`:: Optional: Specifies a directory for the output `ImageDigestMirrorSet` YAML. If unset, the file is written to the current directory.
--
+
For example, the following command converts the `icsp.yaml` and `icsp-2.yaml` file and saves the new YAML files to the `idms-files` directory.
+
[source,terminal]
----
$ oc adm migrate icsp icsp.yaml icsp-2.yaml --dest-dir idms-files
----
+
.Example output
[source,terminal]
----
wrote ImageDigestMirrorSet to idms-files/imagedigestmirrorset_ubi8repo.5911620242173376087.yaml
wrote ImageDigestMirrorSet to idms-files/imagedigestmirrorset_ubi9repo.6456931852378115011.yaml
----

. Create the CR object by running the following command:
+
[source,terminal]
----
$ oc create -f <path_to_the_directory>/<file-name>.yaml
----
+
--
where:

`<path_to_the_directory>`:: Specifies the path to the directory, if you used the `--dest-dir` flag.
`<file_name>`:: Specifies the name of the `ImageDigestMirrorSet` YAML.
--


:leveloffset!:

[id="post-install-mirrored-catalogs"]
== Populating OperatorHub from mirrored Operator catalogs

If you mirrored Operator catalogs for use with disconnected clusters, you can populate OperatorHub with the Operators from your mirrored catalogs. You can use the generated manifests from the mirroring process to create the required `ImageContentSourcePolicy` and `CatalogSource` objects.

[id="prerequisites_post-install-mirrored-catalogs"]
=== Prerequisites

* xref:../installing/disconnected_install/installing-mirroring-installation-images.adoc#olm-mirror-catalog_installing-mirroring-installation-images[Mirroring Operator catalogs for use with disconnected clusters]

:leveloffset: +2

// Module included in the following assemblies:
//
// * post_installation_configuration/preparing-for-users.adoc

:_mod-docs-content-type: PROCEDURE
[id="olm-mirror-catalog-icsp_{context}"]
= Creating the ImageContentSourcePolicy object

After mirroring Operator catalog content to your mirror registry, create the required `ImageContentSourcePolicy` (ICSP) object. The ICSP object configures nodes to translate between the image references stored in Operator manifests and the mirrored registry.

.Procedure

* On a host with access to the disconnected cluster, create the ICSP by running the following command to specify the `imageContentSourcePolicy.yaml` file in your manifests directory:
+
[source,terminal,subs="attributes+"]
----
$ oc create -f <path/to/manifests/dir>/imageContentSourcePolicy.yaml
----
+
where `<path/to/manifests/dir>` is the path to the manifests directory for your mirrored content.
+
You can now create a `CatalogSource` object to reference your mirrored index image and Operator content.

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * post_installation_configuration/preparing-for-users.adoc
// * operators/admin/olm-restricted-networks.adoc
// * operators/admin/managing-custom-catalogs.adoc

:index-image: redhat-operator-index
:tag: v{product-version}
:namespace: openshift-marketplace
:olm-restricted-networks:

:_mod-docs-content-type: PROCEDURE
[id="olm-creating-catalog-from-index_{context}"]
= Adding a catalog source to a cluster

Adding a catalog source to an {product-title} cluster enables the discovery and installation of Operators for users.
Cluster administrators
can create a `CatalogSource` object that references an index image. OperatorHub uses catalog sources to populate the user interface.

// In OSD/ROSA, a dedicated-admin can see catalog sources here, but can't add, edit, or delete them.
[TIP]
====
Alternatively, you can use the web console to manage catalog sources. From the *Administration* -> *Cluster Settings* -> *Configuration* -> *OperatorHub* page, click the *Sources* tab, where you can create, update, delete, disable, and enable individual sources.
====

// In OSD/ROSA, a dedicated-admin can update catalog sources in the console by searching for them.

.Prerequisites

* You built and pushed an index image to a registry.
* You have access to the cluster as a user with the `cluster-admin` role.

.Procedure

. Create a `CatalogSource` object that references your index image.
If you used the `oc adm catalog mirror` command to mirror your catalog to a target registry, you can use the generated `catalogSource.yaml` file in your manifests directory as a starting point.

.. Modify the following to your specifications and save it as a `catalogSource.yaml` file:
+
[source,yaml,subs="attributes+"]
----
apiVersion: operators.coreos.com/v1alpha1
kind: CatalogSource
metadata:
  name: my-operator-catalog <1>
  namespace: {namespace} <2>
spec:
  sourceType: grpc
  grpcPodConfig:
    securityContextConfig: <security_mode> <3>
  image: <registry>/<namespace>/{index-image}:{tag} <4>
  displayName: My Operator Catalog
  publisher: <publisher_name> <5>
  updateStrategy:
    registryPoll: <6>
      interval: 30m
----
<1> If you mirrored content to local files before uploading to a registry, remove any backslash (`/`) characters from the `metadata.name` field to avoid an "invalid resource name" error when you create the object.
<2> If you want the catalog source to be available globally to users in all namespaces, specify the `{namespace}` namespace. Otherwise, you can specify a different namespace for the catalog to be scoped and available only for that namespace.
<3> Specify the value of `legacy` or `restricted`. If the field is not set, the default value is `legacy`. In a future {product-title} release, it is planned that the default value will be `restricted`. If your catalog cannot run with `restricted` permissions, it is recommended that you manually set this field to `legacy`.
<4> Specify your index image. If you specify a tag after the image name, for example `:{tag}`, the catalog source pod uses an image pull policy of `Always`, meaning the pod always pulls the image prior to starting the container. If you specify a digest, for example `@sha256:<id>`, the image pull policy is `IfNotPresent`, meaning the pod pulls the image only if it does not already exist on the node.
<5> Specify your name or an organization name publishing the catalog.
<6> Catalog sources can automatically check for new versions to keep up to date.

.. Use the file to create the `CatalogSource` object:
+
[source,terminal]
----
$ oc apply -f catalogSource.yaml
----

. Verify the following resources are created successfully.

.. Check the pods:
+
[source,terminal,subs="attributes+"]
----
$ oc get pods -n {namespace}
----
+
.Example output
[source,terminal]
----
NAME                                    READY   STATUS    RESTARTS  AGE
my-operator-catalog-6njx6               1/1     Running   0         28s
marketplace-operator-d9f549946-96sgr    1/1     Running   0         26h
----

.. Check the catalog source:
+
[source,terminal,subs="attributes+"]
----
$ oc get catalogsource -n {namespace}
----
+
.Example output
[source,terminal]
----
NAME                  DISPLAY               TYPE PUBLISHER  AGE
my-operator-catalog   My Operator Catalog   grpc            5s
----

.. Check the package manifest:
+
[source,terminal,subs="attributes+"]
----
$ oc get packagemanifest -n {namespace}
----
+
.Example output
[source,terminal]
----
NAME                          CATALOG               AGE
jaeger-product                My Operator Catalog   93s
----

You can now install the Operators from the *OperatorHub* page on your {product-title} web console.

:!index-image:
:!tag:
:!namespace:
:!olm-restricted-networks:

:leveloffset!:
[role="_additional-resources"]
.Additional resources

* xref:../operators/admin/olm-managing-custom-catalogs.adoc#olm-accessing-images-private-registries_olm-managing-custom-catalogs[Accessing images for Operators from private registries]
* xref:../operators/understanding/olm/olm-understanding-olm.adoc#olm-catalogsource-image-template_olm-understanding-olm[Image template for custom catalog sources]
* xref:../openshift_images/managing_images/image-pull-policy.adoc#image-pull-policy[Image pull policy]

:leveloffset: +1

// Module included in the following assemblies:
//
// * operators/user/olm-installing-operators-in-namespace.adoc
// * operators/admin/olm-adding-operators-to-cluster.adoc
// * post_installation_configuration/preparing-for-users.adoc
//
// Module watched for changes by Ecosystem Catalog team:
// https://projects.engineering.redhat.com/projects/RHEC/summary


:_mod-docs-content-type: CONCEPT
[id="olm-installing-operators-from-operatorhub_{context}"]
= About Operator installation with OperatorHub

OperatorHub is a user interface for discovering Operators; it works in conjunction with Operator Lifecycle Manager (OLM), which installs and manages Operators on a cluster.

As a cluster administrator, you can install an Operator from OperatorHub by using the {product-title}



During installation, you must determine the following initial settings for the Operator:


Update Channel:: If an Operator is available through multiple channels, you can choose which channel you want to subscribe to. For example, to deploy from the *stable* channel, if available, select it from the list.

Approval Strategy:: You can choose automatic or manual updates.
+
If you choose automatic updates for an installed Operator, when a new version of that Operator is available in the selected channel, Operator Lifecycle Manager (OLM) automatically upgrades the running instance of your Operator without human intervention.
+
If you select manual updates, when a newer version of an Operator is available, OLM creates an update request. As a
cluster administrator,
you must then manually approve that update request to have the Operator updated to the new version.

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * operators/user/olm-installing-operators-in-namespace.adoc
// * operators/admin/olm-adding-operators-to-cluster.adoc
// * post_installation_configuration/preparing-for-users.adoc
//
// Module watched for changes by Ecosystem Catalog team:
// https://projects.engineering.redhat.com/projects/RHEC/summary

// Add additional ifevals here, but before context == olm-adding-operators-to-a-cluster
//ifeval::["{context}" != "olm-adding-operators-to-a-cluster"]
:filter-type: jaeger
:filter-operator: Jaeger
:olm-admin:

// Keep this ifeval last

:_mod-docs-content-type: PROCEDURE
[id="olm-installing-from-operatorhub-using-web-console_{context}"]
= Installing from OperatorHub using the web console

You can install and subscribe to an Operator from OperatorHub by using the {product-title} web console.

.Prerequisites

* Access to an {product-title} cluster using an account with


.Procedure

. Navigate in the web console to the *Operators  OperatorHub* page.

. Scroll or type a keyword into the *Filter by keyword* box to find the Operator you want. For example, type `{filter-type}` to find the {filter-operator} Operator.
+
You can also filter options by *Infrastructure Features*. For example, select *Disconnected* if you want to see Operators that work in disconnected environments, also known as restricted network environments.

. Select the Operator to display additional information.
+
[NOTE]
====
Choosing a Community Operator warns that Red Hat does not certify Community Operators; you must acknowledge the warning before continuing.
====

. Read the information about the Operator and click *Install*.

. On the *Install Operator* page:

.. Select one of the following:
*** *All namespaces on the cluster (default)* installs the Operator in the default `openshift-operators` namespace to watch and be made available to all namespaces in the cluster. This option is not always available.
*** *A specific namespace on the cluster* allows you to choose a specific, single namespace in which to install the Operator. The Operator will only watch and be made available for use in this single namespace.

.. If the cluster is in STS mode, add the role ARN in the *role ARN* field.

.. If more than one update channel is available, select an *Update channel*.

.. Select *Automatic* or *Manual* approval strategy, as described earlier.
+
[IMPORTANT]
====
If the web console shows that the cluster is in "STS mode", you must set *Update approval* to *Manual*.

Subscriptions with automatic update approvals are not recommended because there might be permission changes to make prior to updating. Subscriptions with manual update approvals ensure that administrators have the opportunity to verify the permissions of the later version and take any necessary steps prior to update.
====

. Click *Install* to make the Operator available to the selected namespaces on this {product-title} cluster.

.. If you selected a *Manual* approval strategy, the upgrade status of the subscription remains *Upgrading* until you review and approve the install plan.
+
After approving on the *Install Plan* page, the subscription upgrade status moves to *Up to date*.

.. If you selected an *Automatic* approval strategy, the upgrade status should resolve to *Up to date* without intervention.

. After the upgrade status of the subscription is *Up to date*, select *Operators  Installed Operators* to verify that the cluster service version (CSV) of the installed Operator eventually shows up. The *Status* should ultimately resolve to *InstallSucceeded* in the relevant namespace.
+
[NOTE]
====
For the *All namespaces...* installation mode, the status resolves to *InstallSucceeded* in the `openshift-operators` namespace, but the status is *Copied* if you check in other namespaces.
====
+
If it does not:

.. Check the logs in any pods in the `openshift-operators` project (or other relevant namespace if *A specific namespace...* installation mode was selected) on the *Workloads  Pods* page that are reporting issues to troubleshoot further.

:leveloffset!:

//# includes=_attributes/common-attributes,modules/identity-provider-overview,modules/identity-provider-parameters,modules/identity-provider-default-CR,modules/rbac-overview,modules/rbac-projects-namespaces,modules/rbac-default-projects,modules/snippets/default-projects,modules/rbac-viewing-cluster-roles,modules/rbac-viewing-local-roles,modules/rbac-adding-roles,modules/rbac-creating-local-role,modules/rbac-local-role-binding-commands,modules/authentication-kubeadmin,modules/authentication-remove-kubeadmin,modules/images-configuration-parameters,modules/images-configuration-file,modules/images-configuration-cas,modules/images-configuration-registry-mirror,modules/images-configuration-registry-mirror-convert,modules/olm-mirroring-catalog-icsp,modules/olm-creating-catalog-from-index,modules/olm-installing-operators-from-operatorhub,modules/olm-installing-from-operatorhub-using-web-console

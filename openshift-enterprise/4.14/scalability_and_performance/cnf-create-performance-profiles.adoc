:_mod-docs-content-type: ASSEMBLY
[id="cnf-create-performance-profiles"]
= Creating a performance profile
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
:context: cnf-create-performance-profiles

toc::[]

Learn about the Performance Profile Creator (PPC) and how you can use it to create a performance profile.

[NOTE]
====
Currently, disabling CPU load balancing is not supported by cgroup v2. As a result, you might not get the desired behavior from performance profiles if you have cgroup v2 enabled. Enabling cgroup v2 is not recommended if you are using performance profiles.
====

:leveloffset: +1

// Module included in the following assemblies:
// Epic CNF-792 (4.8)
// * scalability_and_performance/cnf-create-performance-profiles.adoc

:_mod-docs-content-type: CONCEPT
[id="cnf-about-the-profile-creator-tool_{context}"]
= About the Performance Profile Creator

The Performance Profile Creator (PPC) is a command-line tool, delivered with the Node Tuning Operator, used to create the performance profile.
The tool consumes `must-gather` data from the cluster and several user-supplied profile arguments. The PPC generates a performance profile that is appropriate for your hardware and topology.

The tool is run by one of the following methods:

* Invoking `podman`

* Calling a wrapper script

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
// Epic CNF-792 (4.8)
// * scalability_and_performance/cnf-create-performance-profiles.adoc

:_mod-docs-content-type: PROCEDURE
[id="gathering-data-about-your-cluster-using-must-gather_{context}"]
= Gathering data about your cluster using the must-gather command

The Performance Profile Creator (PPC) tool requires `must-gather` data. As a cluster administrator, run the `must-gather` command to capture information about your cluster.

.Prerequisites

* Access to the cluster as a user with the `cluster-admin` role.
* The OpenShift CLI (`oc`) installed.

.Procedure

. Optional: Verify that a matching machine config pool exists with a label:
+
[source,terminal]
----
$ oc describe mcp/worker-rt
----
+
.Example output
[source,terminal]
----
Name:         worker-rt
Namespace:
Labels:       machineconfiguration.openshift.io/role=worker-rt
----

. If a matching label does not exist add a label for a machine config pool (MCP) that matches with the MCP name:
+
[source,terminal]
----
$ oc label mcp <mcp_name> <mcp_name>=""
----

. Navigate to the directory where you want to store the `must-gather` data.

. Collect cluster information by running the following command:
+
[source,terminal]
----
$ oc adm must-gather
----

. Optional: Create a compressed file from the `must-gather` directory:
+
[source,terminal]
----
$ tar cvaf must-gather.tar.gz must-gather/
----
+
[NOTE]
====
Compressed output is required if you are running the Performance Profile Creator wrapper script.
====

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
// Epic CNF-792 (4.8)
// * scalability_and_performance/cnf-create-performance-profiles.adoc

:_mod-docs-content-type: PROCEDURE
[id="running-the-performance-profile-profile-cluster-using-podman_{context}"]
= Running the Performance Profile Creator using podman

As a cluster administrator, you can run `podman` and the Performance Profile Creator to create a performance profile.

.Prerequisites

* Access to the cluster as a user with the `cluster-admin` role.
* A cluster installed on bare-metal hardware.
* A node with `podman` and OpenShift CLI (`oc`) installed.
* Access to the Node Tuning Operator image.

.Procedure

. Check the machine config pool:
+
[source,terminal]
----
$ oc get mcp
----
.Example output
+
[source,terminal]
----
NAME         CONFIG                                                 UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
master       rendered-master-acd1358917e9f98cbdb599aea622d78b       True      False      False      3              3                   3                     0                      22h
worker-cnf   rendered-worker-cnf-1d871ac76e1951d32b2fe92369879826   False     True       False      2              1                   1                     0                      22h
----

. Use Podman to authenticate to `registry.redhat.io`:
+
[source,terminal]
----
$ podman login registry.redhat.io
----
+
[source,bash]
----
Username: <username>
Password: <password>
----

. Optional: Display help for the PPC tool:
+
[source,terminal,subs="attributes+"]
----
$ podman run --rm --entrypoint performance-profile-creator registry.redhat.io/openshift4/ose-cluster-node-tuning-operator:v{product-version} -h
----
+
.Example output
+
[source,terminal]
----
A tool that automates creation of Performance Profiles

Usage:
  performance-profile-creator [flags]

Flags:
      --disable-ht                        Disable Hyperthreading
  -h, --help                              help for performance-profile-creator
      --info string                       Show cluster information; requires --must-gather-dir-path, ignore the other arguments. [Valid values: log, json] (default "log")
      --mcp-name string                   MCP name corresponding to the target machines (required)
      --must-gather-dir-path string       Must gather directory path (default "must-gather")
      --offlined-cpu-count int            Number of offlined CPUs
      --per-pod-power-management          Enable Per Pod Power Management
      --power-consumption-mode string     The power consumption mode.  [Valid values: default, low-latency, ultra-low-latency] (default "default")
      --profile-name string               Name of the performance profile to be created (default "performance")
      --reserved-cpu-count int            Number of reserved CPUs (required)
      --rt-kernel                         Enable Real Time Kernel (required)
      --split-reserved-cpus-across-numa   Split the Reserved CPUs across NUMA nodes
      --topology-manager-policy string    Kubelet Topology Manager Policy of the performance profile to be created. [Valid values: single-numa-node, best-effort, restricted] (default "restricted")
      --user-level-networking             Run with User level Networking(DPDK) enabled
----

. Run the Performance Profile Creator tool in discovery mode:
+
[NOTE]
====
Discovery mode inspects your cluster using the output from `must-gather`. The output produced includes information on:

* The NUMA cell partitioning with the allocated CPU ids
* Whether hyperthreading is enabled

Using this information you can set appropriate values for some of the arguments supplied to the Performance Profile Creator tool.
====
+
[source,terminal,subs="attributes+"]
----
$ podman run --entrypoint performance-profile-creator -v <path_to_must-gather>/must-gather:/must-gather:z registry.redhat.io/openshift4/ose-cluster-node-tuning-operator:v{product-version} --info log --must-gather-dir-path /must-gather
----
+
[NOTE]
====
This command uses the performance profile creator as a new entry point to `podman`. It maps the `must-gather` data for the host into the container image and invokes the required user-supplied profile arguments to produce the `my-performance-profile.yaml` file.

The `-v` option can be the path to either:

* The `must-gather` output directory
* An existing directory containing the `must-gather` decompressed tarball

The `info` option requires a value which specifies the output format. Possible values are log and JSON. The JSON format is reserved for debugging.
====

. Run `podman`:
+
[source,terminal,subs="attributes+"]
----
$ podman run --entrypoint performance-profile-creator -v /must-gather:/must-gather:z registry.redhat.io/openshift4/ose-cluster-node-tuning-operator:v{product-version} --mcp-name=worker-cnf --reserved-cpu-count=4 --rt-kernel=true --split-reserved-cpus-across-numa=false --must-gather-dir-path /must-gather --power-consumption-mode=ultra-low-latency --offlined-cpu-count=6 > my-performance-profile.yaml
----
+
[NOTE]
====
The Performance Profile Creator arguments are shown in the Performance Profile Creator arguments table. The following arguments are required:

* `reserved-cpu-count`
* `mcp-name`
* `rt-kernel`

The `mcp-name` argument in this example is set to `worker-cnf` based on the output of the command `oc get mcp`. For {sno} use `--mcp-name=master`.
====

. Review the created YAML file:
+
[source,terminal]
----
$ cat my-performance-profile.yaml
----
.Example output
+
[source,yaml]
----
apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
  name: performance
spec:
  cpu:
    isolated: 2-39,48-79
    offlined: 42-47
    reserved: 0-1,40-41
  machineConfigPoolSelector:
    machineconfiguration.openshift.io/role: worker-cnf
  nodeSelector:
    node-role.kubernetes.io/worker-cnf: ""
  numa:
    topologyPolicy: restricted
  realTimeKernel:
    enabled: true
  workloadHints:
    highPowerConsumption: true
    realTime: true
----

. Apply the generated profile:
+
[source,terminal]
----
$ oc apply -f my-performance-profile.yaml
----

:leveloffset!:

:leveloffset: +3

// Module included in the following assemblies:
// Epic CNF-792 (4.8)
// * scalability_and_performance/cnf-create-performance-profiles.adoc

[id="how-to-run-podman-to-create-a-profile_{context}"]
= How to run `podman` to create a performance profile

The following example illustrates how to run `podman` to create a performance profile with 20 reserved CPUs that are to be split across the NUMA nodes.

Node hardware configuration:

* 80 CPUs
* Hyperthreading enabled
* Two NUMA nodes
* Even numbered CPUs run on NUMA node 0 and odd numbered CPUs run on NUMA node 1

Run `podman` to create the performance profile:

[source,terminal,subs="attributes+"]
----
$ podman run --entrypoint performance-profile-creator -v /must-gather:/must-gather:z registry.redhat.io/openshift4/ose-cluster-node-tuning-operator:v{product-version} --mcp-name=worker-cnf --reserved-cpu-count=20 --rt-kernel=true --split-reserved-cpus-across-numa=true --must-gather-dir-path /must-gather > my-performance-profile.yaml
----

The created profile is described in the following YAML:

[source,yaml]
----
  apiVersion: performance.openshift.io/v2
  kind: PerformanceProfile
  metadata:
    name: performance
  spec:
    cpu:
      isolated: 10-39,50-79
      reserved: 0-9,40-49
    nodeSelector:
      node-role.kubernetes.io/worker-cnf: ""
    numa:
      topologyPolicy: restricted
    realTimeKernel:
      enabled: true
----

[NOTE]
====
In this case, 10 CPUs are reserved on NUMA node 0 and 10 are reserved on NUMA node 1.
====

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
// Epic CNF-792 (4.8)
// * scalability_and_performance/cnf-create-performance-profiles.adoc

:_mod-docs-content-type: PROCEDURE
[id="running-the-performance-profile-creator-wrapper-script_{context}"]
= Running the Performance Profile Creator wrapper script

The performance profile wrapper script simplifies the running of the Performance Profile Creator (PPC) tool. It hides the complexities associated with running `podman` and specifying the mapping directories and it enables the creation of the performance profile.

.Prerequisites

* Access to the Node Tuning Operator image.
* Access to the `must-gather` tarball.

.Procedure

. Create a file on your local machine named, for example, `run-perf-profile-creator.sh`:
+
[source,terminal]
----
$ vi run-perf-profile-creator.sh
----

. Paste the following code into the file:
+
[source,bash,subs="attributes+"]
----
#!/bin/bash

readonly CONTAINER_RUNTIME=${CONTAINER_RUNTIME:-podman}
readonly CURRENT_SCRIPT=$(basename "$0")
readonly CMD="${CONTAINER_RUNTIME} run --entrypoint performance-profile-creator"
readonly IMG_EXISTS_CMD="${CONTAINER_RUNTIME} image exists"
readonly IMG_PULL_CMD="${CONTAINER_RUNTIME} image pull"
readonly MUST_GATHER_VOL="/must-gather"

NTO_IMG="registry.redhat.io/openshift4/ose-cluster-node-tuning-operator:v{product-version}"
MG_TARBALL=""
DATA_DIR=""

usage() {
  print "Wrapper usage:"
  print "  ${CURRENT_SCRIPT} [-h] [-p image][-t path] -- [performance-profile-creator flags]"
  print ""
  print "Options:"
  print "   -h                 help for ${CURRENT_SCRIPT}"
  print "   -p                 Node Tuning Operator image"
  print "   -t                 path to a must-gather tarball"

  ${IMG_EXISTS_CMD} "${NTO_IMG}" && ${CMD} "${NTO_IMG}" -h
}

function cleanup {
  [ -d "${DATA_DIR}" ] && rm -rf "${DATA_DIR}"
}
trap cleanup EXIT

exit_error() {
  print "error: $*"
  usage
  exit 1
}

print() {
  echo  "$*" >&2
}

check_requirements() {
  ${IMG_EXISTS_CMD} "${NTO_IMG}" || ${IMG_PULL_CMD} "${NTO_IMG}" || \
      exit_error "Node Tuning Operator image not found"

  [ -n "${MG_TARBALL}" ] || exit_error "Must-gather tarball file path is mandatory"
  [ -f "${MG_TARBALL}" ] || exit_error "Must-gather tarball file not found"

  DATA_DIR=$(mktemp -d -t "${CURRENT_SCRIPT}XXXX") || exit_error "Cannot create the data directory"
  tar -zxf "${MG_TARBALL}" --directory "${DATA_DIR}" || exit_error "Cannot decompress the must-gather tarball"
  chmod a+rx "${DATA_DIR}"

  return 0
}

main() {
  while getopts ':hp:t:' OPT; do
    case "${OPT}" in
      h)
        usage
        exit 0
        ;;
      p)
        NTO_IMG="${OPTARG}"
        ;;
      t)
        MG_TARBALL="${OPTARG}"
        ;;
      ?)
        exit_error "invalid argument: ${OPTARG}"
        ;;
    esac
  done
  shift $((OPTIND - 1))

  check_requirements || exit 1

  ${CMD} -v "${DATA_DIR}:${MUST_GATHER_VOL}:z" "${NTO_IMG}" "$@" --must-gather-dir-path "${MUST_GATHER_VOL}"
  echo "" 1>&2
}

main "$@"
----

. Add execute permissions for everyone on this script:
+
[source,terminal]
----
$ chmod a+x run-perf-profile-creator.sh
----

. Optional: Display the `run-perf-profile-creator.sh` command usage:
+
[source,terminal]
----
$ ./run-perf-profile-creator.sh -h
----
+
.Expected output
+
[source,terminal]
----
Wrapper usage:
  run-perf-profile-creator.sh [-h] [-p image][-t path] -- [performance-profile-creator flags]

Options:
   -h                 help for run-perf-profile-creator.sh
   -p                 Node Tuning Operator image <1>
   -t                 path to a must-gather tarball <2>
A tool that automates creation of Performance Profiles

Usage:
  performance-profile-creator [flags]

Flags:
      --disable-ht                        Disable Hyperthreading
  -h, --help                              help for performance-profile-creator
      --info string                       Show cluster information; requires --must-gather-dir-path, ignore the other arguments. [Valid values: log, json] (default "log")
      --mcp-name string                   MCP name corresponding to the target machines (required)
      --must-gather-dir-path string       Must gather directory path (default "must-gather")
      --offlined-cpu-count int            Number of offlined CPUs
      --per-pod-power-management          Enable Per Pod Power Management
      --power-consumption-mode string     The power consumption mode.  [Valid values: default, low-latency, ultra-low-latency] (default "default")
      --profile-name string               Name of the performance profile to be created (default "performance")
      --reserved-cpu-count int            Number of reserved CPUs (required)
      --rt-kernel                         Enable Real Time Kernel (required)
      --split-reserved-cpus-across-numa   Split the Reserved CPUs across NUMA nodes
      --topology-manager-policy string    Kubelet Topology Manager Policy of the performance profile to be created. [Valid values: single-numa-node, best-effort, restricted] (default "restricted")
      --user-level-networking             Run with User level Networking(DPDK) enabled
----
+
[NOTE]
====
There two types of arguments:

* Wrapper arguments namely `-h`, `-p` and `-t`
* PPC arguments
====
+
<1> Optional: Specify the Node Tuning Operator image. If not set, the default upstream image is used: `registry.redhat.io/openshift4/ose-cluster-node-tuning-operator:v{product-version}`.
<2> `-t` is a required wrapper script argument and specifies the path to a `must-gather` tarball.

. Run the performance profile creator tool in discovery mode:
+
[NOTE]
====
Discovery mode inspects your cluster using the output from `must-gather`. The output produced includes information on:

* The NUMA cell partitioning with the allocated CPU IDs
* Whether hyperthreading is enabled

Using this information you can set appropriate values for some of the arguments supplied to the Performance Profile Creator tool.
====
+
[source,terminal]
----
$ ./run-perf-profile-creator.sh -t /must-gather/must-gather.tar.gz -- --info=log
----
+
[NOTE]
====
The `info` option requires a value which specifies the output format. Possible values are log and JSON. The JSON format is reserved for debugging.
====

. Check the machine config pool:
+
[source,terminal]
----
$ oc get mcp
----
+
.Example output

[source,terminal]
----
NAME         CONFIG                                                 UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
master       rendered-master-acd1358917e9f98cbdb599aea622d78b       True      False      False      3              3                   3                     0                      22h
worker-cnf   rendered-worker-cnf-1d871ac76e1951d32b2fe92369879826   False     True       False      2              1                   1                     0                      22h
----

. Create a performance profile:
+
[source,terminal]
----
$ ./run-perf-profile-creator.sh -t /must-gather/must-gather.tar.gz -- --mcp-name=worker-cnf --reserved-cpu-count=2 --rt-kernel=true > my-performance-profile.yaml
----
+
[NOTE]
====
The Performance Profile Creator arguments are shown in the Performance Profile Creator arguments table. The following arguments are required:

* `reserved-cpu-count`
* `mcp-name`
* `rt-kernel`

The `mcp-name` argument in this example is set to `worker-cnf` based on the output of the command `oc get mcp`. For {sno} use `--mcp-name=master`.
====

. Review the created YAML file:
+
[source,terminal]
----
$ cat my-performance-profile.yaml
----
.Example output
+
[source,terminal]
----
apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
  name: performance
spec:
  cpu:
    isolated: 1-39,41-79
    reserved: 0,40
  nodeSelector:
    node-role.kubernetes.io/worker-cnf: ""
  numa:
    topologyPolicy: restricted
  realTimeKernel:
    enabled: false
----

. Apply the generated profile:
+
[NOTE]
====
Install the Node Tuning Operator before applying the profile.
====

+
[source,terminal]
----
$ oc apply -f my-performance-profile.yaml
----

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
// Epic CNF-792 (4.8)
// * scalability_and_performance/cnf-create-performance-profiles.adoc


[id="performance-profile-creator-arguments_{context}"]
= Performance Profile Creator arguments

.Performance Profile Creator arguments
[cols="30%,70%",options="header"]
|===
| Argument | Description

| `disable-ht`
a|Disable hyperthreading.

Possible values: `true` or `false`.

Default: `false`.

[WARNING]
====
If this argument is set to `true` you should not disable hyperthreading in the BIOS. Disabling hyperthreading is accomplished with a kernel command line argument.
====

| `info`
a| This captures cluster information and is used in discovery mode only. Discovery mode also requires the `must-gather-dir-path` argument. If any other arguments are set they are ignored.

Possible values:

* `log`
* `JSON`

+
[NOTE]
====
These options define the output format with the JSON format being reserved for debugging.
====

Default: `log`.

| `mcp-name`
|MCP name for example `worker-cnf` corresponding to the target machines. This parameter is required.

| `must-gather-dir-path`
| Must gather directory path. This parameter is required.

When the user runs the tool with the wrapper script `must-gather` is supplied by the script itself and the user must not specify it.

| `offlined-cpu-count`
a| Number of offlined CPUs.

[NOTE]
====
This must be a natural number greater than 0. If not enough logical processors are offlined then error messages are logged. The messages are:
[source,terminal]
----
Error: failed to compute the reserved and isolated CPUs: please ensure that reserved-cpu-count plus offlined-cpu-count should be in the range [0,1]
----
[source,terminal]
----
Error: failed to compute the reserved and isolated CPUs: please specify the offlined CPU count in the range [0,1]
----
====

| `power-consumption-mode`
a|The power consumption mode.

Possible values:

* `default`: CPU partitioning with enabled power management and basic low-latency.
* `low-latency`: Enhanced measures to improve latency figures.
* `ultra-low-latency`: Priority given to optimal latency, at the expense of power management.

Default: `default`.

| `per-pod-power-management`
a|Enable per pod power management. You cannot use this argument if you configured `ultra-low-latency` as the power consumption mode.

Possible values: `true` or `false`.

Default: `false`.

| `profile-name`
| Name of the performance profile to create.
Default: `performance`.

| `reserved-cpu-count`
a| Number of reserved CPUs. This parameter is required.

[NOTE]
====
This must be a natural number. A value of 0 is not allowed.
====

| `rt-kernel`
| Enable real-time kernel. This parameter is required.

Possible values: `true` or `false`.

| `split-reserved-cpus-across-numa`
| Split the reserved CPUs across NUMA nodes.

Possible values: `true` or `false`.

Default: `false`.

| `topology-manager-policy`
a| Kubelet Topology Manager policy of the performance profile to be created.

Possible values:

* `single-numa-node`
* `best-effort`
* `restricted`

Default: `restricted`.

| `user-level-networking`
| Run with user level networking (DPDK) enabled.

Possible values: `true` or `false`.

Default: `false`.
|===

:leveloffset!:

[id="cnf-create-performance-profiles-reference"]
== Reference performance profiles

:leveloffset: +2

// Module included in the following assemblies:
//
// * scalability_and_performance/cnf-create-performance-profiles.adoc
// TODO: ^^^ at the moment

:_mod-docs-content-type: REFERENCE
[id="installation-openstack-ovs-dpdk-performance-profile_{context}"]
= A performance profile template for clusters that use OVS-DPDK on OpenStack

To maximize machine performance in a cluster that uses Open vSwitch with the Data Plane Development Kit (OVS-DPDK) on {rh-openstack-first}, you can use a performance profile.

You can use the following performance profile template to create a profile for your deployment.

.A performance profile template for clusters that use OVS-DPDK
[source,yaml]
----
apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
  name: cnf-performanceprofile
spec:
  additionalKernelArgs:
    - nmi_watchdog=0
    - audit=0
    - mce=off
    - processor.max_cstate=1
    - idle=poll
    - intel_idle.max_cstate=0
    - default_hugepagesz=1GB
    - hugepagesz=1G
    - intel_iommu=on
  cpu:
    isolated: <CPU_ISOLATED>
    reserved: <CPU_RESERVED>
  hugepages:
    defaultHugepagesSize: 1G
    pages:
      - count: <HUGEPAGES_COUNT>
        node: 0
        size: 1G
  nodeSelector:
    node-role.kubernetes.io/worker: ''
  realTimeKernel:
    enabled: false
    globallyDisableIrqLoadBalancing: true
----

Insert values that are appropriate for your configuration for the `CPU_ISOLATED`, `CPU_RESERVED`, and `HUGEPAGES_COUNT` keys.

To learn how to create and use performance profiles, see the "Creating a performance profile" page in the "Scalability and performance" section of the {product-title} documentation.

:leveloffset!:

[id="{context}-additional-resources"]
[role="_additional-resources"]
== Additional resources
* For more information about the `must-gather` tool,
see xref:../support/gathering-cluster-data.adoc#nodes-nodes-managing[Gathering data about your cluster].

//# includes=_attributes/common-attributes,modules/cnf-about-the-profile-creator-tool,modules/cnf-gathering-data-about-cluster-using-must-gather,modules/cnf-running-the-performance-creator-profile,modules/cnf-how-run-podman-to-create-profile,modules/cnf-running-the-performance-creator-profile-offline,modules/cnf-performance-profile-creator-arguments,modules/installation-openstack-ovs-dpdk-performance-profile

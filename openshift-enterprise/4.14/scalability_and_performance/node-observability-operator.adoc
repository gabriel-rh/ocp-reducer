:_mod-docs-content-type: ASSEMBLY
[id="using-node-observability-operator"]
= Requesting CRI-O and Kubelet profiling data by using the Node Observability Operator
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
:context: node-observability-operator

toc::[]


The Node Observability Operator collects and stores the CRI-O and Kubelet profiling data of worker nodes. You can query the profiling data to analyze the CRI-O and Kubelet performance trends and debug the performance-related issues.

:FeatureName: The Node Observability Operator
:leveloffset: +0

// When including this file, ensure that {FeatureName} is set immediately before
// the include. Otherwise it will result in an incorrect replacement.

[IMPORTANT]
====
[subs="attributes+"]
{FeatureName} is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.

For more information about the support scope of Red Hat Technology Preview features, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].
====
// Undefine {FeatureName} attribute, so that any mistakes are easily spotted
:!FeatureName:

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * scalability_and_performance/understanding-node-observability-operator.adoc

:_mod-docs-content-type: CONCEPT
[id="workflow-node-observability-operator_{context}"]
= Workflow of the Node Observability Operator

The following workflow outlines on how to query the profiling data using the Node Observability Operator:

. Install the Node Observability Operator in the {product-title} cluster.
. Create a NodeObservability custom resource to enable the CRI-O profiling on the worker nodes of your choice.
. Run the profiling query to generate the profiling data.

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * scalability_and_performance/understanding-node-observability-operator.adoc

:_mod-docs-content-type: CONCEPT
[id="install-node-observability-operator_{context}"]
= Installing the Node Observability Operator
The Node Observability Operator is not installed in {product-title} by default. You can install the Node Observability Operator by using the {product-title} CLI or the web console.

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * scalability_and_performance/understanding-node-observability-operator.adoc

:_mod-docs-content-type: PROCEDURE
[id="install-node-observability-using-cli_{context}"]
= Installing the Node Observability Operator using the CLI

You can install the Node Observability Operator by using the OpenShift CLI (oc).

.Prerequisites

* You have installed the OpenShift CLI (oc).
* You have access to the cluster with `cluster-admin` privileges.

.Procedure

. Confirm that the Node Observability Operator is available by running the following command:
+
[source,terminal]
----
$ oc get packagemanifests -n openshift-marketplace node-observability-operator
----

+
.Example output
[source,terminal]
----
NAME                            CATALOG                AGE
node-observability-operator     Red Hat Operators      9h
----

. Create the `node-observability-operator` namespace by running the following command:
+
[source,terminal]
----
$ oc new-project node-observability-operator
----

. Create an `OperatorGroup` object YAML file:
+
[source,yaml]
----
cat <<EOF | oc apply -f -
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: node-observability-operator
  namespace: node-observability-operator
spec:
  targetNamespaces: []
EOF
----

. Create a `Subscription` object YAML file to subscribe a namespace to an Operator:
+
[source,yaml]
----
cat <<EOF | oc apply -f -
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: node-observability-operator
  namespace: node-observability-operator
spec:
  channel: alpha
  name: node-observability-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
EOF
----

.Verification

. View the install plan name by running the following command:
+
[source,terminal]
----
$ oc -n node-observability-operator get sub node-observability-operator -o yaml | yq '.status.installplan.name'
----

+
.Example output
[source,terminal]
----
install-dt54w
----

. Verify the install plan status by running the following command:
+
[source,terminal]
----
$ oc -n node-observability-operator get ip <install_plan_name> -o yaml | yq '.status.phase'
----
+
`<install_plan_name>` is the install plan name that you obtained from the output of the previous command.

+
.Example output
[source,terminal]
----
COMPLETE
----

. Verify that the Node Observability Operator is up and running:
+
[source,terminal]
----
$ oc get deploy -n node-observability-operator
----

+
.Example output
[source,terminal]
----
NAME                                            READY   UP-TO-DATE  AVAILABLE   AGE
node-observability-operator-controller-manager  1/1     1           1           40h
----

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * scalability_and_performance/understanding-node-observability-operator.adoc

:_mod-docs-content-type: PROCEDURE
[id="install-node-observability-using-web-console_{context}"]
= Installing the Node Observability Operator using the web console

You can install the Node Observability Operator from the {product-title} web console.

.Prerequisites

* You have access to the cluster with `cluster-admin` privileges.
* You have access to the {product-title} web console.

.Procedure

. Log in to the {product-title} web console.
. In the Administrator's navigation panel, expand *Operators* → *OperatorHub*.
. In the *All items* field, enter *Node Observability Operator* and select the *Node Observability Operator* tile.
. Click *Install*.
. On the *Install Operator* page, configure the following settings:
.. In the *Update channel* area, click *alpha*.
.. In the *Installation mode* area, click *A specific namespace on the cluster*.
.. From the *Installed Namespace* list, select *node-observability-operator* from the list.
.. In the *Update approval* area, select *Automatic*.
.. Click *Install*.

.Verification
. In the Administrator's navigation panel, expand *Operators* → *Installed Operators*.
. Verify that the Node Observability Operator is listed in the Operators list.

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * scalability_and_performance/understanding-node-observability-operator.adoc

:_mod-docs-content-type: PROCEDURE
[id="creating-node-observability-custom-resource_{context}"]
= Creating the Node Observability custom resource

You must create and run the `NodeObservability` custom resource (CR) before you run the profiling query. When you run the `NodeObservability` CR, it creates the necessary machine config and machine config pool CRs to enable the CRI-O profiling on the worker nodes matching the `nodeSelector`.

[IMPORTANT]
====
If CRI-O profiling is not enabled on the worker nodes, the `NodeObservabilityMachineConfig` resource gets created. Worker nodes matching the `nodeSelector` specified in `NodeObservability` CR restarts. This might take 10 or more minutes to complete.
====

[NOTE]
====
Kubelet profiling is enabled by default.
====

The CRI-O unix socket of the node is mounted on the agent pod, which allows the agent to communicate with CRI-O to run the pprof request. Similarly, the `kubelet-serving-ca` certificate chain is mounted on the agent pod, which allows secure communication between the agent and node's kubelet endpoint.

.Prerequisites
* You have installed the Node Observability Operator.
* You have installed the OpenShift CLI (oc).
* You have access to the cluster with `cluster-admin` privileges.

.Procedure

. Log in to the {product-title} CLI by running the following command:
+
[source,terminal]
----
$ oc login -u kubeadmin https://<HOSTNAME>:6443
----

. Switch back to the `node-observability-operator` namespace by running the following command:
+
[source,terminal]
----
$ oc project node-observability-operator
----

. Create a CR file named `nodeobservability.yaml` that contains the following text:
+
[source,yaml]
----
    apiVersion: nodeobservability.olm.openshift.io/v1alpha2
    kind: NodeObservability
    metadata:
      name: cluster <1>
    spec:
      nodeSelector:
        kubernetes.io/hostname: <node_hostname> <2>
      type: crio-kubelet
----
<1> You must specify the name as `cluster` because there should be only one `NodeObservability` CR per cluster.
<2> Specify the nodes on which the Node Observability agent must be deployed.

. Run the `NodeObservability` CR:
+
[source,terminal]
----
oc apply -f nodeobservability.yaml
----

+
.Example output
[source,terminal]
----
nodeobservability.olm.openshift.io/cluster created
----

. Review the status of the `NodeObservability` CR by running the following command:
+
[source,terminal]
----
$ oc get nob/cluster -o yaml | yq '.status.conditions'
----

+
.Example output
[source,terminal]
----
conditions:
  conditions:
  - lastTransitionTime: "2022-07-05T07:33:54Z"
    message: 'DaemonSet node-observability-ds ready: true NodeObservabilityMachineConfig
      ready: true'
    reason: Ready
    status: "True"
    type: Ready
----

+
`NodeObservability` CR run is completed when the reason is `Ready` and the status is `True`.

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * scalability_and_performance/understanding-node-observability-operator.adoc

:_mod-docs-content-type: PROCEDURE
[id="running-profiling-query_{context}"]
= Running the profiling query

To run the profiling query, you must create a `NodeObservabilityRun` resource. The profiling query is a blocking operation that fetches CRI-O and Kubelet profiling data for a duration of 30 seconds. After the profiling query is complete, you must retrieve the profiling data inside the container file system `/run/node-observability` directory. The lifetime of data is bound to the agent pod through the `emptyDir` volume, so you can access the profiling data while the agent pod is in the `running` status.

[IMPORTANT]
====
You can request only one profiling query at any point of time.
====

.Prerequisites
* You have installed the Node Observability Operator.
* You have created the `NodeObservability` custom resource (CR).
* You have access to the cluster with `cluster-admin` privileges.

.Procedure

. Create a `NodeObservabilityRun` resource file named `nodeobservabilityrun.yaml` that contains the following text:
+
[source,yaml]
----
apiVersion: nodeobservability.olm.openshift.io/v1alpha2
kind: NodeObservabilityRun
metadata:
  name: nodeobservabilityrun
spec:
  nodeObservabilityRef:
    name: cluster
----

. Trigger the profiling query by running the `NodeObservabilityRun` resource:
+
[source,terminal]
----
$ oc apply -f nodeobservabilityrun.yaml
----

. Review the status of the `NodeObservabilityRun` by running the following command:
+
[source,terminal]
----
$ oc get nodeobservabilityrun nodeobservabilityrun -o yaml  | yq '.status.conditions'
----

+
.Example output
[source,terminal]
----
conditions:
- lastTransitionTime: "2022-07-07T14:57:34Z"
  message: Ready to start profiling
  reason: Ready
  status: "True"
  type: Ready
- lastTransitionTime: "2022-07-07T14:58:10Z"
  message: Profiling query done
  reason: Finished
  status: "True"
  type: Finished
----

+
The profiling query is complete once the status is `True` and type is `Finished`.

. Retrieve the profiling data from the container's `/run/node-observability` path by running the following bash script:
+
[source,bash]
----
for a in $(oc get nodeobservabilityrun nodeobservabilityrun -o yaml | yq .status.agents[].name); do
  echo "agent ${a}"
  mkdir -p "/tmp/${a}"
  for p in $(oc exec "${a}" -c node-observability-agent -- bash -c "ls /run/node-observability/*.pprof"); do
    f="$(basename ${p})"
    echo "copying ${f} to /tmp/${a}/${f}"
    oc exec "${a}" -c node-observability-agent -- cat "${p}" > "/tmp/${a}/${f}"
  done
done
----

:leveloffset!:

//# includes=_attributes/common-attributes,snippets/technology-preview,modules/node-observability-high-level-workflow,modules/node-observability-installation,modules/node-observability-install-cli,modules/node-observability-install-web-console,modules/node-observability-create-custom-resource,modules/node-observability-run-profiling-query

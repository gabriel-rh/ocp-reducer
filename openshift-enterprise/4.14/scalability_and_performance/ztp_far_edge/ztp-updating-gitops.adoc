:_mod-docs-content-type: ASSEMBLY
[id="ztp-updating-gitops"]
= Updating {ztp}
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
:context: ztp-updating-gitops

toc::[]

You can update the {ztp-first} infrastructure independently from the hub cluster, {rh-rhacm-first}, and the managed {product-title} clusters.

[NOTE]
====
You can update the {gitops-title} Operator when new versions become available. When updating the {ztp} plugin, review the updated files in the reference configuration and ensure that the changes meet your requirements.
====

:leveloffset: +1

// Module included in the following assemblies:
//
// * scalability_and_performance/ztp_far_edge/ztp-updating-gitops.adoc

:_mod-docs-content-type: PROCEDURE
[id="ztp-updating-gitops-ztp_{context}"]
= Overview of the {ztp} update process

You can update {ztp-first} for a fully operational hub cluster running an earlier version of the {ztp} infrastructure. The update process avoids impact on managed clusters.

[NOTE]
====
Any changes to policy settings, including adding recommended content, results in updated polices that must be rolled out to the managed clusters and reconciled.
====

At a high level, the strategy for updating the {ztp} infrastructure is as follows:

. Label all existing clusters with the `ztp-done` label.

. Stop the ArgoCD applications.

. Install the new {ztp} tools.

. Update required content and optional changes in the Git repository.

. Update and restart the application configuration.

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * scalability_and_performance/ztp_far_edge/ztp-updating-gitops.adoc

:_mod-docs-content-type: PROCEDURE
[id="ztp-preparing-for-the-gitops-ztp-upgrade_{context}"]
= Preparing for the upgrade

Use the following procedure to prepare your site for the {ztp-first} upgrade.

.Procedure

. Get the latest version of the {ztp} container that has the custom resources (CRs) used to configure {gitops-title} for use with {ztp}.

. Extract the `argocd/deployment` directory by using the following commands:
+
[source,terminal]
----
$ mkdir -p ./update
----
+
[source,terminal,subs="attributes+"]
----
$ podman run --log-driver=none --rm registry.redhat.io/openshift4/ztp-site-generate-rhel8:v{product-version} extract /home/ztp --tar | tar x -C ./update
----
+
The `/update` directory contains the following subdirectories:
+
* `update/extra-manifest`: contains the source CR files that the `SiteConfig` CR uses to generate the extra manifest `configMap`.
* `update/source-crs`: contains the source CR files that the `PolicyGenTemplate` CR uses to generate the {rh-rhacm-first} policies.
* `update/argocd/deployment`: contains patches and YAML files to apply on the hub cluster for use in the next step of this procedure.
* `update/argocd/example`: contains example `SiteConfig` and `PolicyGenTemplate` files that represent the recommended configuration.

. Update the `clusters-app.yaml` and `policies-app.yaml` files to reflect the name of your applications and the URL, branch, and path for your Git repository.
+
If the upgrade includes changes that results in obsolete policies, the obsolete policies should be removed prior to performing the upgrade.

. Diff the changes between the configuration and deployment source CRs in the `/update` folder and Git repo where you manage your fleet site CRs. Apply and push the required changes to your site repository.
+
[IMPORTANT]
====
When you update {ztp} to the latest version, you must apply the changes from the `update/argocd/deployment` directory to your site repository. Do not use older versions of the `argocd/deployment/` files.
====

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * scalability_and_performance/ztp_far_edge/ztp-updating-gitops.adoc

:_mod-docs-content-type: PROCEDURE
[id="ztp-labeling-the-existing-clusters_{context}"]
= Labeling the existing clusters

To ensure that existing clusters remain untouched by the tool updates, label all existing managed clusters with the `ztp-done` label.

[NOTE]
====
This procedure only applies when updating clusters that were not provisioned with {cgu-operator-first}. Clusters that you provision with {cgu-operator} are automatically labeled with `ztp-done`.
====

.Procedure

. Find a label selector that lists the managed clusters that were deployed with {ztp-first}, such as `local-cluster!=true`:
+
[source,terminal]
----
$ oc get managedcluster -l 'local-cluster!=true'
----

. Ensure that the resulting list contains all the managed clusters that were deployed with {ztp}, and then use that selector to add the `ztp-done` label:
+
[source,terminal]
----
$ oc label managedcluster -l 'local-cluster!=true' ztp-done=
----

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * scalability_and_performance/ztp_far_edge/ztp-updating-gitops.adoc

:_mod-docs-content-type: PROCEDURE
[id="ztp-stopping-the-existing-gitops-ztp-applications_{context}"]
= Stopping the existing {ztp} applications

Removing the existing applications ensures that any changes to existing content in the Git repository are not rolled out until the new version of the tools is available.

Use the application files from the `deployment` directory. If you used custom names for the applications, update the names in these files first.

.Procedure

. Perform a non-cascaded delete on the `clusters` application to leave all generated resources in place:
+
[source,terminal]
----
$ oc delete -f update/argocd/deployment/clusters-app.yaml
----

. Perform a cascaded delete on the `policies` application to remove all previous policies:
+
[source,terminal]
----
$ oc patch -f policies-app.yaml -p '{"metadata": {"finalizers": ["resources-finalizer.argocd.argoproj.io"]}}' --type merge
----
+
[source,terminal]
----
$ oc delete -f update/argocd/deployment/policies-app.yaml
----

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * scalability_and_performance/ztp_far_edge/ztp-updating-gitops.adoc

:_mod-docs-content-type: CONCEPT
[id="ztp-required-changes-to-the-git-repository_{context}"]
= Required changes to the Git repository

When upgrading the `ztp-site-generate` container from an earlier release of {ztp-first} to 4.10 or later, there are additional requirements for the contents of the Git repository. Existing content in the repository must be updated to reflect these changes.

* Make required changes to `PolicyGenTemplate` files:
+
All `PolicyGenTemplate` files must be created in a `Namespace` prefixed with `ztp`. This ensures that the {ztp} application is able to manage the policy CRs generated by {ztp} without conflicting with the way {rh-rhacm-first} manages the policies internally.

* Add the `kustomization.yaml` file to the repository:
+
All `SiteConfig` and `PolicyGenTemplate` CRs must be included in a `kustomization.yaml` file under their respective directory trees. For example:
+
[source,terminal]
----
├── policygentemplates
│   ├── site1-ns.yaml
│   ├── site1.yaml
│   ├── site2-ns.yaml
│   ├── site2.yaml
│   ├── common-ns.yaml
│   ├── common-ranGen.yaml
│   ├── group-du-sno-ranGen-ns.yaml
│   ├── group-du-sno-ranGen.yaml
│   └── kustomization.yaml
└── siteconfig
    ├── site1.yaml
    ├── site2.yaml
    └── kustomization.yaml
----
+
[NOTE]
====
The files listed in the `generator` sections must contain either `SiteConfig` or `PolicyGenTemplate` CRs only. If your existing YAML files contain other CRs, for example, `Namespace`, these other CRs must be pulled out into separate files and listed in the `resources` section.
====
+
The `PolicyGenTemplate` kustomization file must contain all `PolicyGenTemplate` YAML files in the `generator` section and `Namespace` CRs in the `resources` section. For example:
+
[source,yaml]
----
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

generators:
- common-ranGen.yaml
- group-du-sno-ranGen.yaml
- site1.yaml
- site2.yaml

resources:
- common-ns.yaml
- group-du-sno-ranGen-ns.yaml
- site1-ns.yaml
- site2-ns.yaml
----
+
The `SiteConfig` kustomization file must contain all `SiteConfig` YAML files in the `generator` section and any other CRs in the resources:
+
[source,terminal]
----
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

generators:
- site1.yaml
- site2.yaml
----

* Remove the `pre-sync.yaml` and `post-sync.yaml` files.
+
In {product-title} 4.10 and later, the `pre-sync.yaml` and `post-sync.yaml` files are no longer required. The `update/deployment/kustomization.yaml` CR manages the policies deployment on the hub cluster.
+
[NOTE]
====
There is a set of `pre-sync.yaml` and `post-sync.yaml` files under both the `SiteConfig` and `PolicyGenTemplate` trees.
====

* Review and incorporate recommended changes
+
Each release may include additional recommended changes to the configuration applied to deployed clusters. Typically these changes result in lower CPU use by the OpenShift platform, additional features, or improved tuning of the platform.
+
Review the reference `SiteConfig` and `PolicyGenTemplate` CRs applicable to the types of cluster in your network. These examples can be found in the `argocd/example` directory extracted from the {ztp} container.

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * scalability_and_performance/ztp_far_edge/ztp-updating-gitops.adoc

:_mod-docs-content-type: PROCEDURE
[id="ztp-installing-the-new-gitops-ztp-applications_{context}"]
= Installing the new {ztp} applications

Using the extracted `argocd/deployment` directory, and after ensuring that the applications point to your site Git repository, apply the full contents of the deployment directory. Applying the full contents of the directory ensures that all necessary resources for the applications are correctly configured.

.Procedure

. To patch the ArgoCD instance in the hub cluster by using the patch file that you previously extracted into the `update/argocd/deployment/` directory, enter the following command:
+
[source,terminal]
----
$ oc patch argocd openshift-gitops \
-n openshift-gitops --type=merge \
--patch-file update/argocd/deployment/argocd-openshift-gitops-patch.json
----

. To apply the contents of the `argocd/deployment` directory, enter the following command:
+
[source,terminal]
----
$ oc apply -k update/argocd/deployment
----

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * scalability_and_performance/ztp_far_edge/ztp-updating-gitops.adoc

:_mod-docs-content-type: CONCEPT
[id="ztp-roll-out-the-configuration-changes_{context}"]
= Rolling out the {ztp} configuration changes

If any configuration changes were included in the upgrade due to implementing recommended changes, the upgrade process results in a set of policy CRs on the hub cluster in the `Non-Compliant` state. With the {ztp-first} version 4.10 and later `ztp-site-generate` container, these policies are set to `inform` mode and are not pushed to the managed clusters without an additional step by the user. This ensures that potentially disruptive changes to the clusters can be managed in terms of when the changes are made, for example, during a maintenance window, and how many clusters are updated concurrently.

To roll out the changes, create one or more `ClusterGroupUpgrade` CRs as detailed in the {cgu-operator} documentation. The CR must contain the list of `Non-Compliant` policies that you want to push out to the managed clusters as well as a list or selector of which clusters should be included in the update.

:leveloffset!:

[role="_additional-resources"]
.Additional resources

* For information about the {cgu-operator-first}, see xref:../../scalability_and_performance/ztp_far_edge/cnf-talm-for-cluster-upgrades.adoc#cnf-about-topology-aware-lifecycle-manager-config_cnf-topology-aware-lifecycle-manager[About the {cgu-operator-full} configuration].

* For information about creating `ClusterGroupUpgrade` CRs, see xref:../../scalability_and_performance/ztp_far_edge/ztp-talm-updating-managed-policies.adoc#talo-precache-autocreated-cgu-for-ztp_ztp-talm[About the auto-created ClusterGroupUpgrade CR for ZTP].

//# includes=_attributes/common-attributes,modules/ztp-updating-gitops-ztp,modules/ztp-preparing-for-the-gitops-ztp-upgrade,modules/ztp-labeling-the-existing-clusters,modules/ztp-stopping-the-existing-gitops-ztp-applications,modules/ztp-required-changes-to-the-git-repository,modules/ztp-installing-the-new-gitops-ztp-applications,modules/ztp-roll-out-the-configuration-changes

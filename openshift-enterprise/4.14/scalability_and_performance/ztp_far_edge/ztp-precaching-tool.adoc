:_mod-docs-content-type: ASSEMBLY
[id="ztp-pre-staging-tool"]
= Pre-caching images for {sno} deployments
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
:context: pre-caching

toc::[]

In environments with limited bandwidth where you use the {ztp-first} solution to deploy a large number of clusters, you want to avoid downloading all the images that are required for bootstrapping and installing {product-title}.
The limited bandwidth at remote {sno} sites can cause long deployment times.
The {factory-prestaging-tool} allows you to pre-stage servers before shipping them to the remote site for ZTP provisioning.

The {factory-prestaging-tool} does the following:

* Downloads the RHCOS rootfs image that is required by the minimal ISO to boot.
* Creates a partition from the installation disk labelled as `data`.
* Formats the disk in xfs.
* Creates a GUID Partition Table (GPT) data partition at the end of the disk, where the size of the partition is configurable by the tool.
* Copies the container images required to install {product-title}.
* Copies the container images required by ZTP to install {product-title}.
* Optional: Copies Day-2 Operators to the partition.

:FeatureName: The factory-precaching-cli tool
// When including this file, ensure that {FeatureName} is set immediately before
// the include. Otherwise it will result in an incorrect replacement.

[IMPORTANT]
====
[subs="attributes+"]
{FeatureName} is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.

For more information about the support scope of Red Hat Technology Preview features, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].
====
// Undefine {FeatureName} attribute, so that any mistakes are easily spotted
:!FeatureName:

:leveloffset: +1

// Module included in the following assemblies:
//
// * scalability_and_performance/ztp_far_edge/ztp-precaching-tool.adoc

:_mod-docs-content-type: PROCEDURE
[id="ztp-getting-tool_{context}"]
= Getting the {factory-prestaging-tool}

The {factory-prestaging-tool} Go binary is publicly available in link:https://quay.io/openshift-kni/telco-ran-tools:latest[the Telco RAN tools container image].
The {factory-prestaging-tool} Go binary in the container image is executed on the server running an {op-system} live image using `podman`.
If you are working in a disconnected environment or have a private registry, you need to copy the image there so you can download the image to the server.

.Procedure

* Pull the {factory-prestaging-tool} image by running the following command:
+
[source,terminal]
----
# podman pull quay.io/openshift-kni/telco-ran-tools:latest
----

.Verification

* To check that the tool is available, query the current version of the {factory-prestaging-tool} Go binary:
+
[source,terminal]
----
# podman run quay.io/openshift-kni/telco-ran-tools:latest -- factory-precaching-cli -v
----

+
.Example output
[source,terminal]
----
factory-precaching-cli version 20221018.120852+main.feecf17
----

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * scalability_and_performance/ztp_far_edge/ztp-precaching-tool.adoc

:_mod-docs-content-type: PROCEDURE
[id="ztp-booting-from-live-os_{context}"]
= Booting from a live operating system image

You can use the {factory-prestaging-tool} with to boot servers where only one disk is available and external disk drive cannot be attached to the server.

[WARNING]
====
{op-system} requires the disk to not be in use when the disk is about to be written with an {op-system} image.
====

Depending on the server hardware, you can mount the {op-system} live ISO on the blank server using one of the following methods:

* Using the Dell RACADM tool on a Dell server.
* Using the HPONCFG tool on a HP server.
* Using the Redfish BMC API.

[NOTE]
====
It is recommended to automate the mounting procedure. To automate the procedure, you need to pull the required images and host them on a local HTTP server.
====

.Prerequisites

* You powered up the host.
* You have network connectivity to the host.

.Procedure

[NOTE]
====
This example procedure uses the Redfish BMC API to mount the {op-system} live ISO.
====

. Mount the {op-system} live ISO:

.. Check virtual media status:
+
[source,terminal]
----
$ curl --globoff -H "Content-Type: application/json" -H \
"Accept: application/json" -k -X GET --user ${username_password} \
https://$BMC_ADDRESS/redfish/v1/Managers/Self/VirtualMedia/1 | python -m json.tool
----

.. Mount the ISO file as a virtual media:
+
[source,terminal]
----
$ curl --globoff -L -w "%{http_code} %{url_effective}\\n" -ku ${username_password} -H "Content-Type: application/json" -H "Accept: application/json" -d '{"Image": "http://[$HTTPd_IP]/RHCOS-live.iso"}' -X POST https://$BMC_ADDRESS/redfish/v1/Managers/Self/VirtualMedia/1/Actions/VirtualMedia.InsertMedia
----

.. Set the boot order to boot from the virtual media once:
+
[source,terminal]
----
$ curl --globoff  -L -w "%{http_code} %{url_effective}\\n"  -ku ${username_password}  -H "Content-Type: application/json" -H "Accept: application/json" -d '{"Boot":{ "BootSourceOverrideEnabled": "Once", "BootSourceOverrideTarget": "Cd", "BootSourceOverrideMode": "UEFI"}}' -X PATCH https://$BMC_ADDRESS/redfish/v1/Systems/Self
----

. Reboot and ensure that the server is booting from virtual media.

:leveloffset!:

[role="_additional-resources"]
.Additional resources

* For more information about the `butane` utility, see xref:../../installing/install_config/installing-customizing.adoc#installation-special-config-butane-about_installing-customizing[About Butane].
* For more information about creating a custom live {op-system} ISO, see xref:../../installing/installing_sno/install-sno-installing-sno.adoc#create-custom-live-rhcos-iso_install-sno-installing-sno-with-the-assisted-installer[Creating a custom live {op-system} ISO for remote server access].
* For more information about using the Dell RACADM tool, see link:https://www.dell.com/support/manuals/en-ie/poweredge-r440/idrac9_6.xx_racadm_pub/supported-racadm-interfaces?guid=guid-a5747353-fc88-4438-b617-c50ca260448e&lang=en-us[Integrated Dell Remote Access Controller 9 RACADM CLI Guide].
* For more information about using the HP HPONCFG tool, see link:https://support.hpe.com/hpesc/public/docDisplay?docId=emr_na-a00007610en_us[Using HPONCFG].
* For more information about using the Redfish BMC API, see xref:../../installing/installing_sno/install-sno-installing-sno.adoc#install-booting-from-an-iso-over-http-redfish_install-sno-installing-sno-with-the-assisted-installer[Booting from an HTTP-hosted ISO image using the Redfish API].

:leveloffset: +1

// Module included in the following assemblies:
//
// * scalability_and_performance/ztp_far_edge/ztp-precaching-tool.adoc

:_module-type: PROCEDURE
[id="ztp-partitioning_{context}"]
= Partitioning the disk

To run the full pre-caching process, you have to boot from a live ISO and use the {factory-prestaging-tool} from a container image to partition and pre-cache all the artifacts required.

A live ISO or {op-system} live ISO is required because the disk must not be in use when the operating system ({op-system}) is written to the device during the provisioning.
Single-disk servers can also be enabled with this procedure.

.Prerequisites

* You have a disk that is not partitioned.
* You have access to the `quay.io/openshift-kni/telco-ran-tools:latest` image.
* You have enough storage to install {product-title} and pre-cache the required images.

.Procedure

. Verify that the disk is cleared:
+
[source,terminal]
----
# lsblk
----

+
.Example output
[source,terminal]
----
NAME    MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
loop0     7:0    0  93.8G  0 loop /run/ephemeral
loop1     7:1    0 897.3M  1 loop /sysroot
sr0      11:0    1   999M  0 rom  /run/media/iso
nvme0n1 259:1    0   1.5T  0 disk
----

. Erase any file system, RAID or partition table signatures from the device:
+
[source,terminal]
----
# wipefs -a /dev/nvme0n1
----

+
.Example output
[source,terminal]
----
/dev/nvme0n1: 8 bytes were erased at offset 0x00000200 (gpt): 45 46 49 20 50 41 52 54
/dev/nvme0n1: 8 bytes were erased at offset 0x1749a955e00 (gpt): 45 46 49 20 50 41 52 54
/dev/nvme0n1: 2 bytes were erased at offset 0x000001fe (PMBR): 55 aa
----

[IMPORTANT]
====
The tool fails if the disk is not empty because it uses partition number 1 of the device for pre-caching the artifacts.
====

[id="ztp-create-partition_{context}"]
== Creating the partition

Once the device is ready, you create a single partition and a GPT partition table.
The partition is automatically labelled as `data` and created at the end of the device.
Otherwise, the partition will be overridden by the `coreos-installer`.

[IMPORTANT]
====
The `coreos-installer` requires the partition to be created at the end of the device and to be labelled as `data`. Both requirements are necessary to save the partition when writing the {op-system} image to the disk.
====

.Prerequisites

* The container must run as `privileged` due to formatting host devices.
* You have to mount the `/dev` folder so that the process can be executed inside the container.

.Procedure

In the following example, the size of the partition is 250 GiB due to allow pre-caching the DU profile for Day 2 Operators.

. Run the container as `privileged` and partition the disk:
+
[source,terminal]
----
# podman run -v /dev:/dev --privileged \
--rm quay.io/openshift-kni/telco-ran-tools:latest -- \
factory-precaching-cli partition \ <1>
-d /dev/nvme0n1 \ <2>
-s 250 <3>
----
<1> Specifies the partitioning function of the {factory-prestaging-tool}.
<2> Defines the root directory on the disk.
<3> Defines the size of the disk in GB.

. Check the storage information:
+
[source,terminal]
----
# lsblk
----

+
.Example output
[source,terminal]
----
NAME        MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
loop0         7:0    0  93.8G  0 loop /run/ephemeral
loop1         7:1    0 897.3M  1 loop /sysroot
sr0          11:0    1   999M  0 rom  /run/media/iso
nvme0n1     259:1    0   1.5T  0 disk
└─nvme0n1p1 259:3    0   250G  0 part
----

.Verification

You must verify that the following requirements are met:

* The device has a GPT partition table
* The partition uses the latest sectors of the device.
* The partition is correctly labeled as `data`.

Query the disk status to verify that the disk is partitioned as expected:

[source,terminal]
----
# gdisk -l /dev/nvme0n1
----

.Example output
[source,terminal]
----
GPT fdisk (gdisk) version 1.0.3

Partition table scan:
  MBR: protective
  BSD: not present
  APM: not present
  GPT: present

Found valid GPT with protective MBR; using GPT.
Disk /dev/nvme0n1: 3125627568 sectors, 1.5 TiB
Model: Dell Express Flash PM1725b 1.6TB SFF
Sector size (logical/physical): 512/512 bytes
Disk identifier (GUID): CB5A9D44-9B3C-4174-A5C1-C64957910B61
Partition table holds up to 128 entries
Main partition table begins at sector 2 and ends at sector 33
First usable sector is 34, last usable sector is 3125627534
Partitions will be aligned on 2048-sector boundaries
Total free space is 2601338846 sectors (1.2 TiB)

Number  Start (sector)    End (sector)  Size       Code  Name
   1      2601338880      3125627534   250.0 GiB   8300  data
----

[id="ztp-mount-partition_{context}"]
== Mounting the partition

After verifying that the disk is partitioned correctly, you can mount the device into `/mnt`.

[IMPORTANT]
====
It is recommended to mount the device into `/mnt` because that mounting point is used during {ztp} preparation.
====

. Verify that the partition is formatted as `xfs`:
+
[source,terminal]
----
# lsblk -f /dev/nvme0n1
----

+
.Example output
[source,terminal]
----
NAME        FSTYPE LABEL UUID                                 MOUNTPOINT
nvme0n1
└─nvme0n1p1 xfs          1bee8ea4-d6cf-4339-b690-a76594794071
----

. Mount the partition:
+
[source,terminal]
----
# mount /dev/nvme0n1p1 /mnt/
----

.Verification

* Check that the partition is mounted:
+
[source,terminal]
----
# lsblk
----

+
.Example output
[source,terminal]
----
NAME        MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
loop0         7:0    0  93.8G  0 loop /run/ephemeral
loop1         7:1    0 897.3M  1 loop /sysroot
sr0          11:0    1   999M  0 rom  /run/media/iso
nvme0n1     259:1    0   1.5T  0 disk
└─nvme0n1p1 259:2    0   250G  0 part /var/mnt <1>
----
<1> The mount point is `/var/mnt` because the `/mnt` folder in {op-system} is a link to `/var/mnt`.

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * scalability_and_performance/ztp_far_edge/ztp-precaching-tool.adoc

:_module-type: PROCEDURE
[id="ztp-downloading-images_{context}"]
= Downloading the images

The {factory-prestaging-tool} allows you to download the following images to your partitioned server:

* {product-title} images
* Operator images that are included in the distributed unit (DU) profile for 5G RAN sites
* Operator images from disconnected registries

[NOTE]
====
The list of available Operator images can vary in different {product-title} releases.
====

[id="ztp-downloading-images-parallel-workers_{context}"]
== Downloading with parallel workers

The {factory-prestaging-tool} uses parallel workers to download multiple images simultaneously.
You can configure the number of workers with the `--parallel` or `-p` option.
The default number is set to 80% of the available CPUs to the server.

[NOTE]
====
Your login shell may be restricted to a subset of CPUs, which reduces the CPUs available to the container.
To remove this restriction, you can precede your commands with `taskset 0xffffffff`, for example:

[source,terminal]
----
# taskset 0xffffffff podman run --rm quay.io/openshift-kni/telco-ran-tools:latest factory-precaching-cli download --help
----
====

[id="ztp-preparing-ocp-images_{context}"]
== Preparing to download the {product-title} images

To download {product-title} container images, you need to know the multicluster engine (MCE) version. When you use the `--du-profile` flag, you also need to specify the {rh-rhacm-first} version running in the hub cluster that is going to provision the {sno}.

.Prerequisites

* You have {rh-rhacm} and MCE installed.
* You partitioned the storage device.
* You have enough space for the images on the partitioned device.
* You connected the bare-metal server to the Internet.
* You have a valid pull secret.

.Procedure

. Check the {rh-rhacm} and MCE version by running the following commands in the hub cluster:
+
[source,terminal]
----
$ oc get csv -A | grep -i advanced-cluster-management
----

+
.Example output
[source,terminal]
----
open-cluster-management                            advanced-cluster-management.v2.6.3           Advanced Cluster Management for Kubernetes   2.6.3                 advanced-cluster-management.v2.6.3                Succeeded
----

+
[source,terminal]
----
$ oc get csv -A | grep -i multicluster-engine
----

+
.Example output
[source,terminal]
----
multicluster-engine                                cluster-group-upgrades-operator.v0.0.3       cluster-group-upgrades-operator              0.0.3                                                                   Pending
multicluster-engine                                multicluster-engine.v2.1.4                   multicluster engine for Kubernetes           2.1.4                 multicluster-engine.v2.0.3                        Succeeded
multicluster-engine                                openshift-gitops-operator.v1.5.7             Red Hat OpenShift GitOps                     1.5.7                 openshift-gitops-operator.v1.5.6-0.1664915551.p   Succeeded
multicluster-engine                                openshift-pipelines-operator-rh.v1.6.4       Red Hat OpenShift Pipelines                  1.6.4                 openshift-pipelines-operator-rh.v1.6.3            Succeeded
----

. To access the container registry, copy a valid pull secret on the server to be installed:

.. Create the `.docker` folder:
+
[source,terminal]
----
$ mkdir /root/.docker
----

.. Copy the valid pull in the `config.json` file to the previously created `.docker/` folder:
+
[source,terminal]
----
$ cp config.json /root/.docker/config.json <1>
----
<1> `/root/.docker/config.json` is the default path where `podman` checks for the login credentials for the registry.

[NOTE]
====
If you use a different registry to pull the required artifacts, you need to copy the proper pull secret.
If the local registry uses TLS, you need to include the certificates from the registry as well.
====

[id="ztp-downloading-ocp-images_{context}"]
== Downloading the {product-title} images

The {factory-prestaging-tool} allows you to pre-cache all the container images required to provision a specific {product-title} release.

.Procedure

* Pre-cache the release by running the following command:
+
[source,terminal,subs="attributes+"]
----
# podman run -v /mnt:/mnt -v /root/.docker:/root/.docker --privileged --rm quay.io/openshift-kni/telco-ran-tools -- \
   factory-precaching-cli download \ <1>
   -r {product-version}.0 \ <2>
   --acm-version 2.6.3 \ <3>
   --mce-version 2.1.4 \ <4>
   -f /mnt \ <5>
   --img quay.io/custom/repository <6>
----
<1> Specifies the downloading function of the {factory-prestaging-tool}.
<2> Defines the {product-title} release version.
<3> Defines the {rh-rhacm} version.
<4> Defines the MCE version.
<5> Defines the folder where you want to download the images on the disk.
<6> Optional. Defines the repository where you store your additional images. These images are downloaded and pre-cached on the disk.

+
.Example output
[source,terminal,subs="attributes+"]
----
Generated /mnt/imageset.yaml
Generating list of pre-cached artifacts...
Processing artifact [1/176]: ocp-v4.0-art-dev@sha256_6ac2b96bf4899c01a87366fd0feae9f57b1b61878e3b5823da0c3f34f707fbf5
Processing artifact [2/176]: ocp-v4.0-art-dev@sha256_f48b68d5960ba903a0d018a10544ae08db5802e21c2fa5615a14fc58b1c1657c
Processing artifact [3/176]: ocp-v4.0-art-dev@sha256_a480390e91b1c07e10091c3da2257180654f6b2a735a4ad4c3b69dbdb77bbc06
Processing artifact [4/176]: ocp-v4.0-art-dev@sha256_ecc5d8dbd77e326dba6594ff8c2d091eefbc4d90c963a9a85b0b2f0e6155f995
Processing artifact [5/176]: ocp-v4.0-art-dev@sha256_274b6d561558a2f54db08ea96df9892315bb773fc203b1dbcea418d20f4c7ad1
Processing artifact [6/176]: ocp-v4.0-art-dev@sha256_e142bf5020f5ca0d1bdda0026bf97f89b72d21a97c9cc2dc71bf85050e822bbf
...
Processing artifact [175/176]: ocp-v4.0-art-dev@sha256_16cd7eda26f0fb0fc965a589e1e96ff8577e560fcd14f06b5fda1643036ed6c8
Processing artifact [176/176]: ocp-v4.0-art-dev@sha256_cf4d862b4a4170d4f611b39d06c31c97658e309724f9788e155999ae51e7188f
...
Summary:

Release:                            {product-version}.0
Hub Version:                        2.6.3
ACM Version:                        2.6.3
MCE Version:                        2.1.4
Include DU Profile:                 No
Workers:                            83
----

.Verification

* Check that all the images are compressed in the target folder of server:
+
[source,terminal]
----
$ ls -l /mnt <1>
----
<1> It is recommended that you pre-cache the images in the `/mnt` folder.

+
.Example output
[source,terminal]
----
-rw-r--r--. 1 root root  136352323 Oct 31 15:19 ocp-v4.0-art-dev@sha256_edec37e7cd8b1611d0031d45e7958361c65e2005f145b471a8108f1b54316c07.tgz
-rw-r--r--. 1 root root  156092894 Oct 31 15:33 ocp-v4.0-art-dev@sha256_ee51b062b9c3c9f4fe77bd5b3cc9a3b12355d040119a1434425a824f137c61a9.tgz
-rw-r--r--. 1 root root  172297800 Oct 31 15:29 ocp-v4.0-art-dev@sha256_ef23d9057c367a36e4a5c4877d23ee097a731e1186ed28a26c8d21501cd82718.tgz
-rw-r--r--. 1 root root  171539614 Oct 31 15:23 ocp-v4.0-art-dev@sha256_f0497bb63ef6834a619d4208be9da459510df697596b891c0c633da144dbb025.tgz
-rw-r--r--. 1 root root  160399150 Oct 31 15:20 ocp-v4.0-art-dev@sha256_f0c339da117cde44c9aae8d0bd054bceb6f19fdb191928f6912a703182330ac2.tgz
-rw-r--r--. 1 root root  175962005 Oct 31 15:17 ocp-v4.0-art-dev@sha256_f19dd2e80fb41ef31d62bb8c08b339c50d193fdb10fc39cc15b353cbbfeb9b24.tgz
-rw-r--r--. 1 root root  174942008 Oct 31 15:33 ocp-v4.0-art-dev@sha256_f1dbb81fa1aa724e96dd2b296b855ff52a565fbef003d08030d63590ae6454df.tgz
-rw-r--r--. 1 root root  246693315 Oct 31 15:31 ocp-v4.0-art-dev@sha256_f44dcf2c94e4fd843cbbf9b11128df2ba856cd813786e42e3da1fdfb0f6ddd01.tgz
-rw-r--r--. 1 root root  170148293 Oct 31 15:00 ocp-v4.0-art-dev@sha256_f48b68d5960ba903a0d018a10544ae08db5802e21c2fa5615a14fc58b1c1657c.tgz
-rw-r--r--. 1 root root  168899617 Oct 31 15:16 ocp-v4.0-art-dev@sha256_f5099b0989120a8d08a963601214b5c5cb23417a707a8624b7eb52ab788a7f75.tgz
-rw-r--r--. 1 root root  176592362 Oct 31 15:05 ocp-v4.0-art-dev@sha256_f68c0e6f5e17b0b0f7ab2d4c39559ea89f900751e64b97cb42311a478338d9c3.tgz
-rw-r--r--. 1 root root  157937478 Oct 31 15:37 ocp-v4.0-art-dev@sha256_f7ba33a6a9db9cfc4b0ab0f368569e19b9fa08f4c01a0d5f6a243d61ab781bd8.tgz
-rw-r--r--. 1 root root  145535253 Oct 31 15:26 ocp-v4.0-art-dev@sha256_f8f098911d670287826e9499806553f7a1dd3e2b5332abbec740008c36e84de5.tgz
-rw-r--r--. 1 root root  158048761 Oct 31 15:40 ocp-v4.0-art-dev@sha256_f914228ddbb99120986262168a705903a9f49724ffa958bb4bf12b2ec1d7fb47.tgz
-rw-r--r--. 1 root root  167914526 Oct 31 15:37 ocp-v4.0-art-dev@sha256_fa3ca9401c7a9efda0502240aeb8d3ae2d239d38890454f17fe5158b62305010.tgz
-rw-r--r--. 1 root root  164432422 Oct 31 15:24 ocp-v4.0-art-dev@sha256_fc4783b446c70df30b3120685254b40ce13ba6a2b0bf8fb1645f116cf6a392f1.tgz
-rw-r--r--. 1 root root  306643814 Oct 31 15:11 troubleshoot@sha256_b86b8aea29a818a9c22944fd18243fa0347c7a2bf1ad8864113ff2bb2d8e0726.tgz
----

[id="ztp-downloading-operator-images_{context}"]
== Downloading the Operator images

You can also pre-cache Day-2 Operators used in the 5G Radio Access Network (RAN) Distributed Unit (DU) cluster configuration. The Day-2 Operators depend on the installed {product-title} version.

[IMPORTANT]
====
You need to include the {rh-rhacm} hub and MCE Operator versions by using the `--acm-version` and `--mce-version` flags so the {factory-prestaging-tool} can pre-cache the appropriate containers images for the {rh-rhacm} and MCE Operators.
====

.Procedure

* Pre-cache the Operator images:
+
[source,terminal,subs="attributes+"]
----
# podman run -v /mnt:/mnt -v /root/.docker:/root/.docker --privileged --rm quay.io/openshift-kni/telco-ran-tools:latest -- factory-precaching-cli download \ <1>
   -r {product-version}.0 \ <2>
   --acm-version 2.6.3 \ <3>
   --mce-version 2.1.4 \ <4>
   -f /mnt \ <5>
   --img quay.io/custom/repository <6>
   --du-profile -s <7>
----
<1> Specifies the downloading function of the {factory-prestaging-tool}.
<2> Defines the {product-title} release version.
<3> Defines the {rh-rhacm} version.
<4> Defines the MCE version.
<5> Defines the folder where you want to download the images on the disk.
<6> Optional. Defines the repository where you store your additional images. These images are downloaded and pre-cached on the disk.
<7> Specifies pre-caching the Operators included in the DU configuration.

+
.Example output
[source,terminal,subs="attributes+"]
----
Generated /mnt/imageset.yaml
Generating list of pre-cached artifacts...
Processing artifact [1/379]: ocp-v4.0-art-dev@sha256_7753a8d9dd5974be8c90649aadd7c914a3d8a1f1e016774c7ac7c9422e9f9958
Processing artifact [2/379]: ose-kube-rbac-proxy@sha256_c27a7c01e5968aff16b6bb6670423f992d1a1de1a16e7e260d12908d3322431c
Processing artifact [3/379]: ocp-v4.0-art-dev@sha256_370e47a14c798ca3f8707a38b28cfc28114f492bb35fe1112e55d1eb51022c99
...
Processing artifact [378/379]: ose-local-storage-operator@sha256_0c81c2b79f79307305e51ce9d3837657cf9ba5866194e464b4d1b299f85034d0
Processing artifact [379/379]: multicluster-operators-channel-rhel8@sha256_c10f6bbb84fe36e05816e873a72188018856ad6aac6cc16271a1b3966f73ceb3
...
Summary:

Release:                            {product-version}.0
Hub Version:                        2.6.3
ACM Version:                        2.6.3
MCE Version:                        2.1.4
Include DU Profile:                 Yes
Workers:                            83
----

[id="ztp-custom-pre-caching-in-disconnected-environment_{context}"]
== Pre-caching custom images in disconnected environments

The `--generate-imageset` argument stops the {factory-prestaging-tool} after the `ImageSetConfiguration` custom resource (CR) is generated.
This allows you to customize the `ImageSetConfiguration` CR before downloading any images.
After you customized the CR, you can use the `--skip-imageset` argument to download the images that you specified in the `ImageSetConfiguration` CR.

You can customize the `ImageSetConfiguration` CR in the following ways:

* Add Operators and additional images
* Remove Operators and additional images
* Change Operator and catalog sources to local or disconnected registries

.Procedure

. Pre-cache the images:
+
[source,terminal,subs="attributes+"]
----
# podman run -v /mnt:/mnt -v /root/.docker:/root/.docker --privileged --rm quay.io/openshift-kni/telco-ran-tools:latest -- factory-precaching-cli download \ <1>
   -r {product-version}.0 \ <2>
   --acm-version 2.6.3 \ <3>
   --mce-version 2.1.4 \ <4>
   -f /mnt \ <5>
   --img quay.io/custom/repository <6>
   --du-profile -s \ <7>
   --generate-imageset <8>
----
<1> Specifies the downloading function of the {factory-prestaging-tool}.
<2> Defines the {product-title} release version.
<3> Defines the {rh-rhacm} version.
<4> Defines the MCE version.
<5> Defines the folder where you want to download the images on the disk.
<6> Optional. Defines the repository where you store your additional images. These images are downloaded and pre-cached on the disk.
<7> Specifies pre-caching the Operators included in the DU configuration.
<8> The `--generate-imageset` argument generates the `ImageSetConfiguration` CR only, which allows you to customize the CR.

+
.Example output
[source,terminal]
----
Generated /mnt/imageset.yaml
----

+
.Example ImageSetConfiguration CR
[source,yaml,subs="attributes+"]
----
apiVersion: mirror.openshift.io/v1alpha2
kind: ImageSetConfiguration
mirror:
  platform:
    channels:
    - name: stable-{product-version}
      minVersion: {product-version}.0 <1>
      maxVersion: {product-version}.0
  additionalImages:
    - name: quay.io/custom/repository
  operators:
    - catalog: registry.redhat.io/redhat/redhat-operator-index:v{product-version}
      packages:
        - name: advanced-cluster-management <2>
          channels:
             - name: 'release-2.6'
               minVersion: 2.6.3
               maxVersion: 2.6.3
        - name: multicluster-engine <2>
          channels:
             - name: 'stable-2.1'
               minVersion: 2.1.4
               maxVersion: 2.1.4
        - name: local-storage-operator <3>
          channels:
            - name: 'stable'
        - name: ptp-operator <3>
          channels:
            - name: 'stable'
        - name: sriov-network-operator <3>
          channels:
            - name: 'stable'
        - name: cluster-logging <3>
          channels:
            - name: 'stable'
        - name: lvms-operator <3>
          channels:
            - name: 'stable-{product-version}'
        - name: amq7-interconnect-operator <3>
          channels:
            - name: '1.10.x'
        - name: bare-metal-event-relay <3>
          channels:
            - name: 'stable'
    - catalog: registry.redhat.io/redhat/certified-operator-index:v{product-version}
      packages:
        - name: sriov-fec <3>
          channels:
            - name: 'stable'
----
<1> The platform versions match the versions passed to the tool.
<2> The versions of {rh-rhacm} and MCE Operators match the versions passed to the tool.
<3> The CR contains all the specified DU Operators.

. Customize the catalog resource in the CR:
+
[source,yaml,subs="attributes+"]
----
apiVersion: mirror.openshift.io/v1alpha2
kind: ImageSetConfiguration
mirror:
  platform:
[...]
  operators:
    - catalog: eko4.cloud.lab.eng.bos.redhat.com:8443/redhat/certified-operator-index:v{product-version}
      packages:
        - name: sriov-fec
          channels:
            - name: 'stable'
----
+
When you download images by using a local or disconnected registry, you have to first add certificates for the registries that you want to pull the content from.

. To avoid any errors, copy the registry certificate into your server:
+
[source,terminal]
----
# cp /tmp/eko4-ca.crt /etc/pki/ca-trust/source/anchors/.
----

. Then, update the certificates trust store:
+
[source,terminal]
----
# update-ca-trust
----

. Mount the host `/etc/pki` folder into the factory-cli image:
+
[source,terminal,subs="attributes+"]
----
# podman run -v /mnt:/mnt -v /root/.docker:/root/.docker -v /etc/pki:/etc/pki --privileged --rm quay.io/openshift-kni/telco-ran-tools:latest -- \
factory-precaching-cli download \ <1>
   -r {product-version}.0 \ <2>
   --acm-version 2.6.3 \ <3>
   --mce-version 2.1.4 \ <4>
   -f /mnt \ <5>
   --img quay.io/custom/repository <6>
   --du-profile -s \ <7>
   --skip-imageset <8>
----
<1> Specifies the downloading function of the {factory-prestaging-tool}.
<2> Defines the {product-title} release version.
<3> Defines the {rh-rhacm} version.
<4> Defines the MCE version.
<5> Defines the folder where you want to download the images on the disk.
<6> Optional. Defines the repository where you store your additional images. These images are downloaded and pre-cached on the disk.
<7> Specifies pre-caching the Operators included in the DU configuration.
<8> The `--skip-imageset` argument allows you to download the images that you specified in your customized `ImageSetConfiguration` CR.

. Download the images without generating a new `imageSetConfiguration` CR:
+
[source,terminal,subs="attributes+"]
----
# podman run -v /mnt:/mnt -v /root/.docker:/root/.docker --privileged --rm quay.io/openshift-kni/telco-ran-tools:latest -- factory-precaching-cli download -r {product-version}.0 \
--acm-version 2.6.3 --mce-version 2.1.4 -f /mnt \
--img quay.io/custom/repository \
--du-profile -s \
--skip-imageset
----

:leveloffset!:

[role="_additional-resources"]
.Additional resources

* To access the online Red Hat registries, see link:https://console.redhat.com/openshift/downloads#tool-pull-secret[OpenShift installation customization tools].

* For more information about using the multicluster engine, see link:https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.8/html/clusters/cluster_mce_overview#mce-intro[About cluster lifecycle with the multicluster engine operator].

:leveloffset: +1

// Module included in the following assemblies:
//
// * scalability_and_performance/ztp_far_edge/ztp-precaching-tool.adoc

:_module-type: CONCEPT
[id="ztp-pre-caching-config-con_{context}"]
= Pre-caching images in {ztp}

The `SiteConfig` manifest defines how an OpenShift cluster is to be installed and configured.
In the {ztp-first} provisioning workflow, the {factory-prestaging-tool} requires the following additional fields in the `SiteConfig` manifest:

* `clusters.ignitionConfigOverride`
* `nodes.installerArgs`
* `nodes.ignitionConfigOverride`

.Example SiteConfig with additional fields
[source,yaml]
----
apiVersion: ran.openshift.io/v1
kind: SiteConfig
metadata:
  name: "example-5g-lab"
  namespace: "example-5g-lab"
spec:
  baseDomain: "example.domain.redhat.com"
  pullSecretRef:
    name: "assisted-deployment-pull-secret"
  clusterImageSetNameRef: "img4.9.10-x86-64-appsub"
  sshPublicKey: "ssh-rsa ..."
  clusters:
  - clusterName: "sno-worker-0"
    clusterImageSetNameRef: "eko4-img4.11.5-x86-64-appsub"
    clusterLabels:
      group-du-sno: ""
      common-411: true
      sites : "example-5g-lab"
      vendor: "OpenShift"
    clusterNetwork:
      - cidr: 10.128.0.0/14
        hostPrefix: 23
    machineNetwork:
      - cidr: 10.19.32.192/26
    serviceNetwork:
      - 172.30.0.0/16
    networkType: "OVNKubernetes"
    additionalNTPSources:
      - clock.corp.redhat.com
    ignitionConfigOverride: '{"ignition":{"version":"3.1.0"},"systemd":{"units":[{"name":"var-mnt.mount","enabled":true,"contents":"[Unit]\nDescription=Mount partition with artifacts\nBefore=precache-images.service\nBindsTo=precache-images.service\nStopWhenUnneeded=true\n\n[Mount]\nWhat=/dev/disk/by-partlabel/data\nWhere=/var/mnt\nType=xfs\nTimeoutSec=30\n\n[Install]\nRequiredBy=precache-images.service"},{"name":"precache-images.service","enabled":true,"contents":"[Unit]\nDescription=Extracts the precached images in discovery stage\nAfter=var-mnt.mount\nBefore=agent.service\n\n[Service]\nType=oneshot\nUser=root\nWorkingDirectory=/var/mnt\nExecStart=bash /usr/local/bin/extract-ai.sh\n#TimeoutStopSec=30\n\n[Install]\nWantedBy=multi-user.target default.target\nWantedBy=agent.service"}]},"storage":{"files":[{"overwrite":true,"path":"/usr/local/bin/extract-ai.sh","mode":755,"user":{"name":"root"},"contents":{"source":"data:,%23%21%2Fbin%2Fbash%0A%0AFOLDER%3D%22%24%7BFOLDER%3A-%24%28pwd%29%7D%22%0AOCP_RELEASE_LIST%3D%22%24%7BOCP_RELEASE_LIST%3A-ai-images.txt%7D%22%0ABINARY_FOLDER%3D%2Fvar%2Fmnt%0A%0Apushd%20%24FOLDER%0A%0Atotal_copies%3D%24%28sort%20-u%20%24BINARY_FOLDER%2F%24OCP_RELEASE_LIST%20%7C%20wc%20-l%29%20%20%23%20Required%20to%20keep%20track%20of%20the%20pull%20task%20vs%20total%0Acurrent_copy%3D1%0A%0Awhile%20read%20-r%20line%3B%0Ado%0A%20%20uri%3D%24%28echo%20%22%24line%22%20%7C%20awk%20%27%7Bprint%241%7D%27%29%0A%20%20%23tar%3D%24%28echo%20%22%24line%22%20%7C%20awk%20%27%7Bprint%242%7D%27%29%0A%20%20podman%20image%20exists%20%24uri%0A%20%20if%20%5B%5B%20%24%3F%20-eq%200%20%5D%5D%3B%20then%0A%20%20%20%20%20%20echo%20%22Skipping%20existing%20image%20%24tar%22%0A%20%20%20%20%20%20echo%20%22Copying%20%24%7Buri%7D%20%5B%24%7Bcurrent_copy%7D%2F%24%7Btotal_copies%7D%5D%22%0A%20%20%20%20%20%20current_copy%3D%24%28%28current_copy%20%2B%201%29%29%0A%20%20%20%20%20%20continue%0A%20%20fi%0A%20%20tar%3D%24%28echo%20%22%24uri%22%20%7C%20%20rev%20%7C%20cut%20-d%20%22%2F%22%20-f1%20%7C%20rev%20%7C%20tr%20%22%3A%22%20%22_%22%29%0A%20%20tar%20zxvf%20%24%7Btar%7D.tgz%0A%20%20if%20%5B%20%24%3F%20-eq%200%20%5D%3B%20then%20rm%20-f%20%24%7Btar%7D.gz%3B%20fi%0A%20%20echo%20%22Copying%20%24%7Buri%7D%20%5B%24%7Bcurrent_copy%7D%2F%24%7Btotal_copies%7D%5D%22%0A%20%20skopeo%20copy%20dir%3A%2F%2F%24%28pwd%29%2F%24%7Btar%7D%20containers-storage%3A%24%7Buri%7D%0A%20%20if%20%5B%20%24%3F%20-eq%200%20%5D%3B%20then%20rm%20-rf%20%24%7Btar%7D%3B%20current_copy%3D%24%28%28current_copy%20%2B%201%29%29%3B%20fi%0Adone%20%3C%20%24%7BBINARY_FOLDER%7D%2F%24%7BOCP_RELEASE_LIST%7D%0A%0A%23%20workaround%20while%20https%3A%2F%2Fgithub.com%2Fopenshift%2Fassisted-service%2Fpull%2F3546%0A%23cp%20%2Fvar%2Fmnt%2Fmodified-rhcos-4.10.3-x86_64-metal.x86_64.raw.gz%20%2Fvar%2Ftmp%2F.%0A%0Aexit%200"}},{"overwrite":true,"path":"/usr/local/bin/agent-fix-bz1964591","mode":755,"user":{"name":"root"},"contents":{"source":"data:,%23%21%2Fusr%2Fbin%2Fsh%0A%0A%23%20This%20script%20is%20a%20workaround%20for%20bugzilla%201964591%20where%20symlinks%20inside%20%2Fvar%2Flib%2Fcontainers%2F%20get%0A%23%20corrupted%20under%20some%20circumstances.%0A%23%0A%23%20In%20order%20to%20let%20agent.service%20start%20correctly%20we%20are%20checking%20here%20whether%20the%20requested%0A%23%20container%20image%20exists%20and%20in%20case%20%22podman%20images%22%20returns%20an%20error%20we%20try%20removing%20the%20faulty%0A%23%20image.%0A%23%0A%23%20In%20such%20a%20scenario%20agent.service%20will%20detect%20the%20image%20is%20not%20present%20and%20pull%20it%20again.%20In%20case%0A%23%20the%20image%20is%20present%20and%20can%20be%20detected%20correctly%2C%20no%20any%20action%20is%20required.%0A%0AIMAGE%3D%24%28echo%20%241%20%7C%20sed%20%27s%2F%3A.%2A%2F%2F%27%29%0Apodman%20image%20exists%20%24IMAGE%20%7C%7C%20echo%20%22already%20loaded%22%20%7C%7C%20echo%20%22need%20to%20be%20pulled%22%0A%23podman%20images%20%7C%20grep%20%24IMAGE%20%7C%7C%20podman%20rmi%20--force%20%241%20%7C%7C%20true"}}]}}'
    nodes:
      - hostName: "snonode.sno-worker-0.example.domain.redhat.com"
        role: "master"
        bmcAddress: "idrac-virtualmedia+https://10.19.28.53/redfish/v1/Systems/System.Embedded.1"
        bmcCredentialsName:
          name: "worker0-bmh-secret"
        bootMACAddress: "e4:43:4b:bd:90:46"
        bootMode: "UEFI"
        rootDeviceHints:
          deviceName: /dev/nvme0n1
        cpuset: "0-1,40-41"
        installerArgs: '["--save-partlabel", "data"]'
        ignitionConfigOverride: '{"ignition":{"version":"3.1.0"},"systemd":{"units":[{"name":"var-mnt.mount","enabled":true,"contents":"[Unit]\nDescription=Mount partition with artifacts\nBefore=precache-ocp-images.service\nBindsTo=precache-ocp-images.service\nStopWhenUnneeded=true\n\n[Mount]\nWhat=/dev/disk/by-partlabel/data\nWhere=/var/mnt\nType=xfs\nTimeoutSec=30\n\n[Install]\nRequiredBy=precache-ocp-images.service"},{"name":"precache-ocp-images.service","enabled":true,"contents":"[Unit]\nDescription=Extracts the precached OCP images into containers storage\nAfter=var-mnt.mount\nBefore=machine-config-daemon-pull.service nodeip-configuration.service\n\n[Service]\nType=oneshot\nUser=root\nWorkingDirectory=/var/mnt\nExecStart=bash /usr/local/bin/extract-ocp.sh\nTimeoutStopSec=60\n\n[Install]\nWantedBy=multi-user.target"}]},"storage":{"files":[{"overwrite":true,"path":"/usr/local/bin/extract-ocp.sh","mode":755,"user":{"name":"root"},"contents":{"source":"data:,%23%21%2Fbin%2Fbash%0A%0AFOLDER%3D%22%24%7BFOLDER%3A-%24%28pwd%29%7D%22%0AOCP_RELEASE_LIST%3D%22%24%7BOCP_RELEASE_LIST%3A-ocp-images.txt%7D%22%0ABINARY_FOLDER%3D%2Fvar%2Fmnt%0A%0Apushd%20%24FOLDER%0A%0Atotal_copies%3D%24%28sort%20-u%20%24BINARY_FOLDER%2F%24OCP_RELEASE_LIST%20%7C%20wc%20-l%29%20%20%23%20Required%20to%20keep%20track%20of%20the%20pull%20task%20vs%20total%0Acurrent_copy%3D1%0A%0Awhile%20read%20-r%20line%3B%0Ado%0A%20%20uri%3D%24%28echo%20%22%24line%22%20%7C%20awk%20%27%7Bprint%241%7D%27%29%0A%20%20%23tar%3D%24%28echo%20%22%24line%22%20%7C%20awk%20%27%7Bprint%242%7D%27%29%0A%20%20podman%20image%20exists%20%24uri%0A%20%20if%20%5B%5B%20%24%3F%20-eq%200%20%5D%5D%3B%20then%0A%20%20%20%20%20%20echo%20%22Skipping%20existing%20image%20%24tar%22%0A%20%20%20%20%20%20echo%20%22Copying%20%24%7Buri%7D%20%5B%24%7Bcurrent_copy%7D%2F%24%7Btotal_copies%7D%5D%22%0A%20%20%20%20%20%20current_copy%3D%24%28%28current_copy%20%2B%201%29%29%0A%20%20%20%20%20%20continue%0A%20%20fi%0A%20%20tar%3D%24%28echo%20%22%24uri%22%20%7C%20%20rev%20%7C%20cut%20-d%20%22%2F%22%20-f1%20%7C%20rev%20%7C%20tr%20%22%3A%22%20%22_%22%29%0A%20%20tar%20zxvf%20%24%7Btar%7D.tgz%0A%20%20if%20%5B%20%24%3F%20-eq%200%20%5D%3B%20then%20rm%20-f%20%24%7Btar%7D.gz%3B%20fi%0A%20%20echo%20%22Copying%20%24%7Buri%7D%20%5B%24%7Bcurrent_copy%7D%2F%24%7Btotal_copies%7D%5D%22%0A%20%20skopeo%20copy%20dir%3A%2F%2F%24%28pwd%29%2F%24%7Btar%7D%20containers-storage%3A%24%7Buri%7D%0A%20%20if%20%5B%20%24%3F%20-eq%200%20%5D%3B%20then%20rm%20-rf%20%24%7Btar%7D%3B%20current_copy%3D%24%28%28current_copy%20%2B%201%29%29%3B%20fi%0Adone%20%3C%20%24%7BBINARY_FOLDER%7D%2F%24%7BOCP_RELEASE_LIST%7D%0A%0Aexit%200"}}]}}'
        nodeNetwork:
          config:
            interfaces:
              - name: ens1f0
                type: ethernet
                state: up
                macAddress: "AA:BB:CC:11:22:33"
                ipv4:
                  enabled: true
                  dhcp: true
                ipv6:
                  enabled: false
          interfaces:
            - name: "ens1f0"
              macAddress: "AA:BB:CC:11:22:33"
----

[id="ztp-pre-caching-config-clusters-ignitionconfigoverride_{context}"]
== Understanding the clusters.ignitionConfigOverride field

The `clusters.ignitionConfigOverride` field adds a configuration in Ignition format during the {ztp} discovery stage.
The configuration includes `systemd` services in the ISO mounted in virtual media. This way, the scripts are part of the discovery {op-system} live ISO and they can be used to load the Assisted Installer (AI) images.

`systemd` services:: The `systemd` services are `var-mnt.mount` and `precache-images.services`. The `precache-images.service` depends on the disk partition to be mounted in `/var/mnt` by the `var-mnt.mount` unit.
The service calls a script called `extract-ai.sh`.
`extract-ai.sh`:: The `extract-ai.sh` script extracts and loads the required images from the disk partition to the local container storage.
When the script finishes successfully, you can use the images locally.
`agent-fix-bz1964591`:: The `agent-fix-bz1964591` script is a workaround for an AI issue.
To prevent AI from removing the images, which can force the `agent.service` to pull the images again from the registry, the `agent-fix-bz1964591` script checks if the requested container images exist.

[id="ztp-pre-caching-config-nodes-installerargs_{context}"]
== Understanding the nodes.installerArgs field

The `nodes.installerArgs` field allows you to configure how the `coreos-installer` utility writes the {op-system} live ISO to disk. You need to indicate to save the disk partition labeled as `data` because the artifacts saved in the `data` partition are needed during the {product-title} installation stage.

The extra parameters are passed directly to the `coreos-installer` utility that writes the live {op-system} to disk.
On the next reboot, the operating system starts from the disk.

You can pass several options to the `coreos-installer` utility:

[source,terminal]
----
OPTIONS:
...
    -u, --image-url <URL>
            Manually specify the image URL

    -f, --image-file <path>
            Manually specify a local image file

    -i, --ignition-file <path>
            Embed an Ignition config from a file

    -I, --ignition-url <URL>
            Embed an Ignition config from a URL
...
        --save-partlabel <lx>...
            Save partitions with this label glob

        --save-partindex <id>...
            Save partitions with this number or range
...
        --insecure-ignition
            Allow Ignition URL without HTTPS or hash
----

[id="ztp-pre-caching-config-nodes-ignitionconfigoverride_{context}"]
== Understanding the nodes.ignitionConfigOverride field

Similarly to `clusters.ignitionConfigOverride`, the `nodes.ignitionConfigOverride` field allows the addtion of configurations in Ignition format to the `coreos-installer` utility, but at the {product-title} installation stage.
When the {op-system} is written to disk, the extra configuration included in the {ztp} discovery ISO is no longer available. During the discovery stage, the extra configuration is stored in the memory of the live OS.

[NOTE]
====
At this stage, the number of container images extracted and loaded is bigger than in the discovery stage. Depending on the {product-title} release and whether you install the Day-2 Operators, the installation time can vary.
====

At the installation stage, the `var-mnt.mount` and `precache-ocp.services` `systemd` services are used.

`precache-ocp.service`:: The `precache-ocp.service` depends on the disk partition to be mounted in `/var/mnt` by the `var-mnt.mount` unit.
The `precache-ocp.service` service calls a script called `extract-ocp.sh`.
+
[IMPORTANT]
====
To extract all the images before the {product-title} installation, you must execute `precache-ocp.service` before executing the `machine-config-daemon-pull.service` and `nodeip-configuration.service` services.
====

`extract-ocp.sh`:: The `extract-ocp.sh` script extracts and loads the required images from the disk partition to the local container storage.
When the script finishes successfully, you can use the images locally.

When you upload the `SiteConfig` and the optional `PolicyGenTemplates` custom resources (CRs) to the Git repo, which Argo CD is monitoring, you can start the {ztp} workflow by syncing the CRs with the hub cluster.

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * scalability_and_performance/ztp_far_edge/ztp-precaching-tool.adoc

:_mod-docs-content-type: PROCEDURE
[id="ztp-pre-staging-troubleshooting_{context}"]
= Troubleshooting

== Rendered catalog is invalid

When you download images by using a local or disconnected registry, you might see the `The rendered catalog is invalid` error. This means that you are missing certificates of the new registry you want to pull content from.

[NOTE]
====
The {factory-prestaging-tool} image is built on a UBI {op-system-base} image. Certificate paths and locations are the same on {op-system}.
====

.Example error
[source,terminal]
----
Generating list of pre-cached artifacts...
error: unable to run command oc-mirror -c /mnt/imageset.yaml file:///tmp/fp-cli-3218002584/mirror --ignore-history --dry-run: Creating directory: /tmp/fp-cli-3218002584/mirror/oc-mirror-workspace/src/publish
Creating directory: /tmp/fp-cli-3218002584/mirror/oc-mirror-workspace/src/v2
Creating directory: /tmp/fp-cli-3218002584/mirror/oc-mirror-workspace/src/charts
Creating directory: /tmp/fp-cli-3218002584/mirror/oc-mirror-workspace/src/release-signatures
backend is not configured in /mnt/imageset.yaml, using stateless mode
backend is not configured in /mnt/imageset.yaml, using stateless mode
No metadata detected, creating new workspace
level=info msg=trying next host error=failed to do request: Head "https://eko4.cloud.lab.eng.bos.redhat.com:8443/v2/redhat/redhat-operator-index/manifests/v4.11": x509: certificate signed by unknown authority host=eko4.cloud.lab.eng.bos.redhat.com:8443

The rendered catalog is invalid.

Run "oc-mirror list operators --catalog CATALOG-NAME --package PACKAGE-NAME" for more information.

error: error rendering new refs: render reference "eko4.cloud.lab.eng.bos.redhat.com:8443/redhat/redhat-operator-index:v4.11": error resolving name : failed to do request: Head "https://eko4.cloud.lab.eng.bos.redhat.com:8443/v2/redhat/redhat-operator-index/manifests/v4.11": x509: certificate signed by unknown authority
----

.Procedure

. Copy the registry certificate into your server:
+
[source,terminal]
----
# cp /tmp/eko4-ca.crt /etc/pki/ca-trust/source/anchors/.
----

. Update the certificates truststore:
+
[source,terminal]
----
# update-ca-trust
----

. Mount the host `/etc/pki` folder into the factory-cli image:
+
[source,terminal,subs="attributes+"]
----
# podman run -v /mnt:/mnt -v /root/.docker:/root/.docker -v /etc/pki:/etc/pki --privileged -it --rm quay.io/openshift-kni/telco-ran-tools:latest -- \
factory-precaching-cli download -r {product-version}.0 --acm-version 2.5.4 \
   --mce-version 2.0.4 -f /mnt \--img quay.io/custom/repository
   --du-profile -s --skip-imageset
----

:leveloffset!:

//# includes=_attributes/common-attributes,snippets/technology-preview,modules/ztp-precaching-getting-tool,modules/ztp-precaching-booting-from-live-os,modules/ztp-precaching-partitioning,modules/ztp-precaching-downloading-artifacts,modules/ztp-precaching-ztp-config,modules/ztp-precaching-troubleshooting

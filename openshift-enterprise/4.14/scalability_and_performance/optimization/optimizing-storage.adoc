:_mod-docs-content-type: ASSEMBLY
[id="optimizing-storage"]
= Optimizing storage
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
:gluster: GlusterFS
:gluster-native: Containerized GlusterFS
:gluster-external: External GlusterFS
:gluster-install-link: https://docs.gluster.org/en/latest/Install-Guide/Overview/
:gluster-admin-link: https://docs.gluster.org/en/latest/Administrator%20Guide/overview/
:gluster-role-link: https://github.com/openshift/openshift-ansible/tree/master/roles/openshift_storage_glusterfs
:context: persistent-storage

toc::[]

Optimizing storage helps to minimize storage use across all resources. By
optimizing storage, administrators help ensure that existing storage resources
are working in an efficient manner.

:leveloffset: +1

// Module included in the following assemblies:
//
// * storage/optimizing-storage.adoc
// * post_installation_configuration/storage-configuration.adoc

[id="available-persistent-storage-options_{context}"]
= Available persistent storage options

Understand your persistent storage options so that you can optimize your
{product-title} environment.

.Available storage options
[cols="1,4,3",options="header"]
|===
| Storage type | Description | Examples

|Block
a|* Presented to the operating system (OS) as a block device
* Suitable for applications that need full control of storage and operate at a low level on files
bypassing the file system
* Also referred to as a Storage Area Network (SAN)
* Non-shareable, which means that only one client at a time can mount an endpoint of this type
| AWS EBS and VMware vSphere support dynamic persistent volume (PV) provisioning natively in {product-title}.
// Ceph RBD, OpenStack Cinder, Azure Disk, GCE persistent disk

|File
a| * Presented to the OS as a file system export to be mounted
* Also referred to as Network Attached Storage (NAS)
* Concurrency, latency, file locking mechanisms, and other capabilities vary widely between protocols, implementations, vendors, and scales.
|RHEL NFS, NetApp NFS ^[1]^, and Vendor NFS
// Azure File, AWS EFS

| Object
a| * Accessible through a REST API endpoint
* Configurable for use in the {product-registry}
* Applications must build their drivers into the application and/or container.
| AWS S3
// Aliyun OSS, Ceph Object Storage (RADOS Gateway)
// Google Cloud Storage, Azure Blob Storage, OpenStack Swift
|===
[.small]
--
1. NetApp NFS supports dynamic PV provisioning when using the Trident plugin.
--

[IMPORTANT]
====
Currently, CNS is not supported in {product-title} {product-version}.
====

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * storage/optimizing-storage.adoc
// * post_installation_configuration/storage-configuration.adoc

[id="recommended-configurable-storage-technology_{context}"]
= Recommended configurable storage technology

The following table summarizes the recommended and configurable storage technologies for the given {product-title} cluster application.

.Recommended and configurable storage technology
[options="header,footer"]
|===
|Storage type|Block|File|Object

| ROX^1^
| Yes^4^
| Yes^4^
| Yes

| RWX^2^
| No
| Yes
| Yes

| Registry
| Configurable
| Configurable
| Recommended

| Scaled registry
| Not configurable
| Configurable
| Recommended

| Metrics^3^
| Recommended
| Configurable^5^
| Not configurable

| Elasticsearch Logging
| Recommended
| Configurable^6^
| Not supported^6^

| Loki Logging
| Configurable
| Not configurable
| Recommended

| Apps
| Recommended
| Recommended
| Not configurable^7^

4+a|
^1^ `ReadOnlyMany`

^2^ `ReadWriteMany`

^3^ Prometheus is the underlying technology used for metrics.

^4^ This does not apply to physical disk, VM physical disk, VMDK, loopback over NFS, AWS EBS, and Azure Disk.

^5^ For metrics, using file storage with the `ReadWriteMany` (RWX) access mode is unreliable. If you use file storage, do not configure the RWX access mode on any persistent volume claims (PVCs) that are configured for use with metrics.

^6^ For logging, review the recommended storage solution in Configuring persistent storage for the log store section. Using NFS storage as a persistent volume or through NAS, such as Gluster, can corrupt the data. Hence, NFS is not supported for Elasticsearch storage and LokiStack log store in {product-title} Logging. You must use one persistent volume type per log store.

^7^ Object storage is not consumed through {product-title}'s PVs or PVCs. Apps must integrate with the object storage REST API.

|===

[NOTE]
====
A scaled registry is an {product-registry} where two or more pod replicas are running.
====

== Specific application storage recommendations

[IMPORTANT]
====
Testing shows issues with using the NFS server on {op-system-base-full} as storage backend for core services. This includes the OpenShift Container Registry and Quay, Prometheus for monitoring storage, and Elasticsearch for logging storage. Therefore, using {op-system-base} NFS to back PVs used by core services is not recommended.

Other NFS implementations on the marketplace might not have these issues. Contact the individual NFS implementation vendor for more information on any testing that was possibly completed against these {product-title} core components.
====

=== Registry

In a non-scaled/high-availability (HA) {product-registry} cluster deployment:

* The storage technology does not have to support RWX access mode.
* The storage technology must ensure read-after-write consistency.
* The preferred storage technology is object storage followed by block storage.
* File storage is not recommended for {product-registry} cluster deployment with production workloads.

=== Scaled registry

In a scaled/HA {product-registry} cluster deployment:

* The storage technology must support RWX access mode.
* The storage technology must ensure read-after-write consistency.
* The preferred storage technology is object storage.
* Red Hat OpenShift Data Foundation (ODF), Amazon Simple Storage Service (Amazon S3), Google Cloud Storage (GCS), Microsoft Azure Blob Storage, and OpenStack Swift are supported.
* Object storage should be S3 or Swift compliant.
* For non-cloud platforms, such as vSphere and bare metal installations, the only configurable technology is file storage.
* Block storage is not configurable.

=== Metrics

In an {product-title} hosted metrics cluster deployment:

* The preferred storage technology is block storage.
* Object storage is not configurable.

[IMPORTANT]
====
It is not recommended to use file storage for a hosted metrics cluster deployment with production workloads.
====

=== Logging

In an {product-title} hosted logging cluster deployment:

* The preferred storage technology is block storage.
* Object storage is not configurable.

=== Applications

Application use cases vary from application to application, as described in the following examples:

* Storage technologies that support dynamic PV provisioning have low mount time latencies, and are not tied to nodes to support a healthy cluster.
* Application developers are responsible for knowing and understanding the storage requirements for their application, and how it works with the provided storage to ensure that issues do not occur when an application scales or interacts with the storage layer.

== Other specific application storage recommendations

[IMPORTANT]
====
It is not recommended to use RAID configurations on `Write` intensive workloads, such as `etcd`. If you are running `etcd` with a RAID configuration, you might be at risk of encountering performance issues with your workloads.
====

* {rh-openstack-first} Cinder: {rh-openstack} Cinder tends to be adept in ROX access mode use cases.
* Databases: Databases (RDBMSs, NoSQL DBs, etc.) tend to perform best with dedicated block storage.
* The etcd database must have enough storage and adequate performance capacity to enable a large cluster. Information about monitoring and benchmarking tools to establish ample storage and a high-performance environment is described in _Recommended etcd practices_.

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * storage/optimizing-storage.adoc

[id="data-storage-management_{context}"]
= Data storage management

The following table summarizes the main directories that {product-title} components write data to.

.Main directories for storing {product-title} data
[options="header,footer"]
|===
|Directory|Notes|Sizing|Expected growth

|*_/var/lib/etcd_*
|Used for etcd storage when storing the database.
|Less than 20 GB.

Database can grow up to 8 GB.
|Will grow slowly with the environment. Only storing metadata.

Additional 20-25 GB for every additional 8 GB of memory.

|*_/var/lib/containers_*
|This is the mount point for the CRI-O runtime. Storage used for active container runtimes, including pods, and storage of local images. Not used for registry storage.
|50 GB for a node with 16 GB memory. Note that this sizing should not be used to determine minimum cluster requirements.

Additional 20-25 GB for every additional 8 GB of memory.
|Growth is limited by capacity for running containers.

|*_/var/lib/kubelet_*
|Ephemeral volume storage for pods. This includes anything external that is mounted into a container at runtime. Includes environment variables, kube secrets, and data volumes not backed by persistent volumes.
|Varies
|Minimal if pods requiring storage are using persistent volumes. If using ephemeral storage, this can grow quickly.

|*_/var/log_*
|Log files for all components.
|10 to 30 GB.
|Log files can grow quickly; size can be managed by growing disks or by using log rotate.

|===

:leveloffset!:

:leveloffset: +1

// Module included in the following assembly:
//
// * ../scalability_and_performance/optimization/optimizing-storage.adoc

:_mod-docs-content-type: REFERENCE

[id="optimizing-storage-azure_{context}"]
= Optimizing storage performance for Microsoft Azure

{product-title} and Kubernetes are sensitive to disk performance, and faster storage is recommended, particularly for etcd on the control plane nodes.

For production Azure clusters and clusters with intensive workloads, the virtual machine operating system disk for control plane machines should be able to sustain a tested and recommended minimum throughput of 5000 IOPS / 200MBps.
This throughput can be provided by having a minimum of 1 TiB Premium SSD (P30).
In Azure and Azure Stack Hub, disk performance is directly dependent on SSD disk sizes. To achieve the throughput supported by a `Standard_D8s_v3` virtual machine, or other similar machine types, and the target of 5000 IOPS, at least a P30 disk is required.

Host caching must be set to `ReadOnly` for low latency and high IOPS and throughput when reading data. Reading data from the cache, which is present either in the VM memory or in the local SSD disk, is much faster than reading from the disk, which is in the blob storage.

:leveloffset!:

[role="_additional-resources"]
[id="admission-plug-ins-additional-resources"]
== Additional resources

* xref:../../logging/config/cluster-logging-log-store.adoc#cluster-logging-elasticsearch-storage_cluster-logging-log-store[Configuring persistent storage for the log store]

//# includes=_attributes/common-attributes,modules/available-persistent-storage-options,modules/recommended-configurable-storage-technology,modules/data-storage-management,modules/optimizing-storage-azure

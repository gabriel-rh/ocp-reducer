:_mod-docs-content-type: ASSEMBLY
[id="troubleshooting-operator-issues"]
= Troubleshooting Operator issues
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
:context: troubleshooting-operator-issues

// This assembly is duplicated in operators/admin/olm-troubleshooting-operator-issues.adoc.

toc::[]

Operators are a method of packaging, deploying, and managing an {product-title} application. They act like an extension of the software vendor's engineering team, watching over an {product-title} environment and using its current state to make decisions in real time. Operators are designed to handle upgrades seamlessly, react to failures automatically, and not take shortcuts, such as skipping a software backup process to save time.

{product-title} {product-version} includes a default set of Operators that are required for proper functioning of the cluster. These default Operators are managed by the Cluster Version Operator (CVO).

As a cluster administrator, you can install application Operators from the OperatorHub using the {product-title} web console or the CLI. You can then subscribe the Operator to one or more namespaces to make it available for developers on your cluster. Application Operators are managed by Operator Lifecycle Manager (OLM).

If you experience Operator issues, verify Operator subscription status. Check Operator pod health across the cluster and gather Operator logs for diagnosis.

// Operator subscription condition types
:leveloffset: +1

// Module included in the following assemblies:
//
// * operators/admin/olm-status.adoc
// * support/troubleshooting/troubleshooting-operator-issues.adoc

[id="olm-status-conditions_{context}"]
= Operator subscription condition types

Subscriptions can report the following condition types:

.Subscription condition types
[cols="1,2",options="header"]
|===
|Condition |Description

|`CatalogSourcesUnhealthy`
|Some or all of the catalog sources to be used in resolution are unhealthy.

|`InstallPlanMissing`
|An install plan for a subscription is missing.

|`InstallPlanPending`
|An install plan for a subscription is pending installation.

|`InstallPlanFailed`
|An install plan for a subscription has failed.

|`ResolutionFailed`
|The dependency resolution for a subscription has failed.

|===

[NOTE]
====
Default {product-title} cluster Operators are managed by the Cluster Version Operator (CVO) and they do not have a `Subscription` object. Application Operators are managed by Operator Lifecycle Manager (OLM) and they have a `Subscription` object.
====

:leveloffset!:
[role="_additional-resources"]
.Additional resources

* xref:../../operators/understanding/olm/olm-understanding-olm.adoc#olm-cs-health_olm-understanding-olm[Catalog health requirements]

// Viewing Operator subscription status by using the CLI
:leveloffset: +1

// Module included in the following assemblies:
//
// * operators/admin/olm-status.adoc
// * support/troubleshooting/troubleshooting-operator-issues.adoc

:_mod-docs-content-type: PROCEDURE
[id="olm-status-viewing-cli_{context}"]
= Viewing Operator subscription status by using the CLI

You can view Operator subscription status by using the CLI.

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` role.
* You have installed the OpenShift CLI (`oc`).

.Procedure

. List Operator subscriptions:
+
[source,terminal]
----
$ oc get subs -n <operator_namespace>
----

. Use the `oc describe` command to inspect a `Subscription` resource:
+
[source,terminal]
----
$ oc describe sub <subscription_name> -n <operator_namespace>
----

. In the command output, find the `Conditions` section for the status of Operator subscription condition types. In the following example, the `CatalogSourcesUnhealthy` condition type has a status of `false` because all available catalog sources are healthy:
+
.Example output
[source,terminal]
----
Name:         cluster-logging
Namespace:    openshift-logging
Labels:       operators.coreos.com/cluster-logging.openshift-logging=
Annotations:  <none>
API Version:  operators.coreos.com/v1alpha1
Kind:         Subscription
# ...
Conditions:
   Last Transition Time:  2019-07-29T13:42:57Z
   Message:               all available catalogsources are healthy
   Reason:                AllCatalogSourcesHealthy
   Status:                False
   Type:                  CatalogSourcesUnhealthy
# ...
----

[NOTE]
====
Default {product-title} cluster Operators are managed by the Cluster Version Operator (CVO) and they do not have a `Subscription` object. Application Operators are managed by Operator Lifecycle Manager (OLM) and they have a `Subscription` object.
====

:leveloffset!:

// Viewing Operator catalog source status by using the CLI
:leveloffset: +1

// Module included in the following assemblies:
//
// * operators/admin/olm-status.adoc
// * support/troubleshooting/troubleshooting-operator-issues.adoc

:global_ns: openshift-marketplace

:_mod-docs-content-type: PROCEDURE
[id="olm-cs-status-cli_{context}"]
= Viewing Operator catalog source status by using the CLI

You can view the status of an Operator catalog source by using the CLI.

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` role.
* You have installed the OpenShift CLI (`oc`).

.Procedure

. List the catalog sources in a namespace. For example, you can check the `{global_ns}` namespace, which is used for cluster-wide catalog sources:
+
[source,terminal,subs="attributes+"]
----
$ oc get catalogsources -n {global_ns}
----
+
.Example output
[source,terminal]
----
NAME                  DISPLAY               TYPE   PUBLISHER   AGE
certified-operators   Certified Operators   grpc   Red Hat     55m
community-operators   Community Operators   grpc   Red Hat     55m
example-catalog       Example Catalog       grpc   Example Org 2m25s
redhat-marketplace    Red Hat Marketplace   grpc   Red Hat     55m
redhat-operators      Red Hat Operators     grpc   Red Hat     55m
----

. Use the `oc describe` command to get more details and status about a catalog source:
+
[source,terminal,subs="attributes+"]
----
$ oc describe catalogsource example-catalog -n {global_ns}
----
+
.Example output
[source,terminal,subs="attributes+"]
----
Name:         example-catalog
Namespace:    {global_ns}
Labels:       <none>
Annotations:  operatorframework.io/managed-by: marketplace-operator
              target.workload.openshift.io/management: {"effect": "PreferredDuringScheduling"}
API Version:  operators.coreos.com/v1alpha1
Kind:         CatalogSource
# ...
Status:
  Connection State:
    Address:              example-catalog.{global_ns}.svc:50051
    Last Connect:         2021-09-09T17:07:35Z
    Last Observed State:  TRANSIENT_FAILURE
  Registry Service:
    Created At:         2021-09-09T17:05:45Z
    Port:               50051
    Protocol:           grpc
    Service Name:       example-catalog
    Service Namespace:  {global_ns}
# ...
----
+
In the preceding example output, the last observed state is `TRANSIENT_FAILURE`. This state indicates that there is a problem establishing a connection for the catalog source.

. List the pods in the namespace where your catalog source was created:
+
[source,terminal,subs="attributes+"]
----
$ oc get pods -n {global_ns}
----
+
.Example output
[source,terminal]
----
NAME                                    READY   STATUS             RESTARTS   AGE
certified-operators-cv9nn               1/1     Running            0          36m
community-operators-6v8lp               1/1     Running            0          36m
marketplace-operator-86bfc75f9b-jkgbc   1/1     Running            0          42m
example-catalog-bwt8z                   0/1     ImagePullBackOff   0          3m55s
redhat-marketplace-57p8c                1/1     Running            0          36m
redhat-operators-smxx8                  1/1     Running            0          36m
----
+
When a catalog source is created in a namespace, a pod for the catalog source is created in that namespace. In the preceding example output, the status for the `example-catalog-bwt8z` pod is `ImagePullBackOff`. This status indicates that there is an issue pulling the catalog source's index image.

. Use the `oc describe` command to inspect a pod for more detailed information:
+
[source,terminal,subs="attributes+"]
----
$ oc describe pod example-catalog-bwt8z -n {global_ns}
----
+
.Example output
[source,terminal,subs="attributes+"]
----
Name:         example-catalog-bwt8z
Namespace:    {global_ns}
Priority:     0
Node:         ci-ln-jyryyg2-f76d1-ggdbq-worker-b-vsxjd/10.0.128.2
...
Events:
  Type     Reason          Age                From               Message
  ----     ------          ----               ----               -------
  Normal   Scheduled       48s                default-scheduler  Successfully assigned {global_ns}/example-catalog-bwt8z to ci-ln-jyryyf2-f76d1-fgdbq-worker-b-vsxjd
  Normal   AddedInterface  47s                multus             Add eth0 [10.131.0.40/23] from openshift-sdn
  Normal   BackOff         20s (x2 over 46s)  kubelet            Back-off pulling image "quay.io/example-org/example-catalog:v1"
  Warning  Failed          20s (x2 over 46s)  kubelet            Error: ImagePullBackOff
  Normal   Pulling         8s (x3 over 47s)   kubelet            Pulling image "quay.io/example-org/example-catalog:v1"
  Warning  Failed          8s (x3 over 47s)   kubelet            Failed to pull image "quay.io/example-org/example-catalog:v1": rpc error: code = Unknown desc = reading manifest v1 in quay.io/example-org/example-catalog: unauthorized: access to the requested resource is not authorized
  Warning  Failed          8s (x3 over 47s)   kubelet            Error: ErrImagePull
----
+
In the preceding example output, the error messages indicate that the catalog source's index image is failing to pull successfully because of an authorization issue. For example, the index image might be stored in a registry that requires login credentials.

:!global_ns:

:leveloffset!:

[role="_additional-resources"]
.Additional resources

* xref:../../operators/understanding/olm/olm-understanding-olm.adoc#olm-catalogsource_olm-understanding-olm[Operator Lifecycle Manager concepts and resources -> Catalog source]
* gRPC documentation: link:https://grpc.github.io/grpc/core/md_doc_connectivity-semantics-and-api.html[States of Connectivity]
* xref:../../operators/admin/olm-managing-custom-catalogs.adoc#olm-accessing-images-private-registries_olm-managing-custom-catalogs[Accessing images for Operators from private registries]

// Querying Operator Pod status
:leveloffset: +1

// Module included in the following assemblies:
//
// * support/troubleshooting/troubleshooting-operator-issues.adoc

:_mod-docs-content-type: PROCEDURE
[id="querying-operator-pod-status_{context}"]
= Querying Operator pod status

You can list Operator pods within a cluster and their status. You can also collect a detailed Operator pod summary.

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` role.
* Your API service is still functional.
* You have installed the OpenShift CLI (`oc`).

.Procedure

. List Operators running in the cluster. The output includes Operator version, availability, and up-time information:
+
[source,terminal]
----
$ oc get clusteroperators
----

. List Operator pods running in the Operator's namespace, plus pod status, restarts, and age:
+
[source,terminal]
----
$ oc get pod -n <operator_namespace>
----

. Output a detailed Operator pod summary:
+
[source,terminal]
----
$ oc describe pod <operator_pod_name> -n <operator_namespace>
----

. If an Operator issue is node-specific, query Operator container status on that node.
.. Start a debug pod for the node:
+
[source,terminal]
----
$ oc debug node/my-node
----
+
.. Set `/host` as the root directory within the debug shell. The debug pod mounts the host's root file system in `/host` within the pod. By changing the root directory to `/host`, you can run binaries contained in the host's executable paths:
+
[source,terminal]
----
# chroot /host
----
+
[NOTE]
====
{product-title} {product-version} cluster nodes running {op-system-first} are immutable and rely on Operators to apply cluster changes. Accessing cluster nodes by using SSH is not recommended. However, if the {product-title} API is not available, or the kubelet is not properly functioning on the target node, `oc` operations will be impacted. In such situations, it is possible to access nodes using `ssh core@<node>.<cluster_name>.<base_domain>` instead.
====
+
.. List details about the node's containers, including state and associated pod IDs:
+
[source,terminal]
----
# crictl ps
----
+
.. List information about a specific Operator container on the node. The following example lists information about the `network-operator` container:
+
[source,terminal]
----
# crictl ps --name network-operator
----
+
.. Exit from the debug shell.

:leveloffset!:

// Gathering Operator logs
:leveloffset: +1

// Module included in the following assemblies:
//
// * support/troubleshooting/troubleshooting-operator-issues.adoc

:_mod-docs-content-type: PROCEDURE
[id="gathering-operator-logs_{context}"]
= Gathering Operator logs

If you experience Operator issues, you can gather detailed diagnostic information from Operator pod logs.

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` role.
* Your API service is still functional.
* You have installed the OpenShift CLI (`oc`).
* You have the fully qualified domain names of the control plane or control plane machines.

.Procedure

. List the Operator pods that are running in the Operator's namespace, plus the pod status, restarts, and age:
+
[source,terminal]
----
$ oc get pods -n <operator_namespace>
----

. Review logs for an Operator pod:
+
[source,terminal]
----
$ oc logs pod/<pod_name> -n <operator_namespace>
----
+
If an Operator pod has multiple containers, the preceding command will produce an error that includes the name of each container. Query logs from an individual container:
+
[source,terminal]
----
$ oc logs pod/<operator_pod_name> -c <container_name> -n <operator_namespace>
----

. If the API is not functional, review Operator pod and container logs on each control plane node by using SSH instead. Replace `<master-node>.<cluster_name>.<base_domain>` with appropriate values.
.. List pods on each control plane node:
+
[source,terminal]
----
$ ssh core@<master-node>.<cluster_name>.<base_domain> sudo crictl pods
----
+
.. For any Operator pods not showing a `Ready` status, inspect the pod's status in detail. Replace `<operator_pod_id>` with the Operator pod's ID listed in the output of the preceding command:
+
[source,terminal]
----
$ ssh core@<master-node>.<cluster_name>.<base_domain> sudo crictl inspectp <operator_pod_id>
----
+
.. List containers related to an Operator pod:
+
[source,terminal]
----
$ ssh core@<master-node>.<cluster_name>.<base_domain> sudo crictl ps --pod=<operator_pod_id>
----
+
.. For any Operator container not showing a `Ready` status, inspect the container's status in detail. Replace `<container_id>` with a container ID listed in the output of the preceding command:
+
[source,terminal]
----
$ ssh core@<master-node>.<cluster_name>.<base_domain> sudo crictl inspect <container_id>
----
+
.. Review the logs for any Operator containers not showing a `Ready` status. Replace `<container_id>` with a container ID listed in the output of the preceding command:
+
[source,terminal]
----
$ ssh core@<master-node>.<cluster_name>.<base_domain> sudo crictl logs -f <container_id>
----
+
[NOTE]
====
{product-title} {product-version} cluster nodes running {op-system-first} are immutable and rely on Operators to apply cluster changes. Accessing cluster nodes by using SSH is not recommended. Before attempting to collect diagnostic data over SSH, review whether the data collected by running `oc adm must gather` and other `oc` commands is sufficient instead. However, if the {product-title} API is not available, or the kubelet is not properly functioning on the target node, `oc` operations will be impacted. In such situations, it is possible to access nodes using `ssh core@<node>.<cluster_name>.<base_domain>`.
====

:leveloffset!:

// cannot patch resource "machineconfigpools"
// Disabling Machine Config Operator from autorebooting
:leveloffset: +1

// Module included in the following assemblies:
//
// * support/troubleshooting/troubleshooting-operator-issues.adoc

[id="troubleshooting-disabling-autoreboot-mco_{context}"]
= Disabling the Machine Config Operator from automatically rebooting

When configuration changes are made by the Machine Config Operator (MCO), {op-system-first} must reboot for the changes to take effect. Whether the configuration change is automatic or manual, an {op-system} node reboots automatically unless it is paused.

[NOTE]
====
// Text snippet included in the following modules:
//
// * modules/about-crio.adoc
// * modules/nodes-containers-using.adoc

:_mod-docs-content-type: SNIPPET

The following modifications do not trigger a node reboot:

* When the MCO detects any of the following changes, it applies the update without draining or rebooting the node:

** Changes to the SSH key in the `spec.config.passwd.users.sshAuthorizedKeys` parameter of a machine config.
** Changes to the global pull secret or pull secret in the `openshift-config` namespace.
** Automatic rotation of the `/etc/kubernetes/kubelet-ca.crt` certificate authority (CA) by the Kubernetes API Server Operator.

* When the MCO detects changes to the `/etc/containers/registries.conf` file, such as adding or editing an `ImageDigestMirrorSet` or `ImageTagMirrorSet` object, it drains the corresponding nodes, applies the changes, and uncordons the nodes.The node drain does not happen for the following changes:
** The addition of a registry with the `pull-from-mirror = "digest-only"` parameter set for each mirror.
** The addition of a mirror with the `pull-from-mirror = "digest-only"` parameter set in a registry.
** The addition of items to the `unqualified-search-registries` list.

====

To avoid unwanted disruptions, you can modify the machine config pool (MCP) to prevent automatic rebooting after the Operator makes changes to the machine config.

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * support/troubleshooting/troubleshooting-operator-issues.adoc

:_mod-docs-content-type: PROCEDURE
[id="troubleshooting-disabling-autoreboot-mco-console_{context}"]
= Disabling the Machine Config Operator from automatically rebooting by using the console

To avoid unwanted disruptions from changes made by the Machine Config Operator (MCO), you can use the {product-title} web console to modify the machine config pool (MCP) to prevent the MCO from making any changes to nodes in that pool. This prevents any reboots that would normally be part of the MCO update process.

[NOTE]
====
See second `NOTE` in xref:../../support/troubleshooting/troubleshooting-operator-issues.adoc#troubleshooting-disabling-autoreboot-mco_troubleshooting-operator-issues[Disabling the Machine Config Operator from automatically rebooting].
====

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` role.

.Procedure

To pause or unpause automatic MCO update rebooting:

* Pause the autoreboot process:

. Log in to the {product-title} web console as a user with the `cluster-admin` role.

. Click *Compute* -> *MachineConfigPools*.

. On the *MachineConfigPools* page, click either *master* or *worker*, depending upon which nodes you want to pause rebooting for.

. On the *master* or *worker* page, click *YAML*.

. In the YAML, update the `spec.paused` field to `true`.
+
.Sample MachineConfigPool object
[source,yaml]
----
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
# ...
spec:
# ...
  paused: true <1>
# ...
----
<1> Update the `spec.paused` field to `true` to pause rebooting.

. To verify that the MCP is paused, return to the *MachineConfigPools* page.
+
On the *MachineConfigPools* page, the *Paused* column reports *True* for the MCP you modified.
+
If the MCP has pending changes while paused, the *Updated* column is *False* and *Updating* is *False*. When *Updated* is *True* and *Updating* is *False*, there are no pending changes.
+
[IMPORTANT]
====
If there are pending changes (where both the *Updated* and *Updating* columns are *False*), it is recommended to schedule a maintenance window for a reboot as early as possible. Use the following steps for unpausing the autoreboot process to apply the changes that were queued since the last reboot.
====

* Unpause the autoreboot process:

. Log in to the {product-title} web console as a user with the `cluster-admin` role.

. Click *Compute* -> *MachineConfigPools*.

. On the *MachineConfigPools* page, click either *master* or *worker*, depending upon which nodes you want to pause rebooting for.

. On the *master* or *worker* page, click *YAML*.

. In the YAML, update the `spec.paused` field to `false`.
+
.Sample MachineConfigPool object
[source,yaml]
----
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
# ...
spec:
# ...
  paused: false <1>
# ...
----
<1> Update the `spec.paused` field to `false` to allow rebooting.
+
[NOTE]
====
By unpausing an MCP, the MCO applies all paused changes reboots {op-system-first} as needed.
====

. To verify that the MCP is paused, return to the *MachineConfigPools* page.
+
On the *MachineConfigPools* page, the *Paused* column reports *False* for the MCP you modified.
+
If the MCP is applying any pending changes, the *Updated* column is *False* and the *Updating* column is *True*. When *Updated* is *True* and *Updating* is *False*, there are no further changes being made.

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * support/troubleshooting/troubleshooting-operator-issues.adoc

:_mod-docs-content-type: PROCEDURE
[id="troubleshooting-disabling-autoreboot-mco-cli_{context}"]
= Disabling the Machine Config Operator from automatically rebooting by using the CLI

To avoid unwanted disruptions from changes made by the Machine Config Operator (MCO), you can modify the machine config pool (MCP) using the OpenShift CLI (oc) to prevent the MCO from making any changes to nodes in that pool. This prevents any reboots that would normally be part of the MCO update process.

[NOTE]
====
See second `NOTE` in xref:../../support/troubleshooting/troubleshooting-operator-issues.adoc#troubleshooting-disabling-autoreboot-mco_troubleshooting-operator-issues[Disabling the Machine Config Operator from automatically rebooting].
====

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` role.
* You have installed the OpenShift CLI (`oc`).

.Procedure

To pause or unpause automatic MCO update rebooting:

* Pause the autoreboot process:

. Update the `MachineConfigPool` custom resource to set the `spec.paused` field to `true`.
+
.Control plane (master) nodes
[source,terminal]
----
$ oc patch --type=merge --patch='{"spec":{"paused":true}}' machineconfigpool/master
----
+
.Worker nodes
[source,terminal]
----
$ oc patch --type=merge --patch='{"spec":{"paused":true}}' machineconfigpool/worker
----

. Verify that the MCP is paused:
+
.Control plane (master) nodes
[source,terminal]
----
$ oc get machineconfigpool/master --template='{{.spec.paused}}'
----
+
.Worker nodes
[source,terminal]
----
$ oc get machineconfigpool/worker --template='{{.spec.paused}}'
----
+
.Example output
[source,terminal]
----
true
----
+
The `spec.paused` field is `true` and the MCP is paused.

. Determine if the MCP has pending changes:
+
[source,terminal]
----
# oc get machineconfigpool
----
+
.Example output
----
NAME     CONFIG                                             UPDATED   UPDATING
master   rendered-master-33cf0a1254318755d7b48002c597bf91   True      False
worker   rendered-worker-e405a5bdb0db1295acea08bcca33fa60   False     False
----
+
If the *UPDATED* column is *False* and *UPDATING* is *False*, there are pending changes. When *UPDATED* is *True* and *UPDATING* is *False*, there are no pending changes. In the previous example, the worker node has pending changes. The control plane node does not have any pending changes.
+
[IMPORTANT]
====
If there are pending changes (where both the *Updated* and *Updating* columns are *False*), it is recommended to schedule a maintenance window for a reboot as early as possible. Use the following steps for unpausing the autoreboot process to apply the changes that were queued since the last reboot.
====

* Unpause the autoreboot process:

. Update the `MachineConfigPool` custom resource to set the `spec.paused` field to `false`.
+
.Control plane (master) nodes
[source,terminal]
----
$ oc patch --type=merge --patch='{"spec":{"paused":false}}' machineconfigpool/master
----
+
.Worker nodes
[source,terminal]
----
$ oc patch --type=merge --patch='{"spec":{"paused":false}}' machineconfigpool/worker
----
+
[NOTE]
====
By unpausing an MCP, the MCO applies all paused changes and reboots {op-system-first} as needed.
====
+
. Verify that the MCP is unpaused:
+
.Control plane (master) nodes
[source,terminal]
----
$ oc get machineconfigpool/master --template='{{.spec.paused}}'
----
+
.Worker nodes
[source,terminal]
----
$ oc get machineconfigpool/worker --template='{{.spec.paused}}'
----
+
.Example output
[source,terminal]
----
false
----
+
The `spec.paused` field is `false` and the MCP is unpaused.

. Determine if the MCP has pending changes:
+
[source,terminal]
----
$ oc get machineconfigpool
----
+
.Example output
----
NAME     CONFIG                                   UPDATED  UPDATING
master   rendered-master-546383f80705bd5aeaba93   True     False
worker   rendered-worker-b4c51bb33ccaae6fc4a6a5   False    True
----
+
If the MCP is applying any pending changes, the *UPDATED* column is *False* and the *UPDATING* column is *True*. When *UPDATED* is *True* and *UPDATING* is *False*, there are no further changes being made. In the previous example, the MCO is updating the worker node.

:leveloffset!:

// Refreshing failing subscriptions
// cannot delete resource "clusterserviceversions", "jobs" in API group "operators.coreos.com" in the namespace "openshift-apiserver"
:leveloffset: +1

// Module included in the following assemblies:
//
// * support/troubleshooting/troubleshooting-operator-issues.adoc
// * serverless/install/removing-openshift-serverless.adoc

:_mod-docs-content-type: PROCEDURE
[id="olm-refresh-subs_{context}"]
= Refreshing failing subscriptions

In Operator Lifecycle Manager (OLM), if you subscribe to an Operator that references images that are not accessible on your network, you can find jobs in the `openshift-marketplace` namespace that are failing with the following errors:

.Example output
[source,terminal]
----
ImagePullBackOff for
Back-off pulling image "example.com/openshift4/ose-elasticsearch-operator-bundle@sha256:6d2587129c846ec28d384540322b40b05833e7e00b25cca584e004af9a1d292e"
----

.Example output
[source,terminal]
----
rpc error: code = Unknown desc = error pinging docker registry example.com: Get "https://example.com/v2/": dial tcp: lookup example.com on 10.0.0.1:53: no such host
----

As a result, the subscription is stuck in this failing state and the Operator is unable to install or upgrade.

You can refresh a failing subscription by deleting the subscription, cluster service version (CSV), and other related objects. After recreating the subscription, OLM then reinstalls the correct version of the Operator.

.Prerequisites

* You have a failing subscription that is unable to pull an inaccessible bundle image.
* You have confirmed that the correct bundle image is accessible.

.Procedure

. Get the names of the `Subscription` and `ClusterServiceVersion` objects from the namespace where the Operator is installed:
+
[source,terminal]
----
$ oc get sub,csv -n <namespace>
----
+
.Example output
[source,terminal]
----
NAME                                                       PACKAGE                  SOURCE             CHANNEL
subscription.operators.coreos.com/elasticsearch-operator   elasticsearch-operator   redhat-operators   5.0

NAME                                                                         DISPLAY                            VERSION    REPLACES   PHASE
clusterserviceversion.operators.coreos.com/elasticsearch-operator.5.0.0-65   OpenShift Elasticsearch Operator   5.0.0-65              Succeeded
----

. Delete the subscription:
+
[source,terminal]
----
$ oc delete subscription <subscription_name> -n <namespace>
----

. Delete the cluster service version:
+
[source,terminal]
----
$ oc delete csv <csv_name> -n <namespace>
----

. Get the names of any failing jobs and related config maps in the `openshift-marketplace` namespace:
+
[source,terminal]
----
$ oc get job,configmap -n openshift-marketplace
----
+
.Example output
[source,terminal]
----
NAME                                                                        COMPLETIONS   DURATION   AGE
job.batch/1de9443b6324e629ddf31fed0a853a121275806170e34c926d69e53a7fcbccb   1/1           26s        9m30s

NAME                                                                        DATA   AGE
configmap/1de9443b6324e629ddf31fed0a853a121275806170e34c926d69e53a7fcbccb   3      9m30s
----

. Delete the job:
+
[source,terminal]
----
$ oc delete job <job_name> -n openshift-marketplace
----
+
This ensures pods that try to pull the inaccessible image are not recreated.

. Delete the config map:
+
[source,terminal]
----
$ oc delete configmap <configmap_name> -n openshift-marketplace
----

. Reinstall the Operator using OperatorHub in the web console.

.Verification

* Check that the Operator has been reinstalled successfully:
+
[source,terminal]
----
$ oc get sub,csv,installplan -n <namespace>
----

:leveloffset!:

// Reinstalling Operators after failed uninstallation
// cannot delete resource "customresourcedefinitions"
:leveloffset: +1

// Module included in the following assemblies:
//
// * support/troubleshooting/troubleshooting-operator-issues.adoc

:_mod-docs-content-type: PROCEDURE
[id="olm-reinstall_{context}"]
= Reinstalling Operators after failed uninstallation

You must successfully and completely uninstall an Operator prior to attempting to reinstall the same Operator. Failure to fully uninstall the Operator properly can leave resources, such as a project or namespace, stuck in a "Terminating" state and cause "error resolving resource" messages. For example:

.Example `Project` resource description
----
...
    message: 'Failed to delete all resource types, 1 remaining: Internal error occurred:
      error resolving resource'
...
----

These types of issues can prevent an Operator from being reinstalled successfully.

[WARNING]
====
Forced deletion of a namespace is not likely to resolve "Terminating" state issues and can lead to unstable or unpredictable cluster behavior, so it is better to try to find related resources that might be preventing the namespace from being deleted. For more information, see the link:https://access.redhat.com/solutions/4165791[Red Hat Knowledgebase Solution #4165791], paying careful attention to the cautions and warnings.
====

The following procedure shows how to troubleshoot when an Operator cannot be reinstalled because an existing custom resource definition (CRD) from a previous installation of the Operator is preventing a related namespace from deleting successfully.

.Procedure

. Check if there are any namespaces related to the Operator that are stuck in "Terminating" state:
+
[source,terminal]
----
$ oc get namespaces
----
+
.Example output
----
operator-ns-1                                       Terminating
----

. Check if there are any CRDs related to the Operator that are still present after the failed uninstallation:
+
[source,terminal]
----
$ oc get crds
----
+
[NOTE]
====
CRDs are global cluster definitions; the actual custom resource (CR) instances related to the CRDs could be in other namespaces or be global cluster instances.
====

. If there are any CRDs that you know were provided or managed by the Operator and that should have been deleted after uninstallation, delete the CRD:
+
[source,terminal]
----
$ oc delete crd <crd_name>
----

. Check if there are any remaining CR instances related to the Operator that are still present after uninstallation, and if so, delete the CRs:

.. The type of CRs to search for can be difficult to determine after uninstallation and can require knowing what CRDs the Operator manages. For example, if you are troubleshooting an uninstallation of the etcd Operator, which provides the `EtcdCluster` CRD, you can search for remaining `EtcdCluster` CRs in a namespace:
+
[source,terminal]
----
$ oc get EtcdCluster -n <namespace_name>
----
+
Alternatively, you can search across all namespaces:
+
[source,terminal]
----
$ oc get EtcdCluster --all-namespaces
----

.. If there are any remaining CRs that should be removed, delete the instances:
+
[source,terminal]
----
$ oc delete <cr_name> <cr_instance_name> -n <namespace_name>
----

. Check that the namespace deletion has successfully resolved:
+
[source,terminal]
----
$ oc get namespace <namespace_name>
----
+
[IMPORTANT]
====
If the namespace or other Operator resources are still not uninstalled cleanly, contact Red Hat Support.
====

. Reinstall the Operator using OperatorHub in the web console.

.Verification

* Check that the Operator has been reinstalled successfully:
+
[source,terminal]
----
$ oc get sub,csv,installplan -n <namespace>
----

:leveloffset!:

[role="_additional-resources"]
.Additional resources

* xref:../../operators/admin/olm-deleting-operators-from-cluster.adoc#olm-deleting-operators-from-a-cluster[Deleting Operators from a cluster]
* xref:../../operators/admin/olm-adding-operators-to-cluster.adoc#olm-adding-operators-to-a-cluster[Adding Operators to a cluster]

//# includes=_attributes/common-attributes,modules/olm-status-conditions,modules/olm-status-viewing-cli,modules/olm-cs-status-cli,modules/querying-operator-pod-status,modules/gathering-operator-logs,modules/troubleshooting-disabling-autoreboot-mco,modules/snippets/node-icsp-no-drain,modules/troubleshooting-disabling-autoreboot-mco-console,modules/troubleshooting-disabling-autoreboot-mco-cli,modules/olm-refresh-subs,modules/olm-reinstall

:_mod-docs-content-type: ASSEMBLY
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// common attributes
:product-short-name: OpenShift Dedicated
:toc:
:toc-title:
:experimental:
:imagesdir: images
:OCP: OpenShift Container Platform
:ocp-version: 4.14
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:AWS: Amazon Web Services (AWS)
:GCP: Google Cloud Platform (GCP)
:product-registry: OpenShift image registry
:kebab: image:kebab.png[title="Options menu"]
:rhq-short: Red Hat Quay
:SMProductName: Red Hat OpenShift Service Mesh
:pipelines-title: Red Hat OpenShift Pipelines
:logging-sd: Red Hat OpenShift Logging
:ServerlessProductName: OpenShift Serverless
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:rhoda: Red Hat OpenShift Database Access
:rhoda-short: RHODA
:rhods: Red Hat OpenShift Data Science
:osd: OpenShift Dedicated
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:hcp: hosted control planes
:hcp-title: ROSA with HCP
:hcp-title-first: {product-title} (ROSA) with {hcp} (HCP)
//ROSA CLI variables
:word: Testing this variable let's go www.google.com
[id="cluster-logging-dashboards"]
= Viewing cluster dashboards
:context: cluster-logging-dashboards

toc::[]

The *Logging/Elasticsearch Nodes* and *Openshift Logging* dashboards in the
{product-title} web console
contain in-depth details about your Elasticsearch instance and the individual Elasticsearch nodes that you can use to prevent and diagnose problems.

The *OpenShift Logging* dashboard contains charts that show details about your Elasticsearch instance at a cluster level, including cluster resources, garbage collection, shards in the cluster, and Fluentd statistics.

The *Logging/Elasticsearch Nodes* dashboard contains charts that show details about your Elasticsearch instance, many at node level, including details on indexing, shards, resources, and so forth.

:leveloffset: +1

// Module included in the following assemblies:
//
// * logging/log_visualization/cluster-logging-dashboards.adoc

:_mod-docs-content-type: PROCEDURE
[id="cluster-logging-dashboards-access_{context}"]
= Accessing the Elasticsearch and OpenShift Logging dashboards

You can view the *Logging/Elasticsearch Nodes* and *OpenShift Logging* dashboards in the
{product-title} web console.

.Procedure

To launch the dashboards:

. In the {product-title} web console, click *Observe* -> *Dashboards*.

. On the *Dashboards* page, select *Logging/Elasticsearch Nodes* or *OpenShift Logging* from the *Dashboard* menu.
+
For the *Logging/Elasticsearch Nodes* dashboard, you can select the Elasticsearch node you want to view and set the data resolution.
+
The appropriate dashboard is displayed, showing multiple charts of data.

. Optional: Select a different time range to display or refresh rate for the data from the *Time Range* and *Refresh Interval* menus.

:leveloffset!:

For information on the dashboard charts, see xref:../../logging/log_visualization/cluster-logging-dashboards.adoc#cluster-logging-dashboards-logging_cluster-logging-dashboards[About the OpenShift Logging dashboard] and xref:../../logging/log_visualization/cluster-logging-dashboards.adoc#cluster-logging-dashboards-es_cluster-logging-dashboards[About the Logging/Elastisearch Nodes dashboard].

:leveloffset: +1

// Module included in the following assemblies:
//
// * logging/log_visualization/cluster-logging-dashboards.adoc

:_mod-docs-content-type: CONCEPT
[id="cluster-logging-dashboards-logging_{context}"]
= About the OpenShift Logging dashboard

The *OpenShift Logging* dashboard contains charts that show details about your Elasticsearch instance at a cluster-level that you can use to diagnose and anticipate problems.

.OpenShift Logging charts
[options="header"]
|===
|Metric|Description

|Elastic Cluster Status
a|The current Elasticsearch status:

* ONLINE - Indicates that the Elasticsearch instance is online.
* OFFLINE - Indicates that the Elasticsearch instance is offline.

|Elastic Nodes
|The total number of Elasticsearch nodes in the Elasticsearch instance.

|Elastic Shards
|The total number of Elasticsearch shards in the Elasticsearch instance.

|Elastic Documents
|The total number of Elasticsearch documents in the Elasticsearch instance.

|Total Index Size on Disk
|The total disk space that is being used for the Elasticsearch indices.

|Elastic Pending Tasks
|The total number of Elasticsearch changes that have not been completed, such as index creation, index mapping, shard allocation, or shard failure.

|Elastic JVM GC time
|The amount of time that the JVM spent executing Elasticsearch garbage collection operations in the cluster.

|Elastic JVM GC Rate
|The total number of times that JVM executed garbage activities per second.

|Elastic Query/Fetch Latency Sum
a|* Query latency: The average time each Elasticsearch search query takes to execute.
* Fetch latency: The average time each Elasticsearch search query spends fetching data.

Fetch latency typically takes less time than query latency. If fetch latency is consistently increasing, it might indicate slow disks, data enrichment, or large requests with too many results.

|Elastic Query Rate
|The total queries executed against the Elasticsearch instance per second for each Elasticsearch node.

|CPU
|The amount of CPU used by Elasticsearch, Fluentd, and Kibana, shown for each component.

|Elastic JVM Heap Used
|The amount of JVM memory used. In a healthy cluster, the graph shows regular drops as memory is freed by JVM garbage collection.

|Elasticsearch Disk Usage
|The total disk space used by the Elasticsearch instance for each Elasticsearch node.

|File Descriptors In Use
|The total number of file descriptors used by Elasticsearch, Fluentd, and Kibana.

|FluentD emit count
|The total number of Fluentd messages per second for the Fluentd default output, and the retry count for the default output.

|FluentD Buffer Usage
|The percent of the Fluentd buffer that is being used for chunks. A full buffer might indicate that Fluentd is not able to process the number of logs received.

|Elastic rx bytes
|The total number of bytes that Elasticsearch has received from FluentD, the Elasticsearch nodes, and other sources.

|Elastic Index Failure Rate
|The total number of times per second that an Elasticsearch index fails. A high rate might indicate an issue with indexing.

|FluentD Output Error Rate
|The total number of times per second that FluentD is not able to output logs.

|===

:leveloffset!:
:leveloffset: +1

// Module included in the following assemblies:
//
// * logging/log_visualization/cluster-logging-dashboards.adoc

[id="cluster-logging-dashboards-es_{context}"]
= Charts on the Logging/Elasticsearch nodes dashboard

The *Logging/Elasticsearch Nodes* dashboard contains charts that show details about your Elasticsearch instance, many at node-level, for further diagnostics.

Elasticsearch status::

The *Logging/Elasticsearch Nodes* dashboard contains the following charts about the status of your Elasticsearch instance.

.Elasticsearch status fields
[options="header"]
|===
|Metric|Description

|Cluster status
a|The cluster health status during the selected time period, using the Elasticsearch green, yellow, and red statuses:

* 0 - Indicates that the Elasticsearch instance is in green status, which means that all shards are allocated.
* 1 - Indicates that the Elasticsearch instance is in yellow status, which means that replica shards for at least one shard are not allocated.
* 2 - Indicates that the Elasticsearch instance is in red status, which means that at least one primary shard and its replicas are not allocated.

|Cluster nodes
|The total number of Elasticsearch nodes in the cluster.

|Cluster data nodes
|The number of Elasticsearch data nodes in the cluster.

|Cluster pending tasks
|The number of cluster state changes that are not finished and are waiting in a cluster queue, for example, index creation, index deletion, or shard allocation. A growing trend indicates that the cluster is not able to keep up with changes.

|===

Elasticsearch cluster index shard status::

Each Elasticsearch index is a logical group of one or more shards, which are basic units of persisted data. There are two types of index shards: primary shards, and replica shards. When a document is indexed into an index, it is stored in one of its primary shards and copied into every replica of that shard. The number of primary shards is specified when the index is created, and the number cannot change during index lifetime. You can change the number of replica shards at any time.

The index shard can be in several states depending on its lifecycle phase or events occurring in the cluster. When the shard is able to perform search and indexing requests, the shard is active. If the shard cannot perform these requests, the shard is non–active. A shard might be non-active if the shard is initializing, reallocating, unassigned, and so forth.

Index shards consist of a number of smaller internal blocks, called index segments, which are physical representations of the data. An index segment is a relatively small, immutable Lucene index that is created when Lucene commits newly-indexed data. Lucene, a search library used by Elasticsearch, merges index segments into larger segments in the background to keep the total number of segments low. If the process of merging segments is slower than the speed at which new segments are created, it could indicate a problem.

When Lucene performs data operations, such as a search operation, Lucene performs the operation against the index segments in the relevant index. For that purpose, each segment contains specific data structures that are loaded in the memory and mapped. Index mapping can have a significant impact on the memory used by segment data structures.

The *Logging/Elasticsearch Nodes* dashboard contains the following charts about the Elasticsearch index shards.

.Elasticsearch cluster shard status charts
[options="header"]

|===
|Metric|Description

|Cluster active shards
|The number of active primary shards and the total number of shards, including replicas, in the cluster. If the number of shards grows higher, the cluster performance can start degrading.

|Cluster initializing shards
|The number of non-active shards in the cluster. A non-active shard is one that is initializing, being reallocated to a different node, or is unassigned. A cluster typically has non–active shards for short periods. A growing number of non–active shards over longer periods could indicate a problem.

|Cluster relocating shards
|The number of shards that Elasticsearch is relocating to a new node. Elasticsearch relocates nodes for multiple reasons, such as high memory use on a node or after a new node is added to the cluster.

|Cluster unassigned shards
|The number of unassigned shards. Elasticsearch shards might be unassigned for reasons such as a new index being added or the failure of a node.

|===

Elasticsearch node metrics::

Each Elasticsearch node has a finite amount of resources that can be used to process tasks. When all the resources are being used and Elasticsearch attempts to perform a new task, Elasticsearch puts the tasks into a queue until some resources become available.

The *Logging/Elasticsearch Nodes* dashboard contains the following charts about resource usage for a selected node and the number of tasks waiting in the Elasticsearch queue.

.Elasticsearch node metric charts
[options="header"]
|===
|Metric|Description

|ThreadPool tasks
|The number of waiting tasks in individual queues, shown by task type. A long–term accumulation of tasks in any queue could indicate node resource shortages or some other problem.

|CPU usage
|The amount of CPU being used by the selected Elasticsearch node as a percentage of the total CPU allocated to the host container.

|Memory usage
|The amount of memory being used by the selected Elasticsearch node.

|Disk usage
|The total disk space being used for index data and metadata on the selected Elasticsearch node.

|Documents indexing rate
|The rate that documents are indexed on the selected Elasticsearch node.

|Indexing latency
|The time taken to index the documents on the selected Elasticsearch node. Indexing latency can be affected by many factors, such as JVM Heap memory and overall load. A growing latency indicates a resource capacity shortage in the instance.

|Search rate
|The number of search requests run on the selected Elasticsearch node.

|Search latency
|The time taken to complete search requests on the selected Elasticsearch node. Search latency can be affected by many factors. A growing latency indicates a resource capacity shortage in the instance.

|Documents count (with replicas)
|The number of Elasticsearch documents stored on the selected Elasticsearch node, including documents stored in both the primary shards and replica shards that are allocated on the node.

|Documents deleting rate
|The number of Elasticsearch documents being deleted from any of the index shards that are allocated to the selected Elasticsearch node.

|Documents merging rate
|The number of Elasticsearch documents being merged in any of index shards that are allocated to the selected Elasticsearch node.

|===

Elasticsearch node fielddata::

link:https://www.elastic.co/guide/en/elasticsearch/reference/6.8/fielddata.html[_Fielddata_] is an Elasticsearch data structure that holds lists of terms in an index and is kept in the JVM Heap. Because fielddata building is an expensive operation, Elasticsearch caches the fielddata structures. Elasticsearch can evict a fielddata cache when the underlying index segment is deleted or merged, or if there is not enough JVM HEAP memory for all the fielddata caches.

The *Logging/Elasticsearch Nodes* dashboard contains the following charts about Elasticsearch fielddata.

.Elasticsearch node fielddata charts
[options="header"]
|===
|Metric|Description

|Fielddata memory size
|The amount of JVM Heap used for the fielddata cache on the selected Elasticsearch node.

|Fielddata evictions
|The number of fielddata structures that were deleted from the selected Elasticsearch node.

|===

Elasticsearch node query cache::

If the data stored in the index does not change, search query results are cached in a node-level query cache for reuse by Elasticsearch.

The *Logging/Elasticsearch Nodes* dashboard contains the following charts about the Elasticsearch node query cache.

.Elasticsearch node query charts
[options="header"]
|===
|Metric|Description

|Query cache size
|The total amount of memory used for the query cache for all the shards allocated to the selected Elasticsearch node.

|Query cache evictions
|The number of query cache evictions on the selected Elasticsearch node.

|Query cache hits
|The number of query cache hits on the selected Elasticsearch node.

|Query cache misses
|The number of query cache misses on the selected Elasticsearch node.

|===

Elasticsearch index throttling::

When indexing documents, Elasticsearch stores the documents in index segments, which are physical representations of the data. At the same time, Elasticsearch periodically merges smaller segments into a larger segment as a way to optimize resource use. If the indexing is faster then the ability to merge segments, the merge process does not complete quickly enough, which can lead to issues with searches and performance. To prevent this situation, Elasticsearch throttles indexing, typically by reducing the number of threads allocated to indexing down to a single thread.

The *Logging/Elasticsearch Nodes* dashboard contains the following charts about Elasticsearch index throttling.

.Index throttling charts
[options="header"]
|===
|Metric|Description

|Indexing throttling
|The amount of time that Elasticsearch has been throttling the indexing operations on the selected Elasticsearch node.

|Merging throttling
|The amount of time that Elasticsearch has been throttling the segment merge operations on the selected Elasticsearch node.

|===

Node JVM Heap statistics::

The *Logging/Elasticsearch Nodes* dashboard contains the following charts about JVM Heap operations.

.JVM Heap statistic charts
[options="header"]
|===
|Metric|Description

|Heap used
|The amount of the total allocated JVM Heap space that is used on the selected Elasticsearch node.

|GC count
|The number of garbage collection operations that have been run on the selected Elasticsearch node, by old and young garbage collection.

|GC time
|The amount of time that the JVM spent running garbage collection operations on the selected Elasticsearch node, by old and young garbage collection.

|===

:leveloffset!:

//# includes=_attributes/common-attributes,_attributes/attributes-openshift-dedicated,modules/cluster-logging-dashboards-access,modules/cluster-logging-dashboards-logging,modules/cluster-logging-dashboards-es

:_mod-docs-content-type: ASSEMBLY
:context: cluster-logging-loki
[id="cluster-logging-loki"]
= Logging using LokiStack
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS

toc::[]

In {logging} documentation, _LokiStack_ refers to the {logging} supported combination of Loki and web proxy with {product-title} authentication integration. LokiStack's proxy uses {product-title} authentication to enforce multi-tenancy. _Loki_ refers to the log store as either the individual component or an external store.

Loki is a horizontally scalable, highly available, multi-tenant log aggregation system currently offered as an alternative to Elasticsearch as a log store for the {logging}. Elasticsearch indexes incoming log records completely during ingestion. Loki only indexes a few fixed labels during ingestion and defers more complex parsing until after the logs have been stored. This means Loki can collect logs more quickly. You can query Loki by using the link:https://grafana.com/docs/loki/latest/logql/[LogQL log query language].

:leveloffset: +1

// Module is included in the following assemblies:
//cluster-logging-loki.adoc

:_mod-docs-content-type: CONCEPT
[id="loki-deployment-sizing_{context}"]
= Deployment Sizing

Sizing for Loki follows the format of `N<x>._<size>_` where the value `<N>` is number of instances and `<size>` specifies performance capabilities.

[NOTE]
====
1x.extra-small is for demo purposes only, and is not supported.
====

.Loki Sizing
[options="header"]
|========================================================================================
|                              | 1x.extra-small  | 1x.small            | 1x.medium
| *Data transfer*              | Demo use only.  | 500GB/day           | 2TB/day
| *Queries per second (QPS)*   | Demo use only.  | 25-50 QPS at 200ms  | 25-75 QPS at 200ms
| *Replication factor*         | None            | 2                   | 3
| *Total CPU requests*         | 5 vCPUs         | 36 vCPUs            | 54 vCPUs
| *Total Memory requests*      | 7.5Gi           | 63Gi                | 139Gi
| *Total Disk requests*        | 150Gi           | 300Gi               | 450Gi
|========================================================================================

[id="CRD-API-support_{context}"]
== Supported API Custom Resource Definitions
LokiStack development is ongoing, not all APIs are supported currently supported.

[options="header"]
|=====================================================================
| CustomResourceDefinition (CRD)| ApiVersion           | Support state
| LokiStack      | lokistack.loki.grafana.com/v1       | Supported in 5.5
| RulerConfig    | rulerconfig.loki.grafana/v1beta1    | Technology Preview
| AlertingRule   | alertingrule.loki.grafana/v1beta1   | Technology Preview
| RecordingRule  | recordingrule.loki.grafana/v1beta1  | Technology Preview
|=====================================================================

:FeatureName: Usage of `RulerConfig`, `AlertingRule` and `RecordingRule` custom resource definitions (CRDs).
// When including this file, ensure that {FeatureName} is set immediately before
// the include. Otherwise it will result in an incorrect replacement.

[IMPORTANT]
====
[subs="attributes+"]
{FeatureName} is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.

For more information about the support scope of Red Hat Technology Preview features, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].
====
// Undefine {FeatureName} attribute, so that any mistakes are easily spotted
:!FeatureName:

:leveloffset!:

//include::modules/cluster-logging-loki-deploy.adoc[leveloffset=+1]

:leveloffset: +1

// Module included in the following assemblies:

//  cluster-logging-loki.adoc

:_mod-docs-content-type: PROCEDURE
[id="logging-creating-new-group-cluster-admin-user-role_{context}"]
= Creating a new group for the cluster-admin user role

// Text snippet included in the following assemblies:
//
//
// Text snippet included in the following modules:
//
// * modules/logging-creating-new-group-cluster-admin-user-role.adoc
// * modules/network-observability-lokistack-create.adoc
//
:_mod-docs-content-type: SNIPPET

[IMPORTANT]
====
Querying application logs for multiple namespaces as a `cluster-admin` user, where the sum total of characters of all of the namespaces in the cluster is greater than 5120, results in the error `Parse error: input size too long (XXXX > 5120)`. For better control over access to logs in LokiStack, make the `cluster-admin` user a member of the `cluster-admin` group. If the `cluster-admin` group does not exist, create it and add the desired users to it.
====

Use the following procedure to create a new group for users with `cluster-admin` permissions.

.Procedure

. Enter the following command to create a new group:
+
[source,terminal]
----
$ oc adm groups new cluster-admin
----
. Enter the following command to add the desired user to the `cluster-admin` group:
+
[source,terminal]
----
$ oc adm groups add-users cluster-admin <username>
----
. Enter the following command to add `cluster-admin` user role to the group:
+
[source,terminal]
----
$ oc adm policy add-cluster-role-to-group cluster-admin cluster-admin
----

:leveloffset!:

:leveloffset: +1

// Module is included in the following assemblies:
//
:_content-type: PROCEDURE
[id="logging-loki-gui-install_{context}"]
= Installing logging Operators using the {product-title} web console

To install and configure logging on your {product-title} cluster, additional Operators must be installed. This can be done from the Operator Hub within the web console.

{Product-title} Operators use custom resources (CR) to manage applications and their components. High-level configuration and settings are provided by the user within a CR. The Operator translates high-level directives into low-level actions, based on best practices embedded within the Operator’s logic. A custom resource definition (CRD) defines a CR and lists all the configurations available to users of the Operator. Installing an Operator creates the CRDs, which are then used to generate CRs.

.Prerequisites

* Supported object store (AWS S3, Google Cloud Storage, Azure, Swift, Minio, OpenShift Data Foundation)

.Procedure

. Install the `Loki Operator`:

.. In the {product-title} web console, click *Operators* -> *OperatorHub*.

.. Type `Loki Operator` in the filter by keyword box. Choose *Loki Operator* from the list of available Operators and click *Install*.
+
[NOTE]
====
The Community Loki Operator is not supported by Red Hat.
====

.. On the Install Operator page, for *Update Channel* select *stable*.
+
--
// Text snippet included in the following assemblies:
//
// logging/logging_release_notes/logging-5-7-release-notes.adoc
// logging/logging_release_notes/logging-5-8-release-notes.adoc
//
// Text snippet included in the following modules:
//
//
:_mod-docs-content-type: SNIPPET

[NOTE]
====
The `stable` channel only provides updates to the most recent release of logging. To continue receiving updates for prior releases, you must change your subscription channel to `stable-X` where `X` is the version of logging you have installed.
====
--

As the Loki Operator must be deployed to the global operator group namespace `openshift-operators-redhat`, *Installation mode* and *Installed Namespace* is already be selected. If this namespace does not already exist, it is created for you.

.. Select *Enable operator-recommended cluster monitoring on this namespace.*

This option sets the `openshift.io/cluster-monitoring: "true"` label in the Namespace object. You must select this option to ensure that cluster monitoring scrapes the `openshift-operators-redhat` namespace.

.. For *Update approval* select *Automatic*, then click *Install*.
+
--
// Text snippet included in the following assemblies:
//
//
// Text snippet included in the following modules:
//
//
:_mod-docs-content-type: SNIPPET

If the approval strategy in the subscription is set to *Automatic*, the update process initiates as soon as a new Operator version is available in the selected channel. If the approval strategy is set to *Manual*, you must manually approve pending updates.
--

. Install the `Red Hat OpenShift Logging` Operator:

.. In the {product-title} web console, click *Operators* -> *OperatorHub*.

.. Type `OpenShift Logging` in the filter by keyword box. Choose *Red Hat OpenShift Logging* from the list of available Operators and click *Install*.

.. On the Install Operator page, under *Update channel* select *stable*.
+
--
// Text snippet included in the following assemblies:
//
// logging/logging_release_notes/logging-5-7-release-notes.adoc
// logging/logging_release_notes/logging-5-8-release-notes.adoc
//
// Text snippet included in the following modules:
//
//
:_mod-docs-content-type: SNIPPET

[NOTE]
====
The `stable` channel only provides updates to the most recent release of logging. To continue receiving updates for prior releases, you must change your subscription channel to `stable-X` where `X` is the version of logging you have installed.
====
--

As the `Red Hat OpenShift Logging` Operator is only deployable to the `openshift-logging` namespace, *Installation mode* and *Installed Namespace* is already selected. If this namespace does not already exist, it is created for you.

.. If you are creating the `openshift-logging` namespace, select the option to *Enable Operator recommended cluster monitoring on this Namespace*.
+
[NOTE]
====
If the `openshift-logging` namespace already exists, you must add the namespace label, `openshift.io/cluster-monitoring: "true"`, to enable metrics service discovery.
====

.. Under *Update approval* select *Automatic*.
+
--
// Text snippet included in the following assemblies:
//
//
// Text snippet included in the following modules:
//
//
:_mod-docs-content-type: SNIPPET

If the approval strategy in the subscription is set to *Automatic*, the update process initiates as soon as a new Operator version is available in the selected channel. If the approval strategy is set to *Manual*, you must manually approve pending updates.
--

.. For *Console plugin* select *Enable*, then click *Install*.

The Operators should now be available to all users and projects that use this cluster.

. Verify the operator installations:
.. Navigate to *Operators* -> *Installed Operators*.
.. Make sure the *openshift-logging* project is selected.
.. In the *Status* column, verify that you see green checks with *InstallSucceeded* and the text *Up to date*, below.
+
[NOTE]
====
An Operator might display a `Failed` status before the installation finishes. If the Operator install completes with an `InstallSucceeded` message, refresh the page.
====

:leveloffset!:

:leveloffset: +1

// Module is included in the following assemblies:
// logging/cluster-logging-loki.adoc
:_content-type: PROCEDURE
[id="logging-loki-cli-install_{context}"]
= Installing logging Operators using the {product-title} CLI

To install and configure logging on your {product-title} cluster, additional Operators must be installed. This can be done from the {product-title} CLI.

{Product-title} Operators use custom resources (CR) to manage applications and their components. High-level configuration and settings are provided by the user within a CR. The Operator translates high-level directives into low-level actions, based on best practices embedded within the Operator’s logic. A custom resource definition (CRD) defines a CR and lists all the configurations available to users of the Operator. Installing an Operator creates the CRDs, which are then used to generate CRs.

.Prerequisites

* Supported object store (AWS S3, Google Cloud Storage, Azure, Swift, Minio, OpenShift Data Foundation)

.Procedure

. Install the `Loki Operator` by creating the following objects:

.. Create a Subscription object YAML file (for example, `olo-sub.yaml`) to
subscribe a namespace to the Loki Operator using the template below:
+
[source,terminal]
----
$ oc create -f <file-name>.yaml
----
+
[source,yaml]
----
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: loki-operator
  namespace: openshift-operators-redhat <1>
spec:
  charsion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: loki-operator
  namespace: openshift-operators-redhat <1>
spec:
  channel: stable <2>
  name: loki-operator
  source: redhat-operators <3>
  sourceNamespace: openshift-marketplace
----
<1> You must specify the `openshift-operators-redhat` namespace.
<2> Specify `stable`, or `stable-5.<y>` as the channel.
<3> Specify `redhat-operators`. If your {product-title} cluster is installed on a restricted network, also known as a disconnected cluster, specify the name of the CatalogSource object you created when you configured the Operator Lifecycle Manager (OLM).

. Create a LokiStack instance:

.. Create an instance object YAML file (for example, `logging-loki.yaml`) using the template below:
+
[source,terminal]
----
$ oc create -f <file-name>.yaml
----
+
[source,yaml]
----
apiVersion: loki.grafana.com/v1
kind: LokiStack
metadata:
  name: logging-loki
  namespace: openshift-logging
spec:
  size: 1x.small # <1>
  storage:
    schemas:
    - version: v12
      effectiveDate: "2022-06-01"
    secret:
      name: logging-loki-s3 # <2>
      type: s3 # <3>
  storageClassName: <storage_class_name> # <4>
  tenants:
    mode: openshift-logging
----
<1> Supported size options for production instances of Loki are `1x.small` and `1x.medium`.
<2> Enter the name of your log store secret.
<3> Enter the type of your log store secret.
<4> Enter the name of an existing storage class for temporary storage. For best performance, specify a storage class that allocates block storage. Available storage classes for your cluster can be listed using `oc get storageclasses`.

. Install the `Red Hat OpenShift Logging` Operator by creating the following objects:

.. Create an Operator Group object YAML file (for example, `olo-og.yaml`) using the template below:
+
[source,terminal]
----
$ oc create -f <file-name>.yaml
----
+
[source,yaml]
----
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: cluster-logging
  namespace: openshift-logging # <1>
spec:
  targetNamespaces:
  - openshift-logging
----
<1> You must specify the `openshift-logging` namespace.

.. Create a Subscription object YAML file (for example, `olo-sub.yaml`) to
subscribe a namespace to the Red Hat OpenShift Logging Operator using the template below:
+
[source,terminal]
----
$ oc create -f <file-name>.yaml
----
+
[source,yaml]
----
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: cluster-logging
  namespace: openshift-logging # <1>
spec:
  channel: stable # <2>
  name: cluster-logging
  source: redhat-operators # <3>
  sourceNamespace: openshift-marketplace
----
<1> You must specify the `openshift-logging` namespace.
<2> Specify `stable`, or `stable-5.<y>` as the channel.
<3> Specify `redhat-operators`. If your {product-title} cluster is installed on a restricted network, also known as a disconnected cluster, specify the name of the CatalogSource object you created when you configured the Operator Lifecycle Manager (OLM).


.. Verify the Operator installation.
+
There should be a Red Hat OpenShift Logging Operator in the `openshift-logging` namespace. The Version number might be different than what is shown.
+
[source,terminal]
----
$ oc get csv -n openshift-logging
----
+
.Example output
[source,terminal]
----
NAME                            DISPLAY                            VERSION   REPLACES                        PHASE
cluster-logging.v5.7.4          Red Hat OpenShift Logging          5.7.4     cluster-logging.v5.7.3          Succeeded
----

. Create an OpenShift Logging instance:

.. Create an instance object YAML file (for example, `olo-instance.yaml`) using the template below:
+
[source,terminal]
----
$ oc create -f <file-name>.yaml
----
+
[source,yaml]
----
apiVersion: logging.openshift.io/v1
kind: ClusterLogging
metadata:
  name: instance
  namespace: openshift-logging
spec:
  logStore:
    type: lokistack
    lokistack:
      name: logging-loki
  collection:
    type: vector
----

. Verify the installation by listing the pods in the *openshift-logging* project.
+
You should see several pods for components of the Logging subsystem, similar to the following list:
+
[source,terminal]
----
$ oc get pods -n openshift-logging
----
+
.Example output
[source,terminal]
----
$ oc get pods -n openshift-logging
NAME                                               READY   STATUS    RESTARTS   AGE
cluster-logging-operator-fb7f7cf69-8jsbq           1/1     Running   0          98m
collector-222js                                    2/2     Running   0          18m
collector-g9ddv                                    2/2     Running   0          18m
collector-hfqq8                                    2/2     Running   0          18m
collector-sphwg                                    2/2     Running   0          18m
collector-vv7zn                                    2/2     Running   0          18m
collector-wk5zz                                    2/2     Running   0          18m
logging-view-plugin-6f76fbb78f-n2n4n               1/1     Running   0          18m
lokistack-sample-compactor-0                       1/1     Running   0          42m
lokistack-sample-distributor-7d7688bcb9-dvcj8      1/1     Running   0          42m
lokistack-sample-gateway-5f6c75f879-bl7k9          2/2     Running   0          42m
lokistack-sample-gateway-5f6c75f879-xhq98          2/2     Running   0          42m
lokistack-sample-index-gateway-0                   1/1     Running   0          42m
lokistack-sample-ingester-0                        1/1     Running   0          42m
lokistack-sample-querier-6b7b56bccc-2v9q4          1/1     Running   0          42m
lokistack-sample-query-frontend-84fb57c578-gq2f7   1/1     Running   0          42m
----

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * logging/cluster-logging-loki.adoc

:_mod-docs-content-type: CONCEPT
[id="logging-loki-restart-hardening_{context}"]
= LokiStack behavior during cluster restarts

In logging version 5.8 and newer versions, when an {product-title} cluster is restarted, LokiStack ingestion and the query path continue to operate within the available CPU and memory resources available for the node. This means that there is no downtime for the LokiStack during {product-title} cluster updates. This behavior is achieved by using `PodDisruptionBudget` resources. The Loki Operator provisions `PodDisruptionBudget` resources for Loki, which determine the minimum number of pods that must be available per component to ensure normal operations under certain conditions.

:leveloffset!:

[role="_additional-resources"]
.Additional resources
* link:https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#pod-disruption-budgets[Pod disruption budgets Kubernetes documentation]

:leveloffset: +1

// Module included in the following assemblies:
//
// * logging/cluster-logging-loki.adoc

:_mod-docs-content-type: CONCEPT
[id="logging-loki-reliability-hardening_{context}"]
= Configuring Loki to tolerate node failure

In the {logging} 5.8 and later versions, the Loki Operator supports setting pod anti-affinity rules to request that pods of the same component are scheduled on different available nodes in the cluster.

// Snippets included in the following assemblies and modules:
//
// * /nodes/scheduling/nodes-scheduler-pod-affinity.adoc
// * /modules/logging-loki-reliability-hardening.adoc

:_mod-docs-content-type: SNIPPET

Affinity is a property of pods that controls the nodes on which they prefer to be scheduled. Anti-affinity is a property of pods
that prevents a pod from being scheduled on a node.

In {product-title}, _pod affinity_ and _pod anti-affinity_ allow you to constrain which nodes your pod is eligible to be scheduled on based on the key-value labels on other pods.

The Operator sets default, preferred `podAntiAffinity` rules for all Loki components, which includes the `compactor`, `distributor`, `gateway`, `indexGateway`, `ingester`, `querier`, `queryFrontend`, and `ruler` components.

You can override the preferred `podAntiAffinity` settings for Loki components by configuring required settings in the `requiredDuringSchedulingIgnoredDuringExecution` field:

.Example user settings for the ingester component
[source,yaml]
----
apiVersion: loki.grafana.com/v1
kind: LokiStack
metadata:
  name: logging-loki
  namespace: openshift-logging
spec:
# ...
  template:
    ingester:
      podAntiAffinity:
      # ...
        requiredDuringSchedulingIgnoredDuringExecution: <1>
        - labelSelector:
            matchLabels: <2>
              app.kubernetes.io/component: ingester
          topologyKey: kubernetes.io/hostname
# ...
----
<1> The stanza to define a required rule.
<2> The key-value pair (label) that must be matched to apply the rule.

:leveloffset!:

[role="_additional-resources"]
.Additional resources
* link:https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.24/#podantiaffinity-v1-core[`PodAntiAffinity` v1 core Kubernetes documentation]
* link:https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity[Assigning Pods to Nodes Kubernetes documentation]

* xref:../nodes/scheduling/nodes-scheduler-pod-affinity.adoc#nodes-scheduler-pod-affinity[Placing pods relative to other pods using affinity and anti-affinity rules]


:leveloffset: +1

// Module included in the following assemblies:
//
// * logging/cluster-logging-loki.adoc

:_mod-docs-content-type: CONCEPT
[id="logging-loki-zone-aware-rep_{context}"]
= Zone aware data replication

In the {logging} 5.8 and later versions, the Loki Operator offers support for zone-aware data replication through pod topology spread constraints. Enabling this feature enhances reliability and safeguards against log loss in the event of a single zone failure. When configuring the deployment size as `1x.extra.small`, `1x.small`, or `1x.medium,` the `replication.factor` field is automatically set to 2.

To ensure proper replication, you need to have at least as many availability zones as the replication factor specifies. While it is possible to have more availability zones than the replication factor, having fewer zones can lead to write failures. Each zone should host an equal number of instances for optimal operation.

.Example LokiStack CR with zone replication enabled
[source,yaml]
----
apiVersion: loki.grafana.com/v1
kind: LokiStack
metadata:
 name: logging-loki
 namespace: openshift-logging
spec:
 replicationFactor: 2 # <1>
 replication:
   factor: 2 # <2>
   zones:
   -  maxSkew: 1 # <3>
      topologyKey: topology.kubernetes.io/zone # <4>
----
<1> Deprecated field, values entered are overwritten by `replication.factor`.
<2> This value is automatically set when deployment size is selected at setup.
<3> The maximum difference in number of pods between any two topology domains. The default is 1, and you cannot specify a value of 0.
<4> Defines zones in the form of a topology key that corresponds to a node label.

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * logging/cluster-logging-loki.adoc

:_mod-docs-content-type: PROCEDURE
[id="logging-loki-zone-fail-recovery_{context}"]
= Recovering Loki pods from failed zones

In {product-title} a zone failure happens when specific availability zone resources become inaccessible. Availability zones are isolated areas within a cloud provider's data center, aimed at enhancing redundancy and fault tolerance. If your {product-title} cluster isn't configured to handle this, a zone failure can lead to service or data loss.

Loki pods are part of a link:https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/[StatefulSet], and they come with Persistent Volume Claims (PVCs) provisioned by a `StorageClass` object. Each Loki pod and its PVCs reside in the same zone. When a zone failure occurs in a cluster, the StatefulSet controller automatically attempts to recover the affected pods in the failed zone.

[WARNING]
====
The following procedure will delete the PVCs in the failed zone, and all data contained therein.  To avoid complete data loss the replication factor field of the `LokiStack` CR should always be set to a value greater than 1 to ensure that Loki is replicating.
====

.Prerequisites
* Logging version 5.8 or later.
* Verify your `LokiStack` CR has a replication factor greater than 1.
* Zone failure detected by the control plane, and nodes in the failed zone are marked by cloud provider integration.

The StatefulSet controller automatically attempts to reschedule pods in a failed zone. Because the associated PVCs are also in the failed zone, automatic rescheduling to a different zone does not work. You must manually delete the PVCs in the failed zone to allow successful re-creation of the stateful Loki Pod and its provisioned PVC in the new zone.


.Procedure
. List the pods in `Pending` status by running the following command:
+
[source,terminal]
----
oc get pods --field-selector status.phase==Pending -n openshift-logging
----
+
.Example `oc get pods` output
[source,terminal]
----
NAME                           READY   STATUS    RESTARTS   AGE # <1>
logging-loki-index-gateway-1   0/1     Pending   0          17m
logging-loki-ingester-1        0/1     Pending   0          16m
logging-loki-ruler-1           0/1     Pending   0          16m
----
<1> These pods are in `Pending` status because their corresponding PVCs are in the failed zone.

.. List the PVCs in `Pending` status by running the following command:
+
[source,terminal]
----
oc get pvc -o=json -n openshift-logging | jq '.items[] | select(.status.phase == "Pending") | .metadata.name' -r
----
+
.Example `oc get pvc` output
[source,terminal]
----
storage-logging-loki-index-gateway-1
storage-logging-loki-ingester-1
wal-logging-loki-ingester-1
storage-logging-loki-ruler-1
wal-logging-loki-ruler-1
----

.. Delete the PVC(s) for a pod by running the following command:
+
[source,terminal]
----
oc delete pvc __<pvc_name>__  -n openshift-logging
----
+
.. Then delete the pod(s) by running the following command:
[source,terminal]
----
oc delete pod __<pod_name>__  -n openshift-logging
----

Once these objects have been successfully deleted, they should automatically be rescheduled in an available zone.

[id="logging-loki-zone-fail-term-state_{context}"]
== Troubleshooting PVC in a terminating state

The PVCs might hang in the terminating state without being deleted, if PVC metadata finalizers are set to `kubernetes.io/pv-protection`. Removing the finalizers should allow the PVCs to delete successfully.

. Remove the finalizer for each PVC by running the command below, then retry deletion.

[source,terminal]
----
oc patch pvc __<pvc_name>__ -p '{"metadata":{"finalizers":null}}' -n openshift-logging
----

:leveloffset!:

[role="_additional-resources"]
.Additional resources
* link:https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/#spread-constraint-definition[Topology spread constraints Kubernetes documentation]

* link:https://kubernetes.io/docs/setup/best-practices/multiple-zones/#storage-access-for-zones[Kubernetes storage documentation].


:leveloffset: +1

// Module included in the following assemblies:
//
// * logging/cluster-logging-loki.adoc

:_content-type: CONCEPT
[id="logging-loki-log-access_{context}"]
= Fine grained access for Loki logs
In {logging} 5.8 and later, the ClusterLogging Operator does not grant all users access to logs by default.  As an administrator, you need to configure your users access unless the Operator was upgraded and prior configurations are in place. Depending on your configuration and need, you can configure fine grain access to logs using the following:

* Cluster wide policies
* Namespace scoped policies
* Creation of custom admin groups

As an administrator, you need to create the role bindings and cluster role bindings appropriate for your deployment. The ClusterLogging Operator provides the following cluster roles:

* `cluster-logging-application-view` grants permission to read application logs.
* `cluster-logging-infrastructure-view` grants permission to read infrastructure logs.
* `cluster-logging-audit-view` grants permission to read audit logs.

If you have upgraded from a prior version, an additional cluster role `logging-application-logs-reader` and associated cluster role binding `logging-all-authenticated-application-logs-reader` provide backward compatibility, allowing any authenticated user read access in their namespaces.

[NOTE]
====
Users with access by namespace must provide a namespace when querying application logs.
====

== Cluster wide access
Cluster role binding resources reference cluster roles, and set permissions cluster wide.

.Example ClusterRoleBinding
[source,yaml]
----
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: logging-all-application-logs-reader
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-logging-application-view # <1>
subjects: # <2>
- kind: Group
  name: system:authenticated
  apiGroup: rbac.authorization.k8s.io
----
<1> Additional `ClusterRoles` are `cluster-logging-infrastructure-view`, and `cluster-logging-audit-view`.
<2> Specifies the users or groups this object applies to.

== Namespaced access

`RoleBinding` resources can be used with `ClusterRole` objects to define the namespace a user or group has access to logs for.

.Example RoleBinding
[source,yaml]
----
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: allow-read-logs
  namespace: log-test-0 # <1>
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-logging-application-view
subjects:
- kind: User
  apiGroup: rbac.authorization.k8s.io
  name: testuser-0
----
<1> Specifies the namespace this `RoleBinding` applies to.

== Custom admin group access

If you have a large deployment with a number of users who require broader permissions, you can create a custom group using the `adminGroup` field. Users who are members of any group specified in the `adminGroups` field of the LokiStack CR are considered admins. Admin users have access to all application logs in all namespaces, if they also get assigned the `cluster-logging-application-view` role.

.Example LokiStack CR
[source,yaml]
----
apiVersion: loki.grafana.com/v1
kind: LokiStack
metadata:
  name: logging-loki
  namespace: openshift-logging
spec:
  tenants:
    mode: openshift-logging # <1>
    openshift:
      adminGroups: # <2>
      - cluster-admin
      - custom-admin-group # <3>
----
<1> Custom admin groups are only available in this mode.
<2> Entering an empty list `[]` value for this field disables admin groups.
<3> Overrides the default groups (`system:cluster-admins`, `cluster-admin`, `dedicated-admin`)

:leveloffset!:

[role="_additional-resources"]
.Additional resources


:leveloffset: +1

// Module included in the following assemblies:
//
// logging/cluster-logging-loki.adoc

:_mod-docs-content-type: PROCEDURE
[id="logging-loki-retention_{context}"]
= Enabling stream-based retention with Loki

With Logging version 5.6 and higher, you can configure retention policies based on log streams. Rules for these may be set globally, per tenant, or both. If you configure both, tenant rules apply before global rules.

. To enable stream-based retention, create a `LokiStack` custom resource (CR):
+
.Example global stream-based retention
[source,yaml]
----
apiVersion: loki.grafana.com/v1
kind: LokiStack
metadata:
  name: logging-loki
  namespace: openshift-logging
spec:
  limits:
    global: <1>
      retention: <2>
        days: 20
        streams:
        - days: 4
          priority: 1
          selector: '{kubernetes_namespace_name=~"test.+"}' <3>
        - days: 1
          priority: 1
          selector: '{log_type="infrastructure"}'
  managementState: Managed
  replicationFactor: 1
  size: 1x.small
  storage:
    schemas:
    - effectiveDate: "2020-10-11"
      version: v11
    secret:
      name: logging-loki-s3
      type: aws
  storageClassName: standard
  tenants:
    mode: openshift-logging
----
<1> Sets retention policy for all log streams. *Note: This field does not impact the retention period for stored logs in object storage.*
<2> Retention is enabled in the cluster when this block is added to the CR.
<3> Contains the link:https://grafana.com/docs/loki/latest/logql/query_examples/#query-examples[LogQL query] used to define the log stream.
+
.Example per-tenant stream-based retention
[source,yaml]
----
apiVersion: loki.grafana.com/v1
kind: LokiStack
metadata:
  name: logging-loki
  namespace: openshift-logging
spec:
  limits:
    global:
      retention:
        days: 20
    tenants: <1>
      application:
        retention:
          days: 1
          streams:
            - days: 4
              selector: '{kubernetes_namespace_name=~"test.+"}' <2>
      infrastructure:
        retention:
          days: 5
          streams:
            - days: 1
              selector: '{kubernetes_namespace_name=~"openshift-cluster.+"}'
  managementState: Managed
  replicationFactor: 1
  size: 1x.small
  storage:
    schemas:
    - effectiveDate: "2020-10-11"
      version: v11
    secret:
      name: logging-loki-s3
      type: aws
  storageClassName: standard
  tenants:
    mode: openshift-logging
----
<1> Sets retention policy by tenant. Valid tenant types are `application`, `audit`, and `infrastructure`.
<2> Contains the link:https://grafana.com/docs/loki/latest/logql/query_examples/#query-examples[LogQL query] used to define the log stream.

. Apply the `LokiStack` CR:
+
[source,terminal]
----
$ oc apply -f <filename>.yaml
----

[NOTE]
====
This is not for managing the retention for stored logs. Global retention periods for stored logs to a supported maximum of 30 days is configured with your object storage.
====

:leveloffset!:

:leveloffset: +1

// Module is included in the following assemblies:
//cluster-logging-loki.adoc
:_mod-docs-content-type: PROCEDURE
[id="cluster-logging-forwarding-lokistack_{context}"]
= Forwarding logs to LokiStack

To configure log forwarding to the LokiStack gateway, you must create a `ClusterLogging` custom resource (CR).

.Prerequisites

* The {logging-title-uc} version 5.5 or newer is installed on your cluster.
* The Loki Operator is installed on your cluster.

.Procedure

* Create a `ClusterLogging` custom resource (CR):
+
[source,yaml]
----
apiVersion: logging.openshift.io/v1
kind: ClusterLogging
metadata:
  name: instance
  namespace: openshift-logging
spec:
  managementState: Managed
  logStore:
    type: lokistack
    lokistack:
      name: logging-loki
  collection:
    type: vector
----

:leveloffset!:

:leveloffset: +2

// Module is included in the following assemblies:
// * logging/cluster-logging-loki.adoc
// * logging/log_collection_forwarding/log-forwarding.adoc

:_mod-docs-content-type: PROCEDURE
[id="loki-rate-limit-errors_{context}"]
= Troubleshooting Loki rate limit errors

If the Log Forwarder API forwards a large block of messages that exceeds the rate limit to Loki, Loki generates rate limit (`429`) errors.

These errors can occur during normal operation. For example, when adding the {logging} to a cluster that already has some logs, rate limit errors might occur while the {logging} tries to ingest all of the existing log entries. In this case, if the rate of addition of new logs is less than the total rate limit, the historical data is eventually ingested, and the rate limit errors are resolved without requiring user intervention.

In cases where the rate limit errors continue to occur, you can fix the issue by modifying the `LokiStack` custom resource (CR).

[IMPORTANT]
====
The `LokiStack` CR is not available on Grafana-hosted Loki. This topic does not apply to Grafana-hosted Loki servers.
====

.Conditions

* The Log Forwarder API is configured to forward logs to Loki.

* Your system sends a block of messages that is larger than 2 MB to Loki. For example:
+
[source,text]
----
"values":[["1630410392689800468","{\"kind\":\"Event\",\"apiVersion\":\
.......
......
......
......
\"received_at\":\"2021-08-31T11:46:32.800278+00:00\",\"version\":\"1.7.4 1.6.0\"}},\"@timestamp\":\"2021-08-31T11:46:32.799692+00:00\",\"viaq_index_name\":\"audit-write\",\"viaq_msg_id\":\"MzFjYjJkZjItNjY0MC00YWU4LWIwMTEtNGNmM2E5ZmViMGU4\",\"log_type\":\"audit\"}"]]}]}
----

* After you enter `oc logs -n openshift-logging -l component=collector`, the collector logs in your cluster show a line containing one of the following error messages:
+
[source,text]
----
429 Too Many Requests Ingestion rate limit exceeded
----
+
.Example Vector error message
[source,text]
----
2023-08-25T16:08:49.301780Z  WARN sink{component_kind="sink" component_id=default_loki_infra component_type=loki component_name=default_loki_infra}: vector::sinks::util::retries: Retrying after error. error=Server responded with an error: 429 Too Many Requests internal_log_rate_limit=true
----
+
.Example Fluentd error message
[source,text]
----
2023-08-30 14:52:15 +0000 [warn]: [default_loki_infra] failed to flush the buffer. retry_times=2 next_retry_time=2023-08-30 14:52:19 +0000 chunk="604251225bf5378ed1567231a1c03b8b" error_class=Fluent::Plugin::LokiOutput::LogPostError error="429 Too Many Requests Ingestion rate limit exceeded for user infrastructure (limit: 4194304 bytes/sec) while attempting to ingest '4082' lines totaling '7820025' bytes, reduce log volume or contact your Loki administrator to see if the limit can be increased\n"
----
+
The error is also visible on the receiving end. For example, in the LokiStack ingester pod:
+
.Example Loki ingester error message
[source,text]
----
level=warn ts=2023-08-30T14:57:34.155592243Z caller=grpc_logging.go:43 duration=1.434942ms method=/logproto.Pusher/Push err="rpc error: code = Code(429) desc = entry with timestamp 2023-08-30 14:57:32.012778399 +0000 UTC ignored, reason: 'Per stream rate limit exceeded (limit: 3MB/sec) while attempting to ingest for stream
----

.Procedure

* Update the `ingestionBurstSize` and `ingestionRate` fields in the `LokiStack` CR:
+
[source,yaml]
----
apiVersion: loki.grafana.com/v1
kind: LokiStack
metadata:
  name: logging-loki
  namespace: openshift-logging
spec:
  limits:
    global:
      ingestion:
        ingestionBurstSize: 16 # <1>
        ingestionRate: 8 # <2>
# ...
----
<1> The `ingestionBurstSize` field defines the maximum local rate-limited sample size per distributor replica in MB. This value is a hard limit. Set this value to at least the maximum logs size expected in a single push request. Single requests that are larger than the `ingestionBurstSize` value are not permitted.
<2> The `ingestionRate` field is a soft limit on the maximum amount of ingested samples per second in MB. Rate limit errors occur if the rate of logs exceeds the limit, but the collector retries sending the logs. As long as the total average is lower than the limit, the system recovers and errors are resolved without user intervention.

:leveloffset!:

[role="_additional-resources"]
[id="additional-resources_cluster-logging-loki"]
== Additional Resources
* link:https://grafana.com/docs/loki/latest/get-started/components/[Loki components documentation]
* link:https://grafana.com/docs/loki/latest/logql/[Loki Query Language (LogQL) documentation]
* link:https://loki-operator.dev/docs/howto_connect_grafana.md/[Grafana Dashboard documentation]
* link:https://loki-operator.dev/docs/object_storage.md/[Loki Object Storage documentation]
* link:https://loki-operator.dev/docs/api.md/#loki-grafana-com-v1-IngestionLimitSpec[Loki Operator `IngestionLimitSpec` documentation]
* link:https://grafana.com/docs/loki/latest/operations/storage/schema/#changing-the-schema[Loki Storage Schema documentation]

//# includes=_attributes/common-attributes,modules/loki-deployment-sizing,modules/snippets/technology-preview,modules/logging-creating-new-group-cluster-admin-user-role,modules/snippets/logging-clusteradmin-access-logs-snip,modules/logging-loki-gui-install,modules/snippets/logging-stable-updates-snip,modules/snippets/logging-approval-strategy-snip,modules/logging-loki-cli-install,modules/logging-loki-restart-hardening,modules/logging-loki-reliability-hardening,modules/snippets/about-pod-affinity,modules/logging-loki-zone-aware-rep,modules/logging-loki-zone-fail-recovery,modules/logging-loki-log-access,modules/logging-loki-retention,modules/cluster-logging-forwarding-lokistack,modules/loki-rate-limit-errors

:_mod-docs-content-type: ASSEMBLY
:context: cluster-logging-deploying
[id="cluster-logging-deploying"]
= Installing Logging
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// common attributes
:product-short-name: OpenShift Dedicated
:toc:
:toc-title:
:experimental:
:imagesdir: images
:OCP: OpenShift Container Platform
:ocp-version: 4.14
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:AWS: Amazon Web Services (AWS)
:GCP: Google Cloud Platform (GCP)
:product-registry: OpenShift image registry
:kebab: image:kebab.png[title="Options menu"]
:rhq-short: Red Hat Quay
:SMProductName: Red Hat OpenShift Service Mesh
:pipelines-title: Red Hat OpenShift Pipelines
:logging-sd: Red Hat OpenShift Logging
:ServerlessProductName: OpenShift Serverless
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:rhoda: Red Hat OpenShift Database Access
:rhoda-short: RHODA
:rhods: Red Hat OpenShift Data Science
:osd: OpenShift Dedicated
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:hcp: hosted control planes
:hcp-title: ROSA with HCP
:hcp-title-first: {product-title} (ROSA) with {hcp} (HCP)
//ROSA CLI variables
:word: Testing this variable let's go www.google.com

toc::[]

You can install the {logging-title} by deploying the Red Hat OpenShift Logging Operator. The {logging} Operator creates and manages the components of the logging stack.

[IMPORTANT]
====
For new installations, Vector and LokiStack are recommended. Documentation for logging is in the process of being updated to reflect these underlying component changes.
====


// Text snippet included in the following assemblies:
//
// * logging/cluster-logging-deploying.adoc
//
// Text snippet included in the following modules:
//
//
:_mod-docs-content-type: SNIPPET

[NOTE]
====
As of logging version 5.6 Fluentd is deprecated and is planned to be removed in a future release. Red Hat will provide bug fixes and support for this feature during the current release lifecycle, but this feature will no longer receive enhancements and will be removed. As an alternative to Fluentd, you can use Vector instead.
====

:leveloffset: +1

// Module included in the following assemblies:
//
// * logging/cluster-logging-deploying.adoc

:_mod-docs-content-type: PROCEDURE
[id="cluster-logging-deploy-console_{context}"]
= Installing the {logging-title} using the web console

[NOTE]
====
If you do not want to use the default Elasticsearch log store, you can remove the internal Elasticsearch `logStore` and Kibana `visualization` components from the `ClusterLogging` custom resource (CR). Removing these components is optional but saves resources.
====

// Text snippet included in the following assemblies:
//
// logging/cluster-logging-support.adoc
// logging/logging_release_notes/logging-5-7-release-notes.adoc
// logging/logging_release_notes/logging-5-8-release-notes.adoc
//
// Text snippet included in the following modules:
//
// logging-deploy-RHOL-console.adoc

:_mod-docs-content-type: SNIPPET

[NOTE]
====
Logging is provided as an installable component, with a distinct release cycle from the core {product-title}. The link:https://access.redhat.com/support/policy/updates/openshift_operators#platform-agnostic[Red Hat OpenShift Container Platform Life Cycle Policy] outlines release compatibility.
====

.Procedure

. In the {product-title} web console, click *Operators* -> *OperatorHub*.
. Choose  *Red Hat OpenShift Logging* from the list of available Operators, and click *Install*.
. Ensure that *A specific namespace on the cluster* is selected under *Installation mode*.
. Ensure that *Operator recommended namespace* is *openshift-logging* under *Installed Namespace*.
. Select *Enable operator recommended cluster monitoring on this namespace*.
+
This option sets the `openshift.io/cluster-monitoring: "true"` label in the Namespace object. You must select this option to ensure that cluster monitoring scrapes the `openshift-logging` namespace.

. Select *stable-5.x* as the *Update channel*.
+
--
// Text snippet included in the following assemblies:
//
// logging/logging_release_notes/logging-5-7-release-notes.adoc
// logging/logging_release_notes/logging-5-8-release-notes.adoc
//
// Text snippet included in the following modules:
//
//
:_mod-docs-content-type: SNIPPET

[NOTE]
====
The `stable` channel only provides updates to the most recent release of logging. To continue receiving updates for prior releases, you must change your subscription channel to `stable-X` where `X` is the version of logging you have installed.
====
--

. Select an *Update approval*.
** The *Automatic* strategy allows Operator Lifecycle Manager (OLM) to automatically update the Operator when a new version is available.
** The *Manual* strategy requires a user with appropriate credentials to approve the Operator update.

. Select *Enable* or *Disable* for the Console plugin.
. Click *Install*.

.Verification

. Verify that the *Red Hat OpenShift Logging Operator* is installed by switching to the *Operators* → *Installed Operators* page.

.. Ensure that *Red Hat OpenShift Logging* is listed in the *openshift-logging* project with a *Status* of *Succeeded*.

.. Verify that the Red Hat OpenShift Logging Operator installed by switching to the *Operators* → *Installed Operators* page.

.. Ensure that *Red Hat OpenShift Logging* is listed in the *openshift-logging* project with a *Status* of *Succeeded*.
+
If the Operator does not appear as installed, to troubleshoot further:
+
* Switch to the *Operators* → *Installed Operators* page and inspect the *Status* column for any errors or failures.
* Switch to the *Workloads* → *Pods* page and check the logs in any pods in the `openshift-logging` project that are reporting issues.

. Create a *ClusterLogging* instance.
+
[NOTE]
====
The form view of the web console does not include all available options. The *YAML view* is recommended for completing your setup.
====
+
.. In the *collection* section, select a Collector Implementation.
+
--
// Text snippet included in the following assemblies:
//
// * logging/cluster-logging-deploying.adoc
//
// Text snippet included in the following modules:
//
//
:_mod-docs-content-type: SNIPPET

[NOTE]
====
As of logging version 5.6 Fluentd is deprecated and is planned to be removed in a future release. Red Hat will provide bug fixes and support for this feature during the current release lifecycle, but this feature will no longer receive enhancements and will be removed. As an alternative to Fluentd, you can use Vector instead.
====
--
.. In the *logStore* section, select a type.
+
--
// Text snippet included in the following assemblies:
//
// * logging/cluster-logging-deploying.adoc
//
// Text snippet included in the following modules:
//
//
:_mod-docs-content-type: SNIPPET

[NOTE]
====
As of logging version 5.4.3 the OpenShift Elasticsearch Operator is deprecated and is planned to be removed in a future release. Red Hat will provide bug fixes and support for this feature during the current release lifecycle, but this feature will no longer receive enhancements and will be removed. As an alternative to using the OpenShift Elasticsearch Operator to manage the default log storage, you can use the Loki Operator.
====
--

.. Click *Create*.

. Create an OpenShift Logging instance:

.. Switch to the *Administration* -> *Custom Resource Definitions* page.

.. On the *Custom Resource Definitions* page, click *ClusterLogging*.

.. On the *Custom Resource Definition details* page, select *View Instances* from the *Actions* menu.

.. On the *ClusterLoggings* page, click *Create ClusterLogging*.
+
You might have to refresh the page to load the data.

.. In the YAML field, replace the code with the following:
+
[NOTE]
====
This default OpenShift Logging configuration should support a wide array of environments. Review the topics on tuning and
configuring {logging} components for information on modifications you can make to your OpenShift Logging cluster.
====
+
[source,yaml]
----
apiVersion: logging.openshift.io/v1
kind: ClusterLogging
metadata:
  name: instance <1>
  namespace: openshift-logging
spec:
  managementState: Managed <2>
  logStore:
    type: elasticsearch <3>
    retentionPolicy: <4>
      application:
        maxAge: 1d
      infra:
        maxAge: 7d
      audit:
        maxAge: 7d
    elasticsearch:
      nodeCount: 3 <5>
      storage:
        storageClassName: <storage_class_name> <6>
        size: 200G
      resources: <7>
          limits:
            memory: 16Gi
          requests:
            memory: 16Gi
      proxy: <8>
        resources:
          limits:
            memory: 256Mi
          requests:
            memory: 256Mi
      redundancyPolicy: SingleRedundancy
  visualization:
    type: kibana <9>
    kibana:
      replicas: 1
  collection:
    logs:
      type: fluentd <10>
      fluentd: {}
----
<1> The name must be `instance`.
<2> The OpenShift Logging management state. In some cases, if you change the OpenShift Logging defaults, you must set this to `Unmanaged`.
However, an unmanaged deployment does not receive updates until OpenShift Logging is placed back into a managed state.
<3> Settings for configuring Elasticsearch. Using the CR, you can configure shard replication policy and persistent storage.
<4> Specify the length of time that Elasticsearch should retain each log source. Enter an integer and a time designation: weeks(w), hours(h/H), minutes(m) and seconds(s). For example, `7d` for seven days. Logs older than the `maxAge` are deleted. You must specify a retention policy for each log source or the Elasticsearch indices will not be created for that source.
<5> Specify the number of Elasticsearch nodes. See the note that follows this list.
<6> Enter the name of an existing storage class for Elasticsearch storage. For best performance, specify a storage class that allocates block storage. If you do not specify a storage class, OpenShift Logging uses ephemeral storage.
<7> Specify the CPU and memory requests for Elasticsearch as needed. If you leave these values blank, the OpenShift Elasticsearch Operator sets default values that should be sufficient for most deployments. The default values are `16Gi` for the memory request and `1` for the CPU request.
<8> Specify the CPU and memory requests for the Elasticsearch proxy as needed. If you leave these values blank, the OpenShift Elasticsearch Operator sets default values that should be sufficient for most deployments. The default values are `256Mi` for the memory request and `100m` for the CPU request.
<9> Settings for configuring Kibana. Using the CR, you can scale Kibana for redundancy and configure the CPU and memory for your Kibana nodes. For more information, see *Configuring the log visualizer*.
<10> Settings for configuring Fluentd. Using the CR, you can configure Fluentd CPU and memory limits. For more information, see *Configuring Fluentd*.
+
[NOTE]
+
====
The maximum number of Elasticsearch control plane nodes is three. If you specify a `nodeCount` greater than `3`, {product-title} creates three Elasticsearch nodes that are Master-eligible nodes, with the master, client, and data roles. The additional Elasticsearch nodes are created as Data-only nodes, using client and data roles. Control plane nodes perform cluster-wide actions such as creating or deleting an index, shard allocation, and tracking nodes. Data nodes hold the shards and perform data-related operations such as CRUD, search, and aggregations. Data-related operations are I/O-, memory-, and CPU-intensive. It is important to monitor these resources and to add more Data nodes if the current nodes are overloaded.

For example, if `nodeCount=4`, the following nodes are created:

[source,terminal]
----
$ oc get deployment
----

.Example output
[source,terminal]
----
cluster-logging-operator       1/1     1            1           18h
elasticsearch-cd-x6kdekli-1    0/1     1            0           6m54s
elasticsearch-cdm-x6kdekli-1   1/1     1            1           18h
elasticsearch-cdm-x6kdekli-2   0/1     1            0           6m49s
elasticsearch-cdm-x6kdekli-3   0/1     1            0           6m44s
----

The number of primary shards for the index templates is equal to the number of Elasticsearch data nodes.
====

.. Click *Create*. This creates the {logging} components, the `Elasticsearch` custom resource and components, and the Kibana interface.

. Verify the install:

.. Switch to the *Workloads* -> *Pods* page.

.. Select the *openshift-logging* project.
+
Confirm that pods exist for the Operator and the Elasticsearch, collector, and Kibana components:
+
* cluster-logging-operator-595f9bf9c4-txrp4
* collector-29bw8
* collector-4kvnl
* collector-7rr7w
* collector-9m2xp
* collector-xt45j
* elasticsearch-cdm-g559ha9u-1-659fd594bf-pcm2f
* elasticsearch-cdm-g559ha9u-2-66455f68db-v46n6
* elasticsearch-cdm-g559ha9u-3-85696bcf55-g7tf8
* elasticsearch-im-app-27934020-9ltxl
* elasticsearch-im-audit-27934020-86cdt
* elasticsearch-im-infra-27934020-6lrgm
* kibana-5c6b7cd56-66c9l

.Troubleshooting
* If Alertmanager logs alerts such as `Prometheus could not scrape fluentd for more than 10m`, make sure that `openshift.io/cluster-monitoring` is set to `"true"` for the OpenShift Elasticsearch Operator and OpenShift Logging Operator. See the Red Hat KnowledgeBase for more information: link:https://access.redhat.com/solutions/5692801[Prometheus could not scrape fluentd for more than 10m alert in Alertmanager]

:leveloffset!:

[id="cluster-logging-deploying-es-operator"]
== Installing the Elasticsearch Operator

// Text snippet included in the following assemblies:
//
// * logging/cluster-logging-deploying.adoc
//
// Text snippet included in the following modules:
//
//
:_mod-docs-content-type: SNIPPET

[NOTE]
====
As of logging version 5.4.3 the OpenShift Elasticsearch Operator is deprecated and is planned to be removed in a future release. Red Hat will provide bug fixes and support for this feature during the current release lifecycle, but this feature will no longer receive enhancements and will be removed. As an alternative to using the OpenShift Elasticsearch Operator to manage the default log storage, you can use the Loki Operator.
====
:leveloffset: +2

// Module included in the following assemblies:
//
// * logging/cluster-logging-deploying.adoc

:_mod-docs-content-type: CONCEPT
[id="logging-es-storage-considerations_{context}"]
= Storage considerations for Elasticsearch

A persistent volume is required for each Elasticsearch deployment configuration. On {product-title} this is achieved using persistent volume claims (PVCs).

[NOTE]
====
If you use a local volume for persistent storage, do not use a raw block volume, which is described with `volumeMode: block` in the `LocalVolume` object. Elasticsearch cannot use raw block volumes.
====

The OpenShift Elasticsearch Operator names the PVCs using the Elasticsearch resource name.

Fluentd ships any logs from *systemd journal* and **/var/log/containers/*.log** to Elasticsearch.

Elasticsearch requires sufficient memory to perform large merge operations. If it does not have enough memory, it becomes unresponsive. To avoid this problem, evaluate how much application log data you need, and allocate approximately double that amount of free storage capacity.

By default, when storage capacity is 85% full, Elasticsearch stops allocating new data to the node. At 90%, Elasticsearch attempts to relocate existing shards from that node to other nodes if possible. But if no nodes have a free capacity below 85%, Elasticsearch effectively rejects creating new indices and becomes RED.

[NOTE]
====
These low and high watermark values are Elasticsearch defaults in the current release. You can modify these default values. Although the alerts use the same default values, you cannot change these values in the alerts.
====

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * logging/cluster-logging-deploying.adoc

:_mod-docs-content-type: PROCEDURE
[id="logging-install-es-operator_{context}"]
= Installing the OpenShift Elasticsearch Operator by using the web console

The OpenShift Elasticsearch Operator creates and manages the Elasticsearch cluster used by OpenShift Logging.

.Prerequisites

* Elasticsearch is a memory-intensive application. Each Elasticsearch node needs at least 16GB of memory for both memory requests and limits, unless you specify otherwise in the `ClusterLogging` custom resource.
+
The initial set of {product-title} nodes might not be large enough to support the Elasticsearch cluster. You must add additional nodes to the {product-title} cluster to run with the recommended or higher memory, up to a maximum of 64GB for each Elasticsearch node.
+
Elasticsearch nodes can operate with a lower memory setting, though this is not recommended for production environments.

* Ensure that you have the necessary persistent storage for Elasticsearch. Note that each Elasticsearch node
requires its own storage volume.
+
[NOTE]
====
If you use a local volume for persistent storage, do not use a raw block volume, which is described with `volumeMode: block` in the `LocalVolume` object. Elasticsearch cannot use raw block volumes.
====

.Procedure

. In the {product-title} web console, click *Operators* -> *OperatorHub*.
. Click *OpenShift Elasticsearch Operator* from the list of available Operators, and click *Install*.
. Ensure that the *All namespaces on the cluster* is selected under *Installation mode*.
. Ensure that *openshift-operators-redhat* is selected under *Installed Namespace*.
+
You must specify the `openshift-operators-redhat` namespace. The `openshift-operators` namespace might contain Community Operators, which are untrusted and could publish a metric with the same name as {product-title} metric, which would cause conflicts.

. Select *Enable operator recommended cluster monitoring on this namespace*.
+
This option sets the `openshift.io/cluster-monitoring: "true"` label in the `Namespace` object. You must select this option to ensure that cluster monitoring scrapes the `openshift-operators-redhat` namespace.

. Select *stable-5.x* as the *Update channel*.
. Select an *Update approval* strategy:
+
* The *Automatic* strategy allows Operator Lifecycle Manager (OLM) to automatically update the Operator when a new version is available.
+
* The *Manual* strategy requires a user with appropriate credentials to approve the Operator update.

. Click *Install*.

.Verification

. Verify that the OpenShift Elasticsearch Operator installed by switching to the *Operators* → *Installed Operators* page.
. Ensure that *OpenShift Elasticsearch Operator* is listed in all projects with a *Status* of *Succeeded*.

:leveloffset!:

[role="_additional-resources"]
.Additional resources
* xref:../operators/admin/olm-adding-operators-to-cluster.adoc#olm-installing-operators-from-operatorhub_olm-adding-operators-to-a-cluster[Installing Operators from the OperatorHub]
* xref:../logging/config/cluster-logging-log-store.adoc#cluster-logging-removing-unused-components-if-no-elasticsearch_cluster-logging-log-store[Removing unused components if you do not use the default Elasticsearch log store]

== Postinstallation tasks

If your network plugin enforces network isolation, xref:#cluster-logging-deploy-multitenant_cluster-logging-deploying[allow network traffic between the projects that contain the {logging} Operators].

:leveloffset: +1

// Module included in the following assemblies:
//
// * logging/cluster-logging-deploying.adoc

:_mod-docs-content-type: PROCEDURE
[id="cluster-logging-deploy-cli_{context}"]
= Installing the {logging-title} using the CLI

You can use the {product-title} CLI to install the OpenShift Elasticsearch and Red Hat OpenShift Logging Operators.

.Prerequisites

* Ensure that you have the necessary persistent storage for Elasticsearch. Note that each Elasticsearch node requires its own storage volume.
+
[NOTE]
====
If you use a local volume for persistent storage, do not use a raw block volume, which is described with `volumeMode: block` in the `LocalVolume` object. Elasticsearch cannot use raw block volumes.
====
+
Elasticsearch is a memory-intensive application. By default, {product-title} installs three Elasticsearch nodes with memory requests and limits of 16 GB. This initial set of three {product-title} nodes might not have enough memory to run Elasticsearch within your cluster. If you experience memory issues that are related to Elasticsearch, add more Elasticsearch nodes to your cluster rather than increasing the memory on existing nodes.


.Procedure

To install the OpenShift Elasticsearch Operator and Red Hat OpenShift Logging Operator using the CLI:

. Create a namespace for the OpenShift Elasticsearch Operator.

.. Create a namespace object YAML file (for example, `eo-namespace.yaml`) for the OpenShift Elasticsearch Operator:
+
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: openshift-operators-redhat <1>
  annotations:
    openshift.io/node-selector: ""
  labels:
    openshift.io/cluster-monitoring: "true" <2>
----
<1> You must specify the `openshift-operators-redhat` namespace. To prevent possible conflicts with metrics, you should configure the Prometheus Cluster Monitoring stack to scrape metrics from the `openshift-operators-redhat` namespace and not the `openshift-operators` namespace. The `openshift-operators` namespace might contain community Operators, which are untrusted and could publish a metric with the same name as
metric, which would cause conflicts.
<2> String. You must specify this label as shown to ensure that cluster monitoring scrapes the `openshift-operators-redhat` namespace.

.. Create the namespace:
+
[source,terminal]
----
$ oc create -f <file-name>.yaml
----
+
For example:
+
[source,terminal]
----
$ oc create -f eo-namespace.yaml
----

. Create a namespace for the Red Hat OpenShift Logging Operator:

.. Create a namespace object YAML file (for example, `olo-namespace.yaml`) for the Red Hat OpenShift Logging Operator:
+
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: openshift-logging
  annotations:
    openshift.io/node-selector: ""
  labels:
    openshift.io/cluster-monitoring: "true"
----

.. Create the namespace:
+
[source,terminal]
----
$ oc create -f <file-name>.yaml
----
+
For example:
+
[source,terminal]
----
$ oc create -f olo-namespace.yaml
----

. Install the OpenShift Elasticsearch Operator by creating the following objects:

.. Create an Operator Group object YAML file (for example, `eo-og.yaml`) for the OpenShift Elasticsearch Operator:
+
[source,yaml]
----
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: openshift-operators-redhat
  namespace: openshift-operators-redhat <1>
spec: {}
----
<1> You must specify the `openshift-operators-redhat` namespace.

.. Create an Operator Group object:
+
[source,terminal]
----
$ oc create -f <file-name>.yaml
----
+
For example:
+
[source,terminal]
----
$ oc create -f eo-og.yaml
----

.. Create a Subscription object YAML file (for example, `eo-sub.yaml`) to
subscribe a namespace to the OpenShift Elasticsearch Operator.
+
.Example Subscription
[source,yaml]
----
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: "elasticsearch-operator"
  namespace: "openshift-operators-redhat" <1>
spec:
  channel: "stable-5.5" <2>
  installPlanApproval: "Automatic" <3>
  source: "redhat-operators" <4>
  sourceNamespace: "openshift-marketplace"
  name: "elasticsearch-operator"
----
<1> You must specify the `openshift-operators-redhat` namespace.
<2> Specify `stable`, or `stable-5.<x>` as the channel. See the following note.
<3> `Automatic` allows the Operator Lifecycle Manager (OLM) to automatically update the Operator when a new version is available. `Manual` requires a user with appropriate credentials to approve the Operator update.
<4> Specify `redhat-operators`. If your {product-title} cluster is installed on a restricted network, also known as a disconnected cluster,
specify the name of the CatalogSource object created when you configured the Operator Lifecycle Manager (OLM).
+
[NOTE]
====
Specifying `stable` installs the current version of the latest stable release. Using `stable` with `installPlanApproval: "Automatic"`, will automatically upgrade your operators to the latest stable major and minor release.

Specifying `stable-5.<x>` installs the current minor version of a specific major release. Using `stable-5.<x>` with `installPlanApproval: "Automatic"`, will automatically upgrade your operators to the latest stable minor release within the major release you specify with `x`.
====


.. Create the Subscription object:
+
[source,terminal]
----
$ oc create -f <file-name>.yaml
----
+
For example:
+
[source,terminal]
----
$ oc create -f eo-sub.yaml
----
+
The OpenShift Elasticsearch Operator is installed to the `openshift-operators-redhat` namespace and copied to each project in the cluster.

.. Verify the Operator installation:
+
[source,terminal]
----
$ oc get csv --all-namespaces
----
+
.Example output
[source,terminal]
----
NAMESPACE                                               NAME                                            DISPLAY                  VERSION               REPLACES   PHASE
default                                                 elasticsearch-operator.5.1.0-202007012112.p0    OpenShift Elasticsearch Operator   5.5.0-202007012112.p0               Succeeded
kube-node-lease                                         elasticsearch-operator.5.5.0-202007012112.p0    OpenShift Elasticsearch Operator   5.5.0-202007012112.p0               Succeeded
kube-public                                             elasticsearch-operator.5.5.0-202007012112.p0    OpenShift Elasticsearch Operator   5.5.0-202007012112.p0               Succeeded
kube-system                                             elasticsearch-operator.5.5.0-202007012112.p0    OpenShift Elasticsearch Operator   5.5.0-202007012112.p0               Succeeded
openshift-apiserver-operator                            elasticsearch-operator.5.5.0-202007012112.p0    OpenShift Elasticsearch Operator   5.5.0-202007012112.p0               Succeeded
openshift-apiserver                                     elasticsearch-operator.5.5.0-202007012112.p0    OpenShift Elasticsearch Operator   5.5.0-202007012112.p0               Succeeded
openshift-authentication-operator                       elasticsearch-operator.5.5.0-202007012112.p0    OpenShift Elasticsearch Operator   5.5.0-202007012112.p0               Succeeded
openshift-authentication                                elasticsearch-operator.5.5.0-202007012112.p0    OpenShift Elasticsearch Operator   5.5.0-202007012112.p0               Succeeded
...
----
+
There should be an OpenShift Elasticsearch Operator in each namespace. The version number might be different than shown.

. Install the Red Hat OpenShift Logging Operator by creating the following objects:

.. Create an Operator Group object YAML file (for example, `olo-og.yaml`) for the Red Hat OpenShift Logging Operator:
+
[source,yaml]
----
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: cluster-logging
  namespace: openshift-logging <1>
spec:
  targetNamespaces:
  - openshift-logging <1>
----
<1> You must specify the `openshift-logging` namespace.

.. Create the OperatorGroup object:
+
[source,terminal]
----
$ oc create -f <file-name>.yaml
----
+
For example:
+
[source,terminal]
----
$ oc create -f olo-og.yaml
----

.. Create a Subscription object YAML file (for example, `olo-sub.yaml`) to
subscribe a namespace to the Red Hat OpenShift Logging Operator.
+
[source,yaml]
----
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: cluster-logging
  namespace: openshift-logging <1>
spec:
  channel: "stable" <2>
  name: cluster-logging
  source: redhat-operators <3>
  sourceNamespace: openshift-marketplace
----
<1> You must specify the `openshift-logging` namespace.
<2> Specify `stable`, or `stable-5.<x>` as the channel.
<3> Specify `redhat-operators`. If your {product-title} cluster is installed on a restricted network, also known as a disconnected cluster, specify the name of the CatalogSource object you created when you configured the Operator Lifecycle Manager (OLM).
+
[source,terminal]
----
$ oc create -f <file-name>.yaml
----
+
For example:
+
[source,terminal]
----
$ oc create -f olo-sub.yaml
----
+
The Red Hat OpenShift Logging Operator is installed to the `openshift-logging` namespace.

.. Verify the Operator installation.
+
There should be a Red Hat OpenShift Logging Operator in the `openshift-logging` namespace. The Version number might be different than shown.
+
[source,terminal]
----
$ oc get csv -n openshift-logging
----
+
.Example output
[source,terminal]
----
NAMESPACE                                               NAME                                         DISPLAY                  VERSION               REPLACES   PHASE
...
openshift-logging                                       clusterlogging.5.1.0-202007012112.p0         OpenShift Logging          5.1.0-202007012112.p0              Succeeded
...
----

. Create an OpenShift Logging instance:

.. Create an instance object YAML file (for example, `olo-instance.yaml`) for the Red Hat OpenShift Logging Operator:
+
[NOTE]
====
This default OpenShift Logging configuration should support a wide array of environments. Review the topics on tuning and
configuring {logging} components for information about modifications you can make to your OpenShift Logging cluster.
====
+
[source,yaml]
----
apiVersion: "logging.openshift.io/v1"
kind: "ClusterLogging"
metadata:
  name: "instance" <1>
  namespace: "openshift-logging"
spec:
  managementState: "Managed"  <2>
  logStore:
    type: "elasticsearch"  <3>
    retentionPolicy: <4>
      application:
        maxAge: 1d
      infra:
        maxAge: 7d
      audit:
        maxAge: 7d
    elasticsearch:
      nodeCount: 3 <5>
      storage:
        storageClassName: "<storage-class-name>" <6>
        size: 200G
      resources: <7>
        limits:
          memory: "16Gi"
        requests:
          memory: "16Gi"
      proxy: <8>
        resources:
          limits:
            memory: 256Mi
          requests:
             memory: 256Mi
      redundancyPolicy: "SingleRedundancy"
  visualization:
    type: "kibana"  <9>
    kibana:
      replicas: 1
  collection:
    logs:
      type: "fluentd"  <10>
      fluentd: {}
----
<1> The name must be `instance`.
<2> The OpenShift Logging management state. In some cases, if you change the OpenShift Logging defaults, you must set this to `Unmanaged`.
However, an unmanaged deployment does not receive updates until OpenShift Logging is placed back into a managed state. Placing a deployment back into a managed state might revert any modifications you made.
<3> Settings for configuring Elasticsearch. Using the custom resource (CR), you can configure shard replication policy and persistent storage.
<4> Specify the length of time that Elasticsearch should retain each log source. Enter an integer and a time designation: weeks(w), hours(h/H), minutes(m) and seconds(s). For example, `7d` for seven days. Logs older than the `maxAge` are deleted. You must specify a retention policy for each log source or the Elasticsearch indices will not be created for that source.
<5> Specify the number of Elasticsearch nodes. See the note that follows this list.
<6> Enter the name of an existing storage class for Elasticsearch storage. For best performance, specify a storage class that allocates block storage. If you do not specify a storage class, {product-title} deploys OpenShift Logging with ephemeral storage only.
<7> Specify the CPU and memory requests for Elasticsearch as needed. If you leave these values blank, the OpenShift Elasticsearch Operator sets default values that are sufficient for most deployments. The default values are `16Gi` for the memory request and `1` for the CPU request.
<8> Specify the CPU and memory requests for the Elasticsearch proxy as needed. If you leave these values blank, the OpenShift Elasticsearch Operator sets default values that should be sufficient for most deployments. The default values are  `256Mi` for the memory request and `100m` for the CPU request.
<9> Settings for configuring Kibana. Using the CR, you can scale Kibana for redundancy and configure the CPU and memory for your Kibana pods. For more information, see *Configuring the log visualizer*.
<10> Settings for configuring Fluentd. Using the CR, you can configure Fluentd CPU and memory limits. For more information, see *Configuring Fluentd*.
+
[NOTE]
+
====
The maximum number of Elasticsearch control plane nodes is three. If you specify a `nodeCount` greater than `3`, {product-title} creates three Elasticsearch nodes that are Master-eligible nodes, with the master, client, and data roles. The additional Elasticsearch nodes are created as Data-only nodes, using client and data roles. Control plane nodes perform cluster-wide actions such as creating or deleting an index, shard allocation, and tracking nodes. Data nodes hold the shards and perform data-related operations such as CRUD, search, and aggregations. Data-related operations are I/O-, memory-, and CPU-intensive. It is important to monitor these resources and to add more Data nodes if the current nodes are overloaded.

For example, if `nodeCount=4`, the following nodes are created:

[source,terminal]
----
$ oc get deployment
----

.Example output
[source,terminal]
----
cluster-logging-operator       1/1     1            1           18h
elasticsearch-cd-x6kdekli-1    1/1     1            0           6m54s
elasticsearch-cdm-x6kdekli-1   1/1     1            1           18h
elasticsearch-cdm-x6kdekli-2   1/1     1            0           6m49s
elasticsearch-cdm-x6kdekli-3   1/1     1            0           6m44s
----

The number of primary shards for the index templates is equal to the number of Elasticsearch data nodes.
====

.. Create the instance:
+
[source,terminal]
----
$ oc create -f <file-name>.yaml
----
+
For example:
+
[source,terminal]
----
$ oc create -f olo-instance.yaml
----
+
This creates the {logging} components, the `Elasticsearch` custom resource and components, and the Kibana interface.

. Verify the installation by listing the pods in the *openshift-logging* project.
+
You should see several pods for components of the Logging subsystem, similar to the following list:
+
[source,terminal]
----
$ oc get pods -n openshift-logging
----
+
.Example output
[source,terminal]
----
NAME                                            READY   STATUS    RESTARTS   AGE
cluster-logging-operator-66f77ffccb-ppzbg       1/1     Running   0          7m
elasticsearch-cdm-ftuhduuw-1-ffc4b9566-q6bhp    2/2     Running   0          2m40s
elasticsearch-cdm-ftuhduuw-2-7b4994dbfc-rd2gc   2/2     Running   0          2m36s
elasticsearch-cdm-ftuhduuw-3-84b5ff7ff8-gqnm2   2/2     Running   0          2m4s
collector-587vb                                   1/1     Running   0          2m26s
collector-7mpb9                                   1/1     Running   0          2m30s
collector-flm6j                                   1/1     Running   0          2m33s
collector-gn4rn                                   1/1     Running   0          2m26s
collector-nlgb6                                   1/1     Running   0          2m30s
collector-snpkt                                   1/1     Running   0          2m28s
kibana-d6d5668c5-rppqm                          2/2     Running   0          2m39s
----

:leveloffset!:

== Postinstallation tasks

If your network plugin enforces network isolation, xref:#cluster-logging-deploy-multitenant_cluster-logging-deploying[allow network traffic between the projects that contain the {logging} Operators].

:leveloffset: +2

// Module included in the following assemblies:
//
// * logging/cluster-logging-deploying.adoc

:_mod-docs-content-type: PROCEDURE
[id="cluster-logging-deploy-multitenant_{context}"]
= Allowing traffic between projects when network isolation is enabled

Your cluster network plugin might enforce network isolation. If so, you must allow network traffic between the projects that contain the operators deployed by OpenShift Logging.

Network isolation blocks network traffic between pods or services that are in different projects. The {logging} installs the _OpenShift Elasticsearch Operator_ in the `openshift-operators-redhat` project and the _Red Hat OpenShift Logging Operator_ in the `openshift-logging` project. Therefore, you must allow traffic between these two projects.

{product-title} offers two supported choices for the network plugin, OpenShift SDN and OVN-Kubernetes. These two providers implement various network isolation policies.

OpenShift SDN has three modes:

network policy:: This is the default mode. If no policy is defined, it allows all traffic. However, if a user defines a policy, they typically start by denying all traffic and then adding exceptions. This process might break applications that are running in different projects. Therefore, explicitly configure the policy to allow traffic to egress from one logging-related project to the other.


subnet:: This mode allows all traffic. It does not enforce network isolation. No action is needed.

OVN-Kubernetes always uses a *network policy*. Therefore, as with OpenShift SDN, you must configure the policy to allow traffic to egress from one logging-related project to the other.

.Procedure

* If you are using OpenShift SDN in *multitenant* mode, join the two projects. For example:
+
[source,terminal]
----
$ oc adm pod-network join-projects --to=openshift-operators-redhat openshift-logging
----

* Otherwise, for OpenShift SDN in *network policy* mode and OVN-Kubernetes, perform the following actions:

.. Set a label on the `openshift-operators-redhat` namespace. For example:
+
[source,terminal]
----
$ oc label namespace openshift-operators-redhat project=openshift-operators-redhat
----

.. Create a network policy object in the `openshift-logging` namespace that allows ingress from the `openshift-operators-redhat`, `openshift-monitoring` and `openshift-ingress` projects to the openshift-logging project. For example:
+
[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-openshift-monitoring-ingress-operators-redhat
spec:
  ingress:
  - from:
    - podSelector: {}
  - from:
    - namespaceSelector:
        matchLabels:
          project: "openshift-operators-redhat"
  - from:
    - namespaceSelector:
        matchLabels:
          name: "openshift-monitoring"
  - from:
    - namespaceSelector:
        matchLabels:
          network.openshift.io/policy-group: ingress
  podSelector: {}
  policyTypes:
  - Ingress
----

:leveloffset!:

[role="_additional-resources"]
.Additional resources


//# includes=_attributes/common-attributes,_attributes/attributes-openshift-dedicated,snippets/logging-fluentd-dep-snip,modules/cluster-logging-deploy-console,modules/snippets/logging-compatibility-snip,modules/snippets/logging-stable-updates-snip,modules/snippets/logging-fluentd-dep-snip,modules/snippets/logging-elastic-dep-snip,snippets/logging-elastic-dep-snip,modules/logging-es-storage-considerations,modules/logging-install-es-operator,modules/cluster-logging-deploy-cli,modules/cluster-logging-deploy-multitenant

:_mod-docs-content-type: ASSEMBLY
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// common attributes
:product-short-name: OpenShift Dedicated
:toc:
:toc-title:
:experimental:
:imagesdir: images
:OCP: OpenShift Container Platform
:ocp-version: 4.14
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:AWS: Amazon Web Services (AWS)
:GCP: Google Cloud Platform (GCP)
:product-registry: OpenShift image registry
:kebab: image:kebab.png[title="Options menu"]
:rhq-short: Red Hat Quay
:SMProductName: Red Hat OpenShift Service Mesh
:pipelines-title: Red Hat OpenShift Pipelines
:logging-sd: Red Hat OpenShift Logging
:ServerlessProductName: OpenShift Serverless
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:rhoda: Red Hat OpenShift Database Access
:rhoda-short: RHODA
:rhods: Red Hat OpenShift Data Science
:osd: OpenShift Dedicated
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:hcp: hosted control planes
:hcp-title: ROSA with HCP
:hcp-title-first: {product-title} (ROSA) with {hcp} (HCP)
//ROSA CLI variables
:word: Testing this variable let's go www.google.com
[id="cluster-logging"]
= About Logging
:context: cluster-logging

toc::[]

As a cluster administrator, you can deploy {logging} on an {product-title} cluster, and use it to collect and aggregate node system audit logs, application container logs, and infrastructure logs. You can forward logs to your chosen log outputs, including on-cluster, Red{nbsp}Hat managed log storage. You can also visualize your log data in the {product-title} web console, or the Kibana web console, depending on your deployed log storage solution.

// Text snippet included in the following assemblies:
//
// logging/cluster-logging.adoc
//
// Text snippet included in the following modules:
//
//

:_mod-docs-content-type: SNIPPET

[NOTE]
====
The Kibana web console is now deprecated is planned to be removed in a future logging release.
====

{product-title} cluster administrators can deploy the {logging} by using Operators. For information, see xref:../logging/cluster-logging-deploying.adoc#cluster-logging-deploying[Installing the {logging-title}].

The Operators are responsible for deploying, upgrading, and maintaining the {logging}. After the Operators are installed, you can create a `ClusterLogging` custom resource (CR) to schedule {logging} pods and other resources necessary to support the {logging}. You can also create a `ClusterLogForwarder` CR to specify which logs are collected, how they are transformed, and where they are forwarded to.

[NOTE]
====
Because the internal {product-title} Elasticsearch log store does not provide secure storage for audit logs, audit logs are not stored in the internal Elasticsearch instance by default. If you want to send the audit logs to the default internal Elasticsearch log store, for example to view the audit logs in Kibana, you must use the Log Forwarding API as described in xref:../logging/config/cluster-logging-log-store.adoc#cluster-logging-elasticsearch-audit_cluster-logging-log-store[Forward audit logs to the log store].
====

:leveloffset: +1

// Module included in the following assemblies:
//
// * logging/cluster-logging.adoc

:_mod-docs-content-type: CONCEPT
[id="logging-architecture-overview_{context}"]
= Logging architecture

The major components of the {logging} are:

Collector:: The collector is a daemonset that deploys pods to each {product-title} node. It collects log data from each node, transforms the data, and forwards it to configured outputs. You can use the Vector collector or the legacy Fluentd collector.
+
--
// Text snippet included in the following assemblies:
//
// * logging/cluster-logging-deploying.adoc
//
// Text snippet included in the following modules:
//
//
:_mod-docs-content-type: SNIPPET

[NOTE]
====
As of logging version 5.6 Fluentd is deprecated and is planned to be removed in a future release. Red Hat will provide bug fixes and support for this feature during the current release lifecycle, but this feature will no longer receive enhancements and will be removed. As an alternative to Fluentd, you can use Vector instead.
====
--

Log store:: The log store stores log data for analysis and is the default output for the log forwarder. You can use the default LokiStack log store, the legacy Elasticsearch log store, or forward logs to additional external log stores.
+
--
// Text snippet included in the following assemblies:
//
// * logging/cluster-logging-deploying.adoc
//
// Text snippet included in the following modules:
//
//
:_mod-docs-content-type: SNIPPET

[NOTE]
====
As of logging version 5.4.3 the OpenShift Elasticsearch Operator is deprecated and is planned to be removed in a future release. Red Hat will provide bug fixes and support for this feature during the current release lifecycle, but this feature will no longer receive enhancements and will be removed. As an alternative to using the OpenShift Elasticsearch Operator to manage the default log storage, you can use the Loki Operator.
====
--

Visualization:: You can use a UI component to view a visual representation of your log data. The UI provides a graphical interface to search, query, and view stored logs. If you are using LokiStack as the default log storage, the {product-title} web console UI is provided by enabling the {product-title} console plugin. If you are using Elasticsearch as the default log storage, you can use Kibana.
+
--
// Text snippet included in the following assemblies:
//
// logging/cluster-logging.adoc
//
// Text snippet included in the following modules:
//
//

:_mod-docs-content-type: SNIPPET

[NOTE]
====
The Kibana web console is now deprecated is planned to be removed in a future logging release.
====
--

The {logging-title} collects container logs and node logs. These are categorized into types:

Application logs:: Container logs generated by user applications running in the cluster, except infrastructure container applications.

Infrastructure logs:: Container logs generated by infrastructure namespaces: `openshift*`, `kube*`, or `default`, as well as journald messages from nodes.

Audit logs:: Logs generated by auditd, the node audit system, which are stored in the */var/log/audit/audit.log* file, and logs from the `auditd`, `kube-apiserver`, `openshift-apiserver` services, as well as the `ovn` project if enabled.

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * virt/support/virt-openshift-cluster-monitoring.adoc
// * logging/cluster-logging.adoc
// * serverless/monitor/cluster-logging-serverless.adoc

// This module uses conditionalized paragraphs so that the module
// can be re-used in associated products.

:_mod-docs-content-type: CONCEPT
[id="cluster-logging-about_{context}"]
= About deploying the {logging-title}

Administrators can deploy the {logging} by using the {product-title} web console or the {oc-first} to install the {logging} Operators. The Operators are responsible for deploying, upgrading, and maintaining the {logging}.

Administrators and application developers can view the logs of the projects for which they have view access.

[id="cluster-logging-about-custom-resources_{context}"]
== Logging custom resources

You can configure your {logging} deployment with custom resource (CR) YAML files implemented by each Operator.

*Red Hat Openshift Logging Operator*:

* `ClusterLogging` (CL) - After the Operators are installed, you create a `ClusterLogging` custom resource (CR) to schedule {logging} pods and other resources necessary to support the {logging}. The `ClusterLogging` CR deploys the collector and forwarder, which currently are both implemented by a daemonset running on each node. The Red Hat OpenShift Logging Operator watches the `ClusterLogging` CR and adjusts the logging deployment accordingly.

* `ClusterLogForwarder` (CLF) - Generates collector configuration to forward logs per user configuration.

*Loki Operator*:

* `LokiStack` - Controls the Loki cluster as log store and the web proxy with OpenShift Container Platform authentication integration to enforce multi-tenancy.

*OpenShift Elasticsearch Operator*:

[NOTE]
====
These CRs are generated and managed by the Red Hat OpenShift Elasticsearch Operator. Manual changes cannot be made without being overwritten by the Operator.
====

* `ElasticSearch` - Configure and deploy an Elasticsearch instance as the default log store.

* `Kibana` - Configure and deploy Kibana instance to search, query and view logs.

:leveloffset!:


:leveloffset: +2

// Module included in the following assemblies:
//
// * logging/cluster-logging.adoc

:_mod-docs-content-type: CONCEPT
[id="cluster-logging-json-logging-about_{context}"]
= About JSON {product-title} Logging

You can use JSON logging to configure the Log Forwarding API to parse JSON strings into a structured object. You can perform the following tasks:

* Parse JSON logs
* Configure JSON log data for Elasticsearch
* Forward JSON logs to the Elasticsearch log store

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * logging/cluster-logging.adoc

:_mod-docs-content-type: CONCEPT
[id="cluster-logging-collecting-storing-kubernetes-events-about_{context}"]
= About collecting and storing Kubernetes events

The {product-title} Event Router is a pod that watches Kubernetes events and logs them for collection by {product-title} Logging. You must manually deploy the Event Router.

:leveloffset!:

For information, see xref:../logging/log_collection_forwarding/cluster-logging-eventrouter.adoc#cluster-logging-eventrouter[About collecting and storing Kubernetes events].

:leveloffset: +2

// Module included in the following assemblies:
//
// * logging/cluster-logging.adoc

:_mod-docs-content-type: CONCEPT
[id="cluster-logging-troubleshoot-logging-about_{context}"]
= About troubleshooting {product-title} Logging

You can troubleshoot the logging issues by performing the following tasks:

* Viewing logging status
* Viewing the status of the log store
* Understanding logging alerts
* Collecting logging data for Red Hat Support
* Troubleshooting for critical alerts

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * logging/cluster-logging.adoc

:_mod-docs-content-type: CONCEPT
[id="cluster-logging-export-fields-about_{context}"]
= About exporting fields

The logging system exports fields. Exported fields are present in the log records and are available for searching from Elasticsearch and Kibana.

:leveloffset!:

For information, see xref:../logging/cluster-logging-exported-fields.adoc#cluster-logging-exported-fields[About exporting fields].

:leveloffset: +2

// Module included in the following assemblies:
//
// * logging/cluster-logging.adoc

:_mod-docs-content-type: CONCEPT
[id="cluster-logging-about-logstore_{context}"]
= About the log store

By default, {product-title} uses link:https://www.elastic.co/products/elasticsearch[Elasticsearch (ES)] to store log data. Optionally you can use the Log Forwarder API to forward logs to an external store. Several types of store are supported, including fluentd, rsyslog, kafka and others.

The {logging} Elasticsearch instance is optimized and tested for short term storage, approximately seven days. If you want to retain your logs over a longer term, it is recommended you move the data to a third-party storage system.

Elasticsearch organizes the log data from Fluentd into datastores, or _indices_, then subdivides each index into multiple pieces called _shards_, which it spreads across a set of Elasticsearch nodes in an Elasticsearch cluster. You can configure Elasticsearch to make copies of the shards, called _replicas_, which Elasticsearch also spreads across the Elasticsearch nodes. The `ClusterLogging` custom resource (CR) allows you to specify how the shards are replicated to provide data redundancy and resilience to failure. You can also specify how long the different types of logs are retained using a retention policy in the `ClusterLogging` CR.

[NOTE]
====
The number of primary shards for the index templates is equal to the number of Elasticsearch data nodes.
====

The Red Hat OpenShift Logging Operator and companion OpenShift Elasticsearch Operator ensure that each Elasticsearch node is deployed using a unique deployment that includes its own storage volume.
You can use a `ClusterLogging` custom resource (CR) to increase the number of Elasticsearch nodes, as needed.
See the link:https://www.elastic.co/guide/en/elasticsearch/guide/current/hardware.html[Elasticsearch documentation] for considerations involved in configuring storage.

[NOTE]
====
A highly-available Elasticsearch environment requires at least three Elasticsearch nodes, each on a different host.
====

Role-based access control (RBAC) applied on the Elasticsearch indices enables the controlled access of the logs to the developers. Administrators can access all logs and developers can access only the logs in their projects.

:leveloffset!:

For information, see xref:../logging/config/cluster-logging-log-store.adoc#cluster-logging-log-store[Configuring the log store].

:leveloffset: +2

// Module included in the following assemblies:
//
// * logging/cluster-logging.adoc

:_mod-docs-content-type: CONCEPT
[id="cluster-logging-eventrouter-about_{context}"]
= About event routing

The Event Router is a pod that watches {product-title} events so they can be collected by the {logging-title}.
The Event Router collects events from all projects and writes them to `STDOUT`. Fluentd collects those events and forwards them into the {product-title} Elasticsearch instance. Elasticsearch indexes the events to the `infra` index.

You must manually deploy the Event Router.

:leveloffset!:

For information, see xref:../logging/log_collection_forwarding/cluster-logging-eventrouter.adoc#cluster-logging-eventrouter[Collecting and storing Kubernetes events].

//# includes=_attributes/common-attributes,_attributes/attributes-openshift-dedicated,snippets/logging-kibana-dep-snip,modules/logging-architecture-overview,modules/snippets/logging-fluentd-dep-snip,modules/snippets/logging-elastic-dep-snip,modules/snippets/logging-kibana-dep-snip,modules/cluster-logging-about,modules/cluster-logging-json-logging-about,modules/cluster-logging-collecting-storing-kubernetes-events,modules/cluster-logging-troubleshoot-logging,modules/cluster-logging-export-fields,modules/cluster-logging-about-logstore,modules/cluster-logging-eventrouter-about

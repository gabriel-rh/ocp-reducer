<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE book [
<!ENTITY % sgml.features "IGNORE">
<!ENTITY % xml.features "INCLUDE">
<!ENTITY % DOCBOOK_ENTS PUBLIC "-//OASIS//ENTITIES DocBook Character Entities V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/dbcentx.mod">
%DOCBOOK_ENTS;
]>
<?asciidoc-toc?>
<?asciidoc-numbered?>
<book xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0">
<info>
<title>Migrating from version 3 to 4</title>
<date>2024-02-23</date>
<title>Migrating from version 3 to 4</title>
<productname>OpenShift Container Platform</productname>
<productnumber>4.14</productnumber>
<subtitle>Enter a short description here.</subtitle>
<abstract>
    <para>A short overview and summary of the book's subject and purpose, traditionally no more than one paragraph long.</para>
</abstract>
<authorgroup>
    <orgname>Red Hat OpenShift Documentation Team</orgname>
</authorgroup>
<xi:include href="Common_Content/Legal_Notice.xml" xmlns:xi="http://www.w3.org/2001/XInclude" />
</info>
<chapter xml:id="migration-from-version-3-to-4-overview">
<title>Migration from OpenShift Container Platform 3 to 4 overview</title>
<simpara>OpenShift Container Platform 4 clusters are different from OpenShift Container Platform 3 clusters. OpenShift Container Platform 4 clusters contain new technologies and functionality that result in a cluster that is self-managing, flexible, and automated. To learn more about migrating from OpenShift Container Platform 3 to 4 see <link linkend="about-migrating-from-3-to-4">About migrating from OpenShift Container Platform 3 to 4</link>.</simpara>
<section xml:id="mtc-3-to-4-overview-differences-mtc">
<title>Differences between OpenShift Container Platform 3 and 4</title>
<simpara>Before migrating from OpenShift Container Platform 3 to 4, you can check <link linkend="planning-migration-3-4">differences between OpenShift Container Platform 3 and 4</link>. Review the following information:</simpara>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/architecture/#architecture">Architecture</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/architecture/#architecture-installation">Installation and update</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/storage/#index">Storage</link>, <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/networking/#understanding-networking">network</link>, <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/logging/#cluster-logging">logging</link>, <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/security_and_compliance/#index">security</link>, and <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/monitoring/#monitoring-overview">monitoring considerations</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="mtc-3-to-4-overview-planning-network-considerations-mtc">
<title>Planning network considerations</title>
<simpara>Before migrating from OpenShift Container Platform 3 to 4, review the <link linkend="planning-migration-3-4">differences between OpenShift Container Platform 3 and 4</link> for information about the following areas:</simpara>
<itemizedlist>
<listitem>
<simpara><link linkend="dns-considerations_planning-considerations-3-4">DNS considerations</link></simpara>
<itemizedlist>
<listitem>
<simpara><link linkend="migration-isolating-dns-domain-of-target-cluster-from-clients_planning-considerations-3-4">Isolating the DNS domain of the target cluster from the clients</link>.</simpara>
</listitem>
<listitem>
<simpara><link linkend="migration-setting-up-target-cluster-to-accept-source-dns-domain_planning-considerations-3-4">Setting up the target cluster to accept the source DNS domain</link>.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<simpara>You can migrate stateful application workloads from OpenShift Container Platform 3 to 4 at the granularity of a namespace. To learn more about MTC see <link linkend="about-mtc-3-4">Understanding MTC</link>.</simpara>
<note>
<simpara>If you are migrating from OpenShift Container Platform 3, see <link linkend="about-migrating-from-3-to-4">About migrating from OpenShift Container Platform 3 to 4</link> and <link linkend="migration-installing-legacy-operator_installing-3-4">Installing the legacy Migration Toolkit for Containers Operator on OpenShift Container Platform 3</link>.</simpara>
</note>
</section>
<section xml:id="mtc-overview-install-mtc">
<title>Installing MTC</title>
<simpara>Review the following tasks to install the MTC:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara><link linkend="migration-installing-mtc-on-ocp-4_installing-3-4">Install the Migration Toolkit for Containers Operator on target cluster by using Operator Lifecycle Manager (OLM)</link>.</simpara>
</listitem>
<listitem>
<simpara><link linkend="migration-installing-legacy-operator_installing-3-4">Install the legacy Migration Toolkit for Containers Operator on the source cluster manually</link>.</simpara>
</listitem>
<listitem>
<simpara><link linkend="configuring-replication-repository_installing-3-4">Configure object storage to use as a replication repository</link>.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="mtc-overview-upgrade-mtc">
<title>Upgrading MTC</title>
<simpara>You <link linkend="upgrading-3-4">upgrade the Migration Toolkit for Containers (MTC)</link> on OpenShift Container Platform 4.14 by using OLM. You upgrade MTC on OpenShift Container Platform 3 by reinstalling the legacy Migration Toolkit for Containers Operator.</simpara>
</section>
<section xml:id="mtc-overview-mtc-checklists">
<title>Reviewing premigration checklists</title>
<simpara>Before you migrate your application workloads with the Migration Toolkit for Containers (MTC), review the <link linkend="premigration-checklists-3-4">premigration checklists</link>.</simpara>
</section>
<section xml:id="mtc-overview-migrate-mtc-applications">
<title>Migrating applications</title>
<simpara>You can migrate your applications by using the MTC <link linkend="migrating-applications-mtc-web-console_migrating-applications-3-4">web console</link> or <link linkend="migrating-applications-cli_advanced-migration-options-3-4">the command line</link>.</simpara>
</section>
<section xml:id="mtc-overview-advanced-migration-options">
<title>Advanced migration options</title>
<simpara>You can automate your migrations and modify MTC custom resources to improve the performance of large-scale migrations by using the following options:</simpara>
<itemizedlist>
<listitem>
<simpara><link linkend="migration-state-migration-cli_advanced-migration-options-3-4">Running a state migration</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="migration-hooks_advanced-migration-options-3-4">Creating migration hooks</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="migration-plan-options_advanced-migration-options-3-4">Editing, excluding, and mapping migrated resources</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="migration-controller-options_advanced-migration-options-3-4">Configuring the migration controller for large migrations</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="mtc-overview-troubleshooting-mtc">
<title>Troubleshooting migrations</title>
<simpara>You can perform the following troubleshooting tasks:</simpara>
<itemizedlist>
<listitem>
<simpara><link linkend="migration-viewing-migration-plan-resources_troubleshooting-3-4">Viewing migration plan resources by using the MTC web console</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="migration-viewing-migration-plan-log_troubleshooting-3-4">Viewing the migration plan aggregated log file</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="migration-using-mig-log-reader_troubleshooting-3-4">Using the migration log reader</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="migration-accessing-performance-metrics_troubleshooting-3-4">Accessing performance metrics</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="migration-using-must-gather_troubleshooting-3-4">Using the <literal>must-gather</literal> tool</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="migration-debugging-velero-resources_troubleshooting-3-4">Using the Velero CLI to debug <literal>Backup</literal> and <literal>Restore</literal> CRs</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="migration-using-mtc-crs-for-troubleshooting_troubleshooting-3-4">Using MTC custom resources for troubleshooting</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="common-issues-and-concerns_troubleshooting-3-4">Checking common issues and concerns</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="mtc-overview-roll-back-mtc">
<title>Rolling back a migration</title>
<simpara>You can <link linkend="rolling-back-migration_troubleshooting-3-4">roll back a migration</link> by using the MTC web console, by using the CLI, or manually.</simpara>
</section>
<section xml:id="mtc-overview-uninstall-mtc">
<title>Uninstalling MTC and deleting resources</title>
<simpara>You can <link linkend="migration-uninstalling-mtc-clean-up_installing-3-4">uninstall the MTC and delete its resources</link> to clean up the cluster.</simpara>
</section>
</chapter>
<chapter xml:id="about-migrating-from-3-to-4">
<title>About migrating from OpenShift Container Platform 3 to 4</title>
<simpara>OpenShift Container Platform 4 contains new technologies and functionality that result in a cluster that is self-managing, flexible, and automated. OpenShift Container Platform 4 clusters are deployed and managed very differently from OpenShift Container Platform 3.</simpara>
<simpara>The most effective way to migrate from OpenShift Container Platform 3 to 4 is by using a CI/CD pipeline to automate deployments in an <link xlink:href="https://www.redhat.com/en/topics/devops/what-is-application-lifecycle-management-alm">application lifecycle management</link> framework.</simpara>
<simpara>If you do not have a CI/CD pipeline or if you are migrating stateful applications, you can use the Migration Toolkit for Containers (MTC) to migrate your application workloads.</simpara>
<simpara>You can use Red Hat Advanced Cluster Management for Kubernetes to help you import and manage your OpenShift Container Platform 3 clusters easily, enforce policies, and redeploy your applications. Take advantage of the <link xlink:href="https://www.redhat.com/en/engage/free-access-redhat-e-202202170127">free subscription</link> to use Red Hat Advanced Cluster Management to simplify your migration process.</simpara>
<simpara>To successfully transition to OpenShift Container Platform 4, review the following information:</simpara>
<variablelist>
<varlistentry>
<term><link linkend="planning-migration-3-4">Differences between OpenShift Container Platform 3 and 4</link></term>
<listitem>
<itemizedlist>
<listitem>
<simpara>Architecture</simpara>
</listitem>
<listitem>
<simpara>Installation and upgrade</simpara>
</listitem>
<listitem>
<simpara>Storage, network, logging, security, and monitoring considerations</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
<varlistentry>
<term><link linkend="about-mtc-3-4">About the Migration Toolkit for Containers</link></term>
<listitem>
<itemizedlist>
<listitem>
<simpara>Workflow</simpara>
</listitem>
<listitem>
<simpara>File system and snapshot copy methods for persistent volumes (PVs)</simpara>
</listitem>
<listitem>
<simpara>Direct volume migration</simpara>
</listitem>
<listitem>
<simpara>Direct image migration</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
<varlistentry>
<term><link linkend="advanced-migration-options-3-4">Advanced migration options</link></term>
<listitem>
<itemizedlist>
<listitem>
<simpara>Automating your migration with migration hooks</simpara>
</listitem>
<listitem>
<simpara>Using the MTC API</simpara>
</listitem>
<listitem>
<simpara>Excluding resources from a migration plan</simpara>
</listitem>
<listitem>
<simpara>Configuring the <literal>MigrationController</literal> custom resource for large-scale migrations</simpara>
</listitem>
<listitem>
<simpara>Enabling automatic PV resizing for direct volume migration</simpara>
</listitem>
<listitem>
<simpara>Enabling cached Kubernetes clients for improved performance</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
</variablelist>
<simpara>For new features and enhancements, technical changes, and known issues, see the <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/migration_toolkit_for_containers/#mtc-release-notes">MTC release notes</link>.</simpara>
</chapter>
<chapter xml:id="planning-migration-3-4">
<title>Differences between OpenShift Container Platform 3 and 4</title>
<simpara>OpenShift Container Platform 4.14 introduces architectural changes and enhancements. The procedures that you used to manage your OpenShift Container Platform 3 cluster might not apply to OpenShift Container Platform 4.</simpara>
<simpara>For information about configuring your OpenShift Container Platform 4 cluster, review the appropriate sections of the OpenShift Container Platform documentation. For information about new features and other notable technical changes, review the <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/release_notes/#ocp-4-15-release-notes">OpenShift Container Platform 4.15 release notes</link>.</simpara>
<simpara>It is not possible to upgrade your existing OpenShift Container Platform 3 cluster to OpenShift Container Platform 4. You must start with a new OpenShift Container Platform 4 installation. Tools are available to assist in migrating your control plane settings and application workloads.</simpara>
<section xml:id="migration-differences-architecture">
<title>Architecture</title>
<simpara>With OpenShift Container Platform 3, administrators individually deployed Red Hat Enterprise Linux (RHEL) hosts, and then installed OpenShift Container Platform on top of these hosts to form a cluster. Administrators were responsible for properly configuring these hosts and performing updates.</simpara>
<simpara>OpenShift Container Platform 4 represents a significant change in the way that OpenShift Container Platform clusters are deployed and managed. OpenShift Container Platform 4 includes new technologies and functionality, such as Operators, machine sets, and Red Hat Enterprise Linux CoreOS (RHCOS), which are core to the operation of the cluster. This technology shift enables clusters to self-manage some functions previously performed by administrators. This also ensures platform stability and consistency, and simplifies installation and scaling.</simpara>
<simpara>Beginning with OpenShift Container Platform 4.13, RHCOS now uses Red Hat Enterprise Linux (RHEL) 9.2 packages. This enhancement enables the latest fixes and features as well as the latest hardware support and driver updates. For more information about how this upgrade to RHEL 9.2 might affect your options configuration and services as well as driver and container support, see the <link xlink:href="https://docs.openshift.com/container-platform/4.13/release_notes/ocp-4-13-release-notes.html#ocp-4-13-rhel-9-considerations">RHCOS now uses RHEL 9.2</link> in the <emphasis>OpenShift Container Platform 4.13 release notes</emphasis>.</simpara>
<simpara>For more information, see <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/architecture/#architecture">OpenShift Container Platform architecture</link>.</simpara>
<bridgehead xml:id="_immutable-infrastructure" renderas="sect3">Immutable infrastructure</bridgehead>
<simpara>OpenShift Container Platform 4 uses Red Hat Enterprise Linux CoreOS (RHCOS), which is designed to run containerized applications, and provides efficient installation, Operator-based management, and simplified upgrades. RHCOS is an immutable container host, rather than a customizable operating system like RHEL. RHCOS enables OpenShift Container Platform 4 to manage and automate the deployment of the underlying container host. RHCOS is a part of OpenShift Container Platform, which means that everything runs inside a container and is deployed using OpenShift Container Platform.</simpara>
<simpara>In OpenShift Container Platform 4, control plane nodes must run RHCOS, ensuring that full-stack automation is maintained for the control plane. This makes rolling out updates and upgrades a much easier process than in OpenShift Container Platform 3.</simpara>
<simpara>For more information, see <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/architecture/#architecture-rhcos">Red Hat Enterprise Linux CoreOS (RHCOS)</link>.</simpara>
<bridgehead xml:id="_operators" renderas="sect3">Operators</bridgehead>
<simpara>Operators are a method of packaging, deploying, and managing a Kubernetes application. Operators ease the operational complexity of running another piece of software. They watch over your environment and use the current state to make decisions in real time. Advanced Operators are designed to upgrade and react to failures automatically.</simpara>
<simpara>For more information, see <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/operators/#olm-what-operators-are">Understanding Operators</link>.</simpara>
</section>
<section xml:id="migration-differences-install">
<title>Installation and upgrade</title>
<bridgehead xml:id="_installation-process" renderas="sect3">Installation process</bridgehead>
<simpara>To install OpenShift Container Platform 3.11, you prepared your Red Hat Enterprise Linux (RHEL) hosts, set all of the configuration values your cluster needed, and then ran an Ansible playbook to install and set up your cluster.</simpara>
<simpara>In OpenShift Container Platform 4.14, you use the OpenShift installation program to create a minimum set of resources required for a cluster. After the cluster is running, you use Operators to further configure your cluster and to install new services. After first boot, Red Hat Enterprise Linux CoreOS (RHCOS) systems are managed by the Machine Config Operator (MCO) that runs in the OpenShift Container Platform cluster.</simpara>
<simpara>For more information, see <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/architecture/#installation-process_architecture-installation">Installation process</link>.</simpara>
<simpara>If you want to add Red Hat Enterprise Linux (RHEL) worker machines to your OpenShift Container Platform 4.14 cluster, you use an Ansible playbook to join the RHEL worker machines after the cluster is running. For more information, see <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/machine_management/#adding-rhel-compute">Adding RHEL compute machines to an OpenShift Container Platform cluster</link>.</simpara>
<bridgehead xml:id="_infrastructure-options" renderas="sect3">Infrastructure options</bridgehead>
<simpara>In OpenShift Container Platform 3.11, you installed your cluster on infrastructure that you prepared and maintained. In addition to providing your own infrastructure, OpenShift Container Platform 4 offers an option to deploy a cluster on infrastructure that the OpenShift Container Platform installation program provisions and the cluster maintains.</simpara>
<simpara>For more information, see <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/architecture/#installation-overview_architecture-installation">OpenShift Container Platform installation overview</link>.</simpara>
<bridgehead xml:id="_upgrading-your-cluster" renderas="sect3">Upgrading your cluster</bridgehead>
<simpara>In OpenShift Container Platform 3.11, you upgraded your cluster by running Ansible playbooks. In OpenShift Container Platform 4.14, the cluster manages its own updates, including updates to Red Hat Enterprise Linux CoreOS (RHCOS) on cluster nodes. You can easily upgrade your cluster by using the web console or by using the <literal>oc adm upgrade</literal> command from the OpenShift CLI and the Operators will automatically upgrade themselves. If your OpenShift Container Platform 4.14 cluster has RHEL worker machines, then you will still need to run an Ansible playbook to upgrade those worker machines.</simpara>
<simpara>For more information, see <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/updating_clusters/#updating-cluster-web-console">Updating clusters</link>.</simpara>
</section>
<section xml:id="migration-considerations">
<title>Migration considerations</title>
<simpara>Review the changes and other considerations that might affect your transition from OpenShift Container Platform 3.11 to OpenShift Container Platform 4.</simpara>
<section xml:id="migration-preparing-storage">
<title>Storage considerations</title>
<simpara>Review the following storage changes to consider when transitioning from OpenShift Container Platform 3.11 to OpenShift Container Platform 4.14.</simpara>
<bridgehead xml:id="_local-volume-persistent-storage" renderas="sect4">Local volume persistent storage</bridgehead>
<simpara>Local storage is only supported by using the Local Storage Operator in OpenShift Container Platform 4.14. It is not supported to use the local provisioner method from OpenShift Container Platform 3.11.</simpara>
<simpara>For more information, see <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/storage/#persistent-storage-using-local-volume">Persistent storage using local volumes</link>.</simpara>
<bridgehead xml:id="_flexvolume-persistent-storage" renderas="sect4">FlexVolume persistent storage</bridgehead>
<simpara>The FlexVolume plugin location changed from OpenShift Container Platform 3.11. The new location in OpenShift Container Platform 4.14 is <literal>/etc/kubernetes/kubelet-plugins/volume/exec</literal>. Attachable FlexVolume plugins are no longer supported.</simpara>
<simpara>For more information, see <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/storage/#persistent-storage-using-flexvolume">Persistent storage using FlexVolume</link>.</simpara>
<bridgehead xml:id="_container-storage-interface-csi-persistent-storage" renderas="sect4">Container Storage Interface (CSI) persistent storage</bridgehead>
<simpara>Persistent storage using the Container Storage Interface (CSI) was <link xlink:href="https://access.redhat.com/support/offerings/techpreview">Technology Preview</link> in OpenShift Container Platform 3.11. OpenShift Container Platform 4.14 ships with <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/storage/#csi-drivers-supported_persistent-storage-csi">several CSI drivers</link>. You can also install your own driver.</simpara>
<simpara>For more information, see <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/storage/#persistent-storage-using-csi">Persistent storage using the Container Storage Interface (CSI)</link>.</simpara>
<bridgehead xml:id="_red-hat-openshift-data-foundation" renderas="sect4">Red Hat OpenShift Data Foundation</bridgehead>
<simpara>OpenShift Container Storage 3, which is available for use with OpenShift Container Platform 3.11, uses Red Hat Gluster Storage as the backing storage.</simpara>
<simpara>Red Hat OpenShift Data Foundation 4, which is available for use with OpenShift Container Platform 4, uses Red Hat Ceph Storage as the backing storage.</simpara>
<simpara>For more information, see <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/storage/#red-hat-openshift-data-foundation">Persistent storage using Red Hat OpenShift Data Foundation</link> and the <link xlink:href="https://access.redhat.com/articles/4731161">interoperability matrix</link> article.</simpara>
<bridgehead xml:id="_unsupported-persistent-storage-options" renderas="sect4">Unsupported persistent storage options</bridgehead>
<simpara>Support for the following persistent storage options from OpenShift Container Platform 3.11 has changed in OpenShift Container Platform 4.14:</simpara>
<itemizedlist>
<listitem>
<simpara>GlusterFS is no longer supported.</simpara>
</listitem>
<listitem>
<simpara>CephFS as a standalone product is no longer supported.</simpara>
</listitem>
<listitem>
<simpara>Ceph RBD as a standalone product is no longer supported.</simpara>
</listitem>
</itemizedlist>
<simpara>If you used one of these in OpenShift Container Platform 3.11, you must choose a different persistent storage option for full support in OpenShift Container Platform 4.14.</simpara>
<simpara>For more information, see <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/storage/#understanding-persistent-storage">Understanding persistent storage</link>.</simpara>
<bridgehead xml:id="_migration-of-in-tree-volumes-to-csi-drivers" renderas="sect4">Migration of in-tree volumes to CSI drivers</bridgehead>
<simpara>OpenShift Container Platform 4 is migrating in-tree volume plugins to their Container Storage Interface (CSI) counterparts. In OpenShift Container Platform 4.14, CSI drivers are the new default for the following in-tree volume types:</simpara>
<itemizedlist>
<listitem>
<simpara>Amazon Web Services (AWS) Elastic Block Storage (EBS)</simpara>
</listitem>
<listitem>
<simpara>Azure Disk</simpara>
</listitem>
<listitem>
<simpara>Azure File</simpara>
</listitem>
<listitem>
<simpara>Google Cloud Platform Persistent Disk (GCP PD)</simpara>
</listitem>
<listitem>
<simpara>OpenStack Cinder</simpara>
</listitem>
<listitem>
<simpara>VMware vSphere</simpara>
<note>
<simpara>As of OpenShift Container Platform 4.13, VMware vSphere is not available by default. However, you can opt into VMware vSphere.</simpara>
</note>
</listitem>
</itemizedlist>
<simpara>All aspects of volume lifecycle, such as creation, deletion, mounting, and unmounting, is handled by the CSI driver.</simpara>
<simpara>For more information, see <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/storage/#persistent-storage-csi-migration">CSI automatic migration</link>.</simpara>
</section>
<section xml:id="migration-preparing-networking">
<title>Networking considerations</title>
<simpara>Review the following networking changes to consider when transitioning from OpenShift Container Platform 3.11 to OpenShift Container Platform 4.14.</simpara>
<bridgehead xml:id="_network-isolation-mode" renderas="sect4">Network isolation mode</bridgehead>
<simpara>The default network isolation mode for OpenShift Container Platform 3.11 was <literal>ovs-subnet</literal>, though users frequently switched to use <literal>ovn-multitenant</literal>. The default network isolation mode for OpenShift Container Platform 4.14 is controlled by a network policy.</simpara>
<simpara>If your OpenShift Container Platform 3.11 cluster used the <literal>ovs-subnet</literal> or <literal>ovs-multitenant</literal> mode, it is recommended to switch to a network policy for your OpenShift Container Platform 4.14 cluster. Network policies are supported upstream, are more flexible, and they provide the functionality that <literal>ovs-multitenant</literal> does. If you want to maintain the <literal>ovs-multitenant</literal> behavior while using a network policy in OpenShift Container Platform 4.14, follow the steps to <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/networking/#multitenant-network-policy">configure multitenant isolation using network policy</link>.</simpara>
<simpara>For more information, see <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/networking/#about-network-policy">About network policy</link>.</simpara>
<bridgehead xml:id="_ovn-kubernetes-as-the-default-networking-plugin-in-red-hat-openshift-networking" renderas="sect4">OVN-Kubernetes as the default networking plugin in Red Hat OpenShift Networking</bridgehead>
<simpara>In OpenShift Container Platform 3.11, OpenShift SDN was the default networking plugin in Red Hat OpenShift Networking. In OpenShift Container Platform 4.14, OVN-Kubernetes is now the default networking plugin.</simpara>
<simpara>For information on migrating to OVN-Kubernetes from OpenShift SDN, see <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/networking/#migrate-from-openshift-sdn">Migrating from the OpenShift SDN network plugin</link>.</simpara>
</section>
<section xml:id="migration-preparing-logging">
<title>Logging considerations</title>
<simpara>Review the following logging changes to consider when transitioning from OpenShift Container Platform 3.11 to OpenShift Container Platform 4.14.</simpara>
<bridgehead xml:id="_deploying-openshift-logging" renderas="sect4">Deploying OpenShift Logging</bridgehead>
<simpara>OpenShift Container Platform 4 provides a simple deployment mechanism for OpenShift Logging, by using a Cluster Logging custom resource.</simpara>
<simpara>For more information, see <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/logging/#cluster-logging-deploying_cluster-logging-deploying">Installing OpenShift Logging</link>.</simpara>
<bridgehead xml:id="_aggregated-logging-data" renderas="sect4">Aggregated logging data</bridgehead>
<simpara>You cannot transition your aggregate logging data from OpenShift Container Platform 3.11 into your new OpenShift Container Platform 4 cluster.</simpara>
<simpara>For more information, see <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/logging/#cluster-logging-about_cluster-logging">About OpenShift Logging</link>.</simpara>
<bridgehead xml:id="_unsupported-logging-configurations" renderas="sect4">Unsupported logging configurations</bridgehead>
<simpara>Some logging configurations that were available in OpenShift Container Platform 3.11 are no longer supported in OpenShift Container Platform 4.14.</simpara>
<simpara>For more information on the explicitly unsupported logging cases, see the <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/logging/#cluster-logging-support">logging support documentation</link>.</simpara>
</section>
<section xml:id="migration-preparing-security">
<title>Security considerations</title>
<simpara>Review the following security changes to consider when transitioning from OpenShift Container Platform 3.11 to OpenShift Container Platform 4.14.</simpara>
<bridgehead xml:id="_unauthenticated-access-to-discovery-endpoints" renderas="sect4">Unauthenticated access to discovery endpoints</bridgehead>
<simpara>In OpenShift Container Platform 3.11, an unauthenticated user could access the discovery endpoints (for example, <literal>/api/*</literal> and <literal>/apis/*</literal>). For security reasons, unauthenticated access to the discovery endpoints is no longer allowed in OpenShift Container Platform 4.14. If you do need to allow unauthenticated access, you can configure the RBAC settings as necessary; however, be sure to consider the security implications as this can expose internal cluster components to the external network.</simpara>
<bridgehead xml:id="_identity-providers" renderas="sect4">Identity providers</bridgehead>
<simpara>Configuration for identity providers has changed for OpenShift Container Platform 4, including the following notable changes:</simpara>
<itemizedlist>
<listitem>
<simpara>The request header identity provider in OpenShift Container Platform 4.14 requires mutual TLS, where in OpenShift Container Platform 3.11 it did not.</simpara>
</listitem>
<listitem>
<simpara>The configuration of the OpenID Connect identity provider was simplified in OpenShift Container Platform 4.14. It now obtains data, which previously had to specified in OpenShift Container Platform 3.11, from the provider&#8217;s <literal>/.well-known/openid-configuration</literal> endpoint.</simpara>
</listitem>
</itemizedlist>
<simpara>For more information, see <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/authentication_and_authorization/#understanding-identity-provider">Understanding identity provider configuration</link>.</simpara>
<bridgehead xml:id="_oauth-token-storage-format" renderas="sect4">OAuth token storage format</bridgehead>
<simpara>Newly created OAuth HTTP bearer tokens no longer match the names of their OAuth access token objects. The object names are now a hash of the bearer token and are no longer sensitive. This reduces the risk of leaking sensitive information.</simpara>
<bridgehead xml:id="_default-security-context-constraints" renderas="sect4">Default security context constraints</bridgehead>
<simpara>The <literal>restricted</literal> security context constraints (SCC) in OpenShift Container Platform 4 can no longer be accessed by any authenticated user as the <literal>restricted</literal> SCC in OpenShift Container Platform 3.11. The broad authenticated access is now granted to the <literal>restricted-v2</literal> SCC, which is more restrictive than the old <literal>restricted</literal> SCC. The <literal>restricted</literal> SCC still exists; users that want to use it must be specifically given permissions to do it.</simpara>
<simpara>For more information, see <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/authentication_and_authorization/#managing-pod-security-policies">Managing security context constraints</link>.</simpara>
</section>
<section xml:id="migration-preparing-monitoring">
<title>Monitoring considerations</title>
<simpara>Review the following monitoring changes when transitioning from OpenShift Container Platform 3.11 to OpenShift Container Platform 4.14. You cannot migrate Hawkular configurations and metrics to Prometheus.</simpara>
<bridgehead xml:id="_alert-for-monitoring-infrastructure-availability" renderas="sect4">Alert for monitoring infrastructure availability</bridgehead>
<simpara>The default alert that triggers to ensure the availability of the monitoring structure was called <literal>DeadMansSwitch</literal> in OpenShift Container Platform 3.11. This was renamed to <literal>Watchdog</literal> in OpenShift Container Platform 4. If you had PagerDuty integration set up with this alert in OpenShift Container Platform 3.11, you must set up the PagerDuty integration for the <literal>Watchdog</literal> alert in OpenShift Container Platform 4.</simpara>
<simpara>For more information, see <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/monitoring/#applying-custom-alertmanager-configuration_managing-alerts">Applying custom Alertmanager configuration</link>.</simpara>
</section>
</section>
</chapter>
<chapter xml:id="planning-considerations-3-4">
<title>Network considerations</title>
<simpara>Review the strategies for redirecting your application network traffic after migration.</simpara>
<section xml:id="dns-considerations_planning-considerations-3-4">
<title>DNS considerations</title>
<simpara>The DNS domain of the target cluster is different from the domain of the source cluster. By default, applications get FQDNs of the target cluster after migration.</simpara>
<simpara>To preserve the source DNS domain of migrated applications, select one of the two options described below.</simpara>
<section xml:id="migration-isolating-dns-domain-of-target-cluster-from-clients_planning-considerations-3-4">
<title>Isolating the DNS domain of the target cluster from the clients</title>
<simpara>You can allow the clients' requests sent to the DNS domain of the source cluster to reach the DNS domain of the target cluster without exposing the target cluster to the clients.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Place an exterior network component, such as an application load balancer or a reverse proxy, between the clients and the target cluster.</simpara>
</listitem>
<listitem>
<simpara>Update the application FQDN on the source cluster in the DNS server to return the IP address of the exterior network component.</simpara>
</listitem>
<listitem>
<simpara>Configure the network component to send requests received for the application in the source domain to the load balancer in the target cluster domain.</simpara>
</listitem>
<listitem>
<simpara>Create a wildcard DNS record for the <literal>*.apps.source.example.com</literal> domain that points to the IP address of the load balancer of the source cluster.</simpara>
</listitem>
<listitem>
<simpara>Create a DNS record for each application that points to the IP address of the exterior network component in front of the target cluster. A specific DNS record has higher priority than a wildcard record, so no conflict arises when the application FQDN is resolved.</simpara>
</listitem>
</orderedlist>
<note>
<itemizedlist>
<listitem>
<simpara>The exterior network component must terminate all secure TLS connections. If the connections pass through to the target cluster load balancer, the FQDN of the target application is exposed to the client and certificate errors occur.</simpara>
</listitem>
<listitem>
<simpara>The applications must not return links referencing the target cluster domain to the clients. Otherwise, parts of the application might not load or work properly.</simpara>
</listitem>
</itemizedlist>
</note>
</section>
<section xml:id="migration-setting-up-target-cluster-to-accept-source-dns-domain_planning-considerations-3-4">
<title>Setting up the target cluster to accept the source DNS domain</title>
<simpara>You can set up the target cluster to accept requests for a migrated application in the DNS domain of the source cluster.</simpara>
<formalpara>
<title>Procedure</title>
<para>For both non-secure HTTP access and secure HTTPS access, perform the following steps:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Create a route in the target cluster&#8217;s project that is configured to accept requests addressed to the application&#8217;s FQDN in the source cluster:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc expose svc &lt;app1-svc&gt; --hostname &lt;app1.apps.source.example.com&gt; \
 -n &lt;app1-namespace&gt;</programlisting>
<simpara>With this new route in place, the server accepts any request for that FQDN and sends it to the corresponding application pods.
In addition, when you migrate the application, another route is created in the target cluster domain. Requests reach the migrated application using either of these hostnames.</simpara>
</listitem>
<listitem>
<simpara>Create a DNS record with your DNS provider that points the application&#8217;s FQDN in the source cluster to the IP address of the default load balancer of the target cluster. This will redirect traffic away from your source cluster to your target cluster.</simpara>
<simpara>The FQDN of the application resolves to the load balancer of the target cluster. The default Ingress Controller router accept requests for that FQDN because a route for that hostname is exposed.</simpara>
</listitem>
</orderedlist>
<simpara>For secure HTTPS access, perform the following additional step:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Replace the x509 certificate of the default Ingress Controller created during the installation process with a custom certificate.</simpara>
</listitem>
<listitem>
<simpara>Configure this certificate to include the wildcard DNS domains for both the source and target clusters in the <literal>subjectAltName</literal> field.</simpara>
<simpara>The new certificate is valid for securing connections made using either DNS domain.</simpara>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara>See <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/security_and_compliance/#replacing-default-ingress">Replacing the default ingress certificate</link> for more information.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="migration-network-traffic-redirection-strategies_planning-considerations-3-4">
<title>Network traffic redirection strategies</title>
<simpara>After a successful migration, you must redirect network traffic of your stateless applications from the source cluster to the target cluster.</simpara>
<simpara>The strategies for redirecting network traffic are based on the following assumptions:</simpara>
<itemizedlist>
<listitem>
<simpara>The application pods are running on both the source and target clusters.</simpara>
</listitem>
<listitem>
<simpara>Each application has a route that contains the source cluster hostname.</simpara>
</listitem>
<listitem>
<simpara>The route with the source cluster hostname contains a CA certificate.</simpara>
</listitem>
<listitem>
<simpara>For HTTPS, the target router CA certificate contains a Subject Alternative Name for the wildcard DNS record of the source cluster.</simpara>
</listitem>
</itemizedlist>
<simpara>Consider the following strategies and select the one that meets your objectives.</simpara>
<itemizedlist>
<listitem>
<simpara>Redirecting all network traffic for all applications at the same time</simpara>
<simpara>Change the wildcard DNS record of the source cluster to point to the target cluster router&#8217;s virtual IP address (VIP).</simpara>
<simpara>This strategy is suitable for simple applications or small migrations.</simpara>
</listitem>
<listitem>
<simpara>Redirecting network traffic for individual applications</simpara>
<simpara>Create a DNS record for each application with the source cluster hostname pointing to the target cluster router&#8217;s VIP. This DNS record takes precedence over the source cluster wildcard DNS record.</simpara>
</listitem>
<listitem>
<simpara>Redirecting network traffic gradually for individual applications</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Create a proxy that can direct traffic to both the source cluster router&#8217;s VIP and the target cluster router&#8217;s VIP, for each application.</simpara>
</listitem>
<listitem>
<simpara>Create a DNS record for each application with the source cluster hostname pointing to the proxy.</simpara>
</listitem>
<listitem>
<simpara>Configure the proxy entry for the application to route a percentage of the traffic to the target cluster router&#8217;s VIP and the rest of the traffic to the source cluster router&#8217;s VIP.</simpara>
</listitem>
<listitem>
<simpara>Gradually increase the percentage of traffic that you route to the target cluster router&#8217;s VIP until all the network traffic is redirected.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>User-based redirection of traffic for individual applications</simpara>
<simpara>Using this strategy, you can filter TCP/IP headers of user requests to redirect network traffic for predefined groups of users. This allows you to test the redirection process on specific populations of users before redirecting the entire network traffic.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Create a proxy that can direct traffic to both the source cluster router&#8217;s VIP and the target cluster router&#8217;s VIP, for each application.</simpara>
</listitem>
<listitem>
<simpara>Create a DNS record for each application with the source cluster hostname pointing to the proxy.</simpara>
</listitem>
<listitem>
<simpara>Configure the proxy entry for the application to route traffic matching a given header pattern, such as <literal>test customers</literal>, to the target cluster router&#8217;s VIP and the rest of the traffic to the source cluster router&#8217;s VIP.</simpara>
</listitem>
<listitem>
<simpara>Redirect traffic to the target cluster router&#8217;s VIP in stages until all the traffic is on the target cluster router&#8217;s VIP.</simpara>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="about-mtc-3-4">
<title>About the Migration Toolkit for Containers</title>
<simpara>The Migration Toolkit for Containers (MTC) enables you to migrate stateful application workloads from OpenShift Container Platform 3 to 4.14 at the granularity of a namespace.</simpara>
<important>
<simpara>Before you begin your migration, be sure to review the <link linkend="planning-migration-3-4">differences between OpenShift Container Platform 3 and 4</link>.</simpara>
</important>
<simpara>MTC provides a web console and an API, based on Kubernetes custom resources, to help you control the migration and minimize application downtime.</simpara>
<simpara>The MTC console is installed on the target cluster by default. You can configure the Migration Toolkit for Containers Operator to install the console on an <link xlink:href="https://access.redhat.com/articles/5064151">OpenShift Container Platform 3 source cluster or on a remote cluster</link>.</simpara>
<simpara>MTC supports the file system and snapshot data copy methods for migrating data from the source cluster to the target cluster. You can select a method that is suited for your environment and is supported by your storage provider.</simpara>
<simpara>The service catalog is deprecated in OpenShift Container Platform 4. You can migrate workload resources provisioned with the service catalog from OpenShift Container Platform 3 to 4 but you cannot perform service catalog actions such as <literal>provision</literal>, <literal>deprovision</literal>, or <literal>update</literal> on these workloads after migration. The MTC console displays a message if the service catalog resources cannot be migrated.</simpara>
<section xml:id="migration-terminology_about-mtc-3-4">
<title>Terminology</title>
<table frame="all" rowsep="1" colsep="1">
<title>MTC terminology</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Term</entry>
<entry align="left" valign="top">Definition</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Source cluster</simpara></entry>
<entry align="left" valign="top"><simpara>Cluster from which the applications are migrated.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Destination cluster<superscript>[1]</superscript></simpara></entry>
<entry align="left" valign="top"><simpara>Cluster to which the applications are migrated.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Replication repository</simpara></entry>
<entry align="left" valign="top"><simpara>Object storage used for copying images, volumes, and Kubernetes objects during indirect migration or for Kubernetes objects during direct volume migration or direct image migration.</simpara>
<simpara>The replication repository must be accessible to all clusters.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Host cluster</simpara></entry>
<entry align="left" valign="top"><simpara>Cluster on which the <literal>migration-controller</literal> pod and the web console are running. The host cluster is usually the destination cluster but this is not required.</simpara>
<simpara>The host cluster does not require an exposed registry route for direct image migration.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Remote cluster</simpara></entry>
<entry align="left" valign="top"><simpara>A remote cluster is usually the source cluster but this is not required.</simpara>
<simpara>A remote cluster requires a <literal>Secret</literal> custom resource that contains the <literal>migration-controller</literal> service account token.</simpara>
<simpara>A remote cluster requires an exposed secure registry route for direct image migration.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Indirect migration</simpara></entry>
<entry align="left" valign="top"><simpara>Images, volumes, and Kubernetes objects are copied from the source cluster to the replication repository and then from the replication repository to the destination cluster.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Direct volume migration</simpara></entry>
<entry align="left" valign="top"><simpara>Persistent volumes are copied directly from the source cluster to the destination cluster.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Direct image migration</simpara></entry>
<entry align="left" valign="top"><simpara>Images are copied directly from the source cluster to the destination cluster.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Stage migration</simpara></entry>
<entry align="left" valign="top"><simpara>Data is copied to the destination cluster without stopping the application.</simpara>
<simpara>Running a stage migration multiple times reduces the duration of the cutover migration.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Cutover migration</simpara></entry>
<entry align="left" valign="top"><simpara>The application is stopped on the source cluster and its resources are migrated to the destination cluster.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>State migration</simpara></entry>
<entry align="left" valign="top"><simpara>Application state is migrated by copying specific persistent volume claims to the destination cluster.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Rollback migration</simpara></entry>
<entry align="left" valign="top"><simpara>Rollback migration rolls back a completed migration.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara><superscript>1</superscript>  Called the <emphasis>target</emphasis> cluster in the MTC web console.</simpara>
</section>
<section xml:id="migration-mtc-workflow_about-mtc-3-4">
<title>MTC workflow</title>
<simpara>You can migrate Kubernetes resources, persistent volume data, and internal container images to OpenShift Container Platform 4.14 by using the Migration Toolkit for Containers (MTC) web console or the Kubernetes API.</simpara>
<simpara>MTC migrates the following resources:</simpara>
<itemizedlist>
<listitem>
<simpara>A namespace specified in a migration plan.</simpara>
</listitem>
<listitem>
<simpara>Namespace-scoped resources: When the MTC migrates a namespace, it migrates all the objects and resources associated with that namespace, such as services or pods. Additionally, if a resource that exists in the namespace but not at the cluster level depends on a resource that exists at the cluster level, the MTC migrates both resources.</simpara>
<simpara>For example, a security context constraint (SCC) is a resource that exists at the cluster level and a service account (SA) is a resource that exists at the namespace level. If an SA exists in a namespace that the MTC migrates, the MTC automatically locates any SCCs that are linked to the SA and also migrates those SCCs. Similarly, the MTC migrates persistent volumes that are linked to the persistent volume claims of the namespace.</simpara>
<note>
<simpara>Cluster-scoped resources might have to be migrated manually, depending on the resource.</simpara>
</note>
</listitem>
<listitem>
<simpara>Custom resources (CRs) and custom resource definitions (CRDs): MTC automatically migrates CRs and CRDs at the namespace level.</simpara>
</listitem>
</itemizedlist>
<simpara>Migrating an application with the MTC web console involves the following steps:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Install the Migration Toolkit for Containers Operator on all clusters.</simpara>
<simpara>You can install the Migration Toolkit for Containers Operator in a restricted environment with limited or no internet access. The source and target clusters must have network access to each other and to a mirror registry.</simpara>
</listitem>
<listitem>
<simpara>Configure the replication repository, an intermediate object storage that MTC uses to migrate data.</simpara>
<simpara>The source and target clusters must have network access to the replication repository during migration. If you are using a proxy server, you must configure it to allow network traffic between the replication repository and the clusters.</simpara>
</listitem>
<listitem>
<simpara>Add the source cluster to the MTC web console.</simpara>
</listitem>
<listitem>
<simpara>Add the replication repository to the MTC web console.</simpara>
</listitem>
<listitem>
<simpara>Create a migration plan, with one of the following data migration options:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Copy</emphasis>: MTC copies the data from the source cluster to the replication repository, and from the replication repository to the target cluster.</simpara>
<note>
<simpara>If you are using direct image migration or direct volume migration, the images or volumes are copied directly from the source cluster to the target cluster.</simpara>
</note>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/migration-PV-copy.png"/>
</imageobject>
<textobject><phrase>migration PV copy</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<simpara><emphasis role="strong">Move</emphasis>: MTC unmounts a remote volume, for example, NFS, from the source cluster, creates a PV resource on the target cluster pointing to the remote volume, and then mounts the remote volume on the target cluster. Applications running on the target cluster use the same remote volume that the source cluster was using. The remote volume must be accessible to the source and target clusters.</simpara>
<note>
<simpara>Although the replication repository does not appear in this diagram, it is required for migration.</simpara>
</note>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/migration-PV-move.png"/>
</imageobject>
<textobject><phrase>migration PV move</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Run the migration plan, with one of the following options:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Stage</emphasis> copies data to the target cluster without stopping the application.</simpara>
<simpara>A stage migration can be run multiple times so that most of the data is copied to the target before migration. Running one or more stage migrations reduces the duration of the cutover migration.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Cutover</emphasis> stops the application on the source cluster and moves the resources to the target cluster.</simpara>
<simpara>Optional: You can clear the <emphasis role="strong">Halt transactions on the source cluster during migration</emphasis> checkbox.</simpara>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/OCP_3_to_4_App_migration.png"/>
</imageobject>
<textobject><phrase>OCP 3 to 4 App migration</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="migration-understanding-data-copy-methods_about-mtc-3-4">
<title>About data copy methods</title>
<simpara>The Migration Toolkit for Containers (MTC) supports the file system and snapshot data copy methods for migrating data from the source cluster to the target cluster. You can select a method that is suited for your environment and is supported by your storage provider.</simpara>
<section xml:id="file-system-copy-method_about-mtc-3-4">
<title>File system copy method</title>
<simpara>MTC copies data files from the source cluster to the replication repository, and from there to the target cluster.</simpara>
<simpara>The file system copy method uses Restic for indirect migration or Rsync for direct volume migration.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>File system copy method summary</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Benefits</entry>
<entry align="left" valign="top">Limitations</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><itemizedlist>
<listitem>
<simpara>Clusters can have different storage classes.</simpara>
</listitem>
<listitem>
<simpara>Supported for all S3 storage providers.</simpara>
</listitem>
<listitem>
<simpara>Optional data verification with checksum.</simpara>
</listitem>
<listitem>
<simpara>Supports direct volume migration, which significantly increases performance.</simpara>
</listitem>
</itemizedlist></entry>
<entry align="left" valign="top"><itemizedlist>
<listitem>
<simpara>Slower than the snapshot copy method.</simpara>
</listitem>
<listitem>
<simpara>Optional data verification significantly reduces performance.</simpara>
</listitem>
</itemizedlist></entry>
</row>
</tbody>
</tgroup>
</table>
<note>
<simpara>The Restic and Rsync PV migration assumes that the PVs supported are only <literal>volumeMode=filesystem</literal>. Using <literal>volumeMode=Block</literal> for file system migration is <emphasis>not</emphasis>
supported.</simpara>
</note>
</section>
<section xml:id="snapshot-copy-method_about-mtc-3-4">
<title>Snapshot copy method</title>
<simpara>MTC copies a snapshot of the source cluster data to the replication repository of a cloud provider. The data is restored on the target cluster.</simpara>
<simpara>The snapshot copy method can be used with Amazon Web Services, Google Cloud Provider, and Microsoft Azure.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Snapshot copy method summary</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Benefits</entry>
<entry align="left" valign="top">Limitations</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><itemizedlist>
<listitem>
<simpara>Faster than the file system copy method.</simpara>
</listitem>
</itemizedlist></entry>
<entry align="left" valign="top"><itemizedlist>
<listitem>
<simpara>Cloud provider must support snapshots.</simpara>
</listitem>
<listitem>
<simpara>Clusters must be on the same cloud provider.</simpara>
</listitem>
<listitem>
<simpara>Clusters must be in the same location or region.</simpara>
</listitem>
<listitem>
<simpara>Clusters must have the same storage class.</simpara>
</listitem>
<listitem>
<simpara>Storage class must be compatible with snapshots.</simpara>
</listitem>
<listitem>
<simpara>Does not support direct volume migration.</simpara>
</listitem>
</itemizedlist></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
</section>
<section xml:id="migration-direct-volume-migration-and-direct-image-migration_about-mtc-3-4">
<title>Direct volume migration and direct image migration</title>
<simpara>You can use direct image migration (DIM) and direct volume migration (DVM) to migrate images and data directly from the source cluster to the target cluster.</simpara>
<simpara>If you run DVM with nodes that are in different availability zones, the migration might fail because the migrated pods cannot access the persistent volume claim.</simpara>
<simpara>DIM and DVM have significant performance benefits because the intermediate steps of backing up files from the source cluster to the replication repository and restoring files from the replication repository to the target cluster are skipped. The data is transferred with <link xlink:href="https://rsync.samba.org/">Rsync</link>.</simpara>
<simpara>DIM and DVM have additional prerequisites.</simpara>
</section>
</chapter>
<chapter xml:id="installing-3-4">
<title>Installing the Migration Toolkit for Containers</title>
<simpara>You can install the Migration Toolkit for Containers (MTC) on OpenShift Container Platform 3 and 4.</simpara>
<simpara>After you install the Migration Toolkit for Containers Operator on OpenShift Container Platform 4.14 by using the Operator Lifecycle Manager, you manually install the legacy Migration Toolkit for Containers Operator on OpenShift Container Platform 3.</simpara>
<simpara>By default, the MTC web console and the <literal>Migration Controller</literal> pod run on the target cluster. You can configure the <literal>Migration Controller</literal> custom resource manifest to run the MTC web console and the <literal>Migration Controller</literal> pod on a <link xlink:href="https://access.redhat.com/articles/5064151">source cluster or on a remote cluster</link>.</simpara>
<simpara>After you have installed MTC, you must configure an object storage to use as a replication repository.</simpara>
<simpara>To uninstall MTC, see <link linkend="migration-uninstalling-mtc-clean-up_installing-3-4">Uninstalling MTC and deleting resources</link>.</simpara>
<section xml:id="migration-compatibility-guidelines_installing-3-4">
<title>Compatibility guidelines</title>
<simpara>You must install the Migration Toolkit for Containers (MTC) Operator that is compatible with your OpenShift Container Platform version.</simpara>
<variablelist>
<title>Definitions</title>
<varlistentry>
<term>legacy platform</term>
<listitem>
<simpara>OpenShift Container Platform 4.5 and earlier.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>modern platform</term>
<listitem>
<simpara>OpenShift Container Platform 4.6 and later.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>legacy operator</term>
<listitem>
<simpara>The MTC Operator designed for legacy platforms.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>modern operator</term>
<listitem>
<simpara>The MTC Operator designed for modern platforms.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>control cluster</term>
<listitem>
<simpara>The cluster that runs the MTC controller and GUI.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>remote cluster</term>
<listitem>
<simpara>A source or destination cluster for a migration that runs Velero. The Control Cluster communicates with Remote clusters via the Velero API to drive migrations.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>You must use the compatible MTC version for migrating your OpenShift Container Platform clusters. For the migration to succeed both your source cluster and the destination cluster must use the same version of MTC.</simpara>
<simpara>MTC 1.7 supports migrations from OpenShift Container Platform 3.11 to 4.8.</simpara>
<simpara>MTC 1.8 only supports migrations from OpenShift Container Platform 4.9 and later.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>MTC compatibility: Migrating from a legacy or a modern platform</title>
<tgroup cols="5">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="20*"/>
<colspec colname="col_4" colwidth="20*"/>
<colspec colname="col_5" colwidth="20*"/>
<thead>
<row>
<entry align="left" valign="top">Details</entry>
<entry align="left" valign="top">OpenShift Container Platform 3.11</entry>
<entry align="left" valign="top">OpenShift Container Platform 4.0 to 4.5</entry>
<entry align="left" valign="top">OpenShift Container Platform 4.6 to 4.8</entry>
<entry align="left" valign="top">OpenShift Container Platform 4.9 or later</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Stable MTC version</simpara></entry>
<entry align="left" valign="top"><simpara>MTC v.1.7.<emphasis>z</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>MTC v.1.7.<emphasis>z</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>MTC v.1.7.<emphasis>z</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>MTC v.1.8.<emphasis>z</emphasis></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Installation</simpara></entry>
<entry align="left" valign="top"></entry>
<entry align="left" valign="top"><simpara>Legacy MTC v.1.7.<emphasis>z</emphasis> operator: Install manually with the <literal>operator.yml</literal> file.</simpara><simpara>[<emphasis role="strong">IMPORTANT</emphasis>]
This cluster cannot be the control cluster.</simpara></entry>
<entry align="left" valign="top"><simpara>Install with OLM, release channel <literal>release-v1.7</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Install with OLM, release channel <literal>release-v1.8</literal></simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara>Edge cases exist in which network restrictions prevent modern clusters from connecting to other clusters involved in the migration. For example, when migrating from an OpenShift Container Platform 3.11 cluster on premises to a modern OpenShift Container Platform cluster in the cloud, where the modern cluster cannot connect to the OpenShift Container Platform 3.11 cluster.</simpara>
<simpara>With MTC v.1.7.<emphasis>z</emphasis>, if one of the remote clusters is unable to communicate with the control cluster because of network restrictions, use the <literal>crane tunnel-api</literal> command.</simpara>
<simpara>With the stable MTC release, although you should always designate the most modern cluster as the control cluster, in this specific case it is possible to designate the legacy cluster as the control cluster and push workloads to the remote cluster.</simpara>
</section>
<section xml:id="migration-installing-legacy-operator_installing-3-4">
<title>Installing the legacy Migration Toolkit for Containers Operator on OpenShift Container Platform 3</title>
<simpara>You can install the legacy Migration Toolkit for Containers Operator manually on OpenShift Container Platform 3.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must be logged in as a user with <literal>cluster-admin</literal> privileges on all clusters.</simpara>
</listitem>
<listitem>
<simpara>You must have access to <literal>registry.redhat.io</literal>.</simpara>
</listitem>
<listitem>
<simpara>You must have <literal>podman</literal> installed.</simpara>
</listitem>
<listitem>
<simpara>You must create an <link xlink:href="https://access.redhat.com/solutions/3772061">image stream secret</link> and copy it to each node in the cluster.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Log in to <literal>registry.redhat.io</literal> with your Red Hat Customer Portal credentials:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ podman login registry.redhat.io</programlisting>
</listitem>
<listitem>
<simpara>Download the <literal>operator.yml</literal> file by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">podman cp $(podman create registry.redhat.io/rhmtc/openshift-migration-legacy-rhel8-operator:v1.7):/operator.yml ./</programlisting>
</listitem>
<listitem>
<simpara>Download the <literal>controller.yml</literal> file by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">podman cp $(podman create registry.redhat.io/rhmtc/openshift-migration-legacy-rhel8-operator:v1.7):/controller.yml ./</programlisting>
</listitem>
<listitem>
<simpara>Log in to your OpenShift Container Platform source cluster.</simpara>
</listitem>
<listitem>
<simpara>Verify that the cluster can authenticate with <literal>registry.redhat.io</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc run test --image registry.redhat.io/ubi9 --command sleep infinity</programlisting>
</listitem>
<listitem>
<simpara>Create the Migration Toolkit for Containers Operator object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f operator.yml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">namespace/openshift-migration created
rolebinding.rbac.authorization.k8s.io/system:deployers created
serviceaccount/migration-operator created
customresourcedefinition.apiextensions.k8s.io/migrationcontrollers.migration.openshift.io created
role.rbac.authorization.k8s.io/migration-operator created
rolebinding.rbac.authorization.k8s.io/migration-operator created
clusterrolebinding.rbac.authorization.k8s.io/migration-operator created
deployment.apps/migration-operator created
Error from server (AlreadyExists): error when creating "./operator.yml":
rolebindings.rbac.authorization.k8s.io "system:image-builders" already exists <co xml:id="CO1-1"/>
Error from server (AlreadyExists): error when creating "./operator.yml":
rolebindings.rbac.authorization.k8s.io "system:image-pullers" already exists</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO1-1">
<para>You can ignore <literal>Error from server (AlreadyExists)</literal> messages. They are caused by the Migration Toolkit for Containers Operator creating resources for earlier versions of OpenShift Container Platform 4 that are provided in later releases.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the <literal>MigrationController</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f controller.yml</programlisting>
</listitem>
<listitem>
<simpara>Verify that the MTC pods are running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -n openshift-migration</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="migration-installing-mtc-on-ocp-4_installing-3-4">
<title>Installing the Migration Toolkit for Containers Operator on OpenShift Container Platform 4.14</title>
<simpara>You install the Migration Toolkit for Containers Operator on OpenShift Container Platform 4.14 by using the Operator Lifecycle Manager.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must be logged in as a user with <literal>cluster-admin</literal> privileges on all clusters.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the OpenShift Container Platform web console, click <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">OperatorHub</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Use the <emphasis role="strong">Filter by keyword</emphasis> field to find the <emphasis role="strong">Migration Toolkit for Containers Operator</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select the <emphasis role="strong">Migration Toolkit for Containers Operator</emphasis> and click <emphasis role="strong">Install</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Install</emphasis>.</simpara>
<simpara>On the <emphasis role="strong">Installed Operators</emphasis> page, the <emphasis role="strong">Migration Toolkit for Containers Operator</emphasis> appears in the <emphasis role="strong">openshift-migration</emphasis> project with the status <emphasis role="strong">Succeeded</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Migration Toolkit for Containers Operator</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Under <emphasis role="strong">Provided APIs</emphasis>, locate the <emphasis role="strong">Migration Controller</emphasis> tile, and click <emphasis role="strong">Create Instance</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Workloads</emphasis> &#8594; <emphasis role="strong">Pods</emphasis> to verify that the MTC pods are running.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="migration-about-configuring-proxies_installing-3-4">
<title>Proxy configuration</title>
<simpara>For OpenShift Container Platform 4.1 and earlier versions, you must configure proxies in the <literal>MigrationController</literal> custom resource (CR) manifest after you install the Migration Toolkit for Containers Operator because these versions do not support a cluster-wide <literal>proxy</literal> object.</simpara>
<simpara>For OpenShift Container Platform 4.2 to 4.14, the Migration Toolkit for Containers (MTC) inherits the cluster-wide proxy settings. You can change the proxy parameters if you want to override the cluster-wide proxy settings.</simpara>
<section xml:id="direct-volume-migration_installing-3-4">
<title>Direct volume migration</title>
<simpara>Direct Volume Migration (DVM) was introduced in MTC 1.4.2. DVM supports only one proxy. The source cluster cannot access the route of the target cluster if the target cluster is also behind a proxy.</simpara>
<simpara>If you want to perform a DVM from a source cluster behind a proxy, you must configure a TCP proxy that works at the transport layer and forwards the SSL connections transparently without decrypting and re-encrypting them with their own SSL certificates. A Stunnel proxy is an example of such a proxy.</simpara>
<section xml:id="tcp-proxy-setup-for-dvm_installing-3-4">
<title>TCP proxy setup for DVM</title>
<simpara>You can set up a direct connection between the source and the target cluster through a TCP proxy and configure the <literal>stunnel_tcp_proxy</literal> variable in the <literal>MigrationController</literal> CR to use the proxy:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: migration.openshift.io/v1alpha1
kind: MigrationController
metadata:
  name: migration-controller
  namespace: openshift-migration
spec:
  [...]
  stunnel_tcp_proxy: http://username:password@ip:port</programlisting>
<simpara>Direct volume migration (DVM) supports only basic authentication for the proxy. Moreover, DVM works only from behind proxies that can tunnel a TCP connection transparently. HTTP/HTTPS proxies in man-in-the-middle mode do not work. The existing cluster-wide proxies might not support this behavior. As a result, the proxy settings for DVM are intentionally kept different from the usual proxy configuration in MTC.</simpara>
</section>
<section xml:id="why-tcp-proxy-instead-of-an-http-https-proxy_installing-3-4">
<title>Why use a TCP proxy instead of an HTTP/HTTPS proxy?</title>
<simpara>You can enable DVM by running Rsync between the source and the target cluster over an OpenShift route.  Traffic is encrypted using Stunnel, a TCP proxy. The Stunnel running on the source cluster initiates a TLS connection with the target Stunnel and transfers data over an encrypted channel.</simpara>
<simpara>Cluster-wide HTTP/HTTPS proxies in OpenShift are usually configured in man-in-the-middle mode where they negotiate their own TLS session with the outside servers. However, this does not work with Stunnel. Stunnel requires that its TLS session be untouched by the proxy, essentially making the proxy a transparent tunnel which simply forwards the TCP connection as-is. Therefore, you must use a TCP proxy.</simpara>
</section>
<section xml:id="dvm-known-issues_installing-3-4">
<title>Known issue</title>
<formalpara>
<title>Migration fails with error <literal>Upgrade request required</literal></title>
<para>The migration Controller uses the SPDY protocol to execute commands within remote pods. If the remote cluster is behind a proxy or a firewall that does not support the SPDY protocol, the migration controller fails to execute remote commands. The migration fails with the error message <literal>Upgrade request required</literal>.
Workaround: Use a proxy that supports the SPDY protocol.</para>
</formalpara>
<simpara>In addition to supporting the SPDY protocol, the proxy or firewall also must pass the <literal>Upgrade</literal> HTTP header to the API server. The client uses this header to open a websocket connection with the API server. If the <literal>Upgrade</literal> header is blocked by the proxy or firewall, the migration fails with the error message <literal>Upgrade request required</literal>.
Workaround: Ensure that the proxy forwards the <literal>Upgrade</literal> header.</simpara>
</section>
</section>
<section xml:id="tuning-network-policies-for-migrations_installing-3-4">
<title>Tuning network policies for migrations</title>
<simpara>OpenShift supports restricting traffic to or from pods using <emphasis>NetworkPolicy</emphasis> or <emphasis>EgressFirewalls</emphasis> based on the network plugin used by the cluster. If any of the source namespaces involved in a migration use such mechanisms to restrict network traffic to pods, the restrictions might inadvertently stop traffic to Rsync pods during migration.</simpara>
<simpara>Rsync pods running on both the source and the target clusters must connect to each other over an OpenShift Route. Existing <emphasis>NetworkPolicy</emphasis> or <emphasis>EgressNetworkPolicy</emphasis> objects can be configured to automatically exempt Rsync pods from these traffic restrictions.</simpara>
<section xml:id="dvm-network-policy-configuration_installing-3-4">
<title>NetworkPolicy configuration</title>
<section xml:id="egress-traffic-from-rsync-pods_installing-3-4">
<title>Egress traffic from Rsync pods</title>
<simpara>You can use the unique labels of Rsync pods to allow egress traffic to pass from them if the <literal>NetworkPolicy</literal> configuration in the source or destination namespaces blocks this type of traffic. The following policy allows <emphasis role="strong">all</emphasis> egress traffic from Rsync pods in the namespace:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-all-egress-from-rsync-pods
spec:
  podSelector:
    matchLabels:
      owner: directvolumemigration
      app: directvolumemigration-rsync-transfer
  egress:
  - {}
  policyTypes:
  - Egress</programlisting>
</section>
<section xml:id="ingress-traffic-to-rsync-pods_installing-3-4">
<title>Ingress traffic to Rsync pods</title>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-all-egress-from-rsync-pods
spec:
  podSelector:
    matchLabels:
      owner: directvolumemigration
      app: directvolumemigration-rsync-transfer
  ingress:
  - {}
  policyTypes:
  - Ingress</programlisting>
</section>
</section>
<section xml:id="egressnetworkpolicy-config_installing-3-4">
<title>EgressNetworkPolicy configuration</title>
<simpara>The <literal>EgressNetworkPolicy</literal> object or <emphasis>Egress Firewalls</emphasis> are OpenShift constructs designed to block egress traffic leaving the cluster.</simpara>
<simpara>Unlike the <literal>NetworkPolicy</literal> object, the Egress Firewall works at a project level because it applies to all pods in the namespace. Therefore, the unique labels of Rsync pods do not exempt only Rsync pods from the restrictions. However, you can add the CIDR ranges of the source or target cluster to the <emphasis>Allow</emphasis> rule of the policy so that a direct connection can be setup between two clusters.</simpara>
<simpara>Based on which cluster the Egress Firewall is present in, you can add the CIDR range of the other cluster to allow egress traffic between the two:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: network.openshift.io/v1
kind: EgressNetworkPolicy
metadata:
  name: test-egress-policy
  namespace: &lt;namespace&gt;
spec:
  egress:
  - to:
      cidrSelector: &lt;cidr_of_source_or_target_cluster&gt;
    type: Deny</programlisting>
</section>
<section xml:id="choosing-alternate-endpoints-for-data-transfer_installing-3-4">
<title>Choosing alternate endpoints for data transfer</title>
<simpara>By default, DVM uses an OpenShift Container Platform route as an endpoint to transfer PV data to destination clusters. You can choose another type of supported endpoint, if cluster topologies allow.</simpara>
<simpara>For each cluster, you can configure an endpoint by setting the <literal>rsync_endpoint_type</literal> variable on the appropriate <emphasis role="strong">destination</emphasis> cluster in your <literal>MigrationController</literal> CR:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: migration.openshift.io/v1alpha1
kind: MigrationController
metadata:
  name: migration-controller
  namespace: openshift-migration
spec:
  [...]
  rsync_endpoint_type: [NodePort|ClusterIP|Route]</programlisting>
</section>
<section xml:id="configuring-supplemental-groups-for-rsync-pods_installing-3-4">
<title>Configuring supplemental groups for Rsync pods</title>
<simpara>When your PVCs use a shared storage, you can configure the access to that storage by adding supplemental groups to Rsync pod definitions in order for the pods to allow access:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Supplementary groups for Rsync pods</title>
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<thead>
<row>
<entry align="left" valign="top">Variable</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Default</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>src_supplemental_groups</literal></simpara></entry>
<entry align="left" valign="top"><simpara>string</simpara></entry>
<entry align="left" valign="top"><simpara>Not set</simpara></entry>
<entry align="left" valign="top"><simpara>Comma-separated list of supplemental groups for source Rsync pods</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>target_supplemental_groups</literal></simpara></entry>
<entry align="left" valign="top"><simpara>string</simpara></entry>
<entry align="left" valign="top"><simpara>Not set</simpara></entry>
<entry align="left" valign="top"><simpara>Comma-separated list of supplemental groups for target Rsync pods</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<formalpara>
<title>Example usage</title>
<para>The <literal>MigrationController</literal> CR can be updated to set values for these supplemental groups:</para>
</formalpara>
<programlisting language="yaml" linenumbering="unnumbered">spec:
  src_supplemental_groups: "1000,2000"
  target_supplemental_groups: "2000,3000"</programlisting>
</section>
</section>
<section xml:id="migration-configuring-proxies_installing-3-4">
<title>Configuring proxies</title>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must be logged in as a user with <literal>cluster-admin</literal> privileges on all clusters.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Get the <literal>MigrationController</literal> CR manifest:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get migrationcontroller &lt;migration_controller&gt; -n openshift-migration</programlisting>
</listitem>
<listitem>
<simpara>Update the proxy parameters:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: migration.openshift.io/v1alpha1
kind: MigrationController
metadata:
  name: &lt;migration_controller&gt;
  namespace: openshift-migration
...
spec:
  stunnel_tcp_proxy: http://&lt;username&gt;:&lt;password&gt;@&lt;ip&gt;:&lt;port&gt; <co xml:id="CO2-1"/>
  noProxy: example.com <co xml:id="CO2-2"/></programlisting>
<calloutlist>
<callout arearefs="CO2-1">
<para>Stunnel proxy URL for direct volume migration.</para>
</callout>
<callout arearefs="CO2-2">
<para>Comma-separated list of destination domain names, domains, IP addresses, or other network CIDRs to exclude proxying.</para>
</callout>
</calloutlist>
<simpara>Preface a domain with <literal>.</literal> to match subdomains only. For example, <literal>.y.com</literal> matches <literal>x.y.com</literal>, but not <literal>y.com</literal>. Use <literal>*</literal> to bypass proxy for all destinations.
If you scale up workers that are not included in the network defined by the <literal>networking.machineNetwork[].cidr</literal> field from the installation configuration, you must add them to this list to prevent connection issues.</simpara>
<simpara>This field is ignored if neither the <literal>httpProxy</literal> nor the <literal>httpsProxy</literal> field is set.</simpara>
</listitem>
<listitem>
<simpara>Save the manifest as <literal>migration-controller.yaml</literal>.</simpara>
</listitem>
<listitem>
<simpara>Apply the updated manifest:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc replace -f migration-controller.yaml -n openshift-migration</programlisting>
</listitem>
</orderedlist>
<simpara>For more information, see <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/networking/#nw-proxy-configure-object_config-cluster-wide-proxy">Configuring the cluster-wide proxy</link>.</simpara>
</section>
</section>
<section xml:id="configuring-replication-repository_installing-3-4">
<title>Configuring a replication repository</title>
<simpara>You must configure an object storage to use as a replication repository. The Migration Toolkit for Containers (MTC) copies data from the source cluster to the replication repository, and then from the replication repository to the target cluster.</simpara>
<simpara>MTC supports the <link linkend="migration-understanding-data-copy-methods_about-mtc-3-4">file system and snapshot data copy methods</link> for migrating data from the source cluster to the target cluster. You can select a method that is suited for your environment and is supported by your storage provider.</simpara>
<simpara>The following storage providers are supported:</simpara>
<itemizedlist>
<listitem>
<simpara><link linkend="migration-configuring-mcg_installing-3-4">Multicloud Object Gateway</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="migration-configuring-aws-s3_installing-3-4">Amazon Web Services S3</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="migration-configuring-gcp_installing-3-4">Google Cloud Platform</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="migration-configuring-azure_installing-3-4">Microsoft Azure Blob</link></simpara>
</listitem>
<listitem>
<simpara>Generic S3 object storage, for example, Minio or Ceph S3</simpara>
</listitem>
</itemizedlist>
<section xml:id="replication-repository-prerequisites_installing-3-4">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>All clusters must have uninterrupted network access to the replication repository.</simpara>
</listitem>
<listitem>
<simpara>If you use a proxy server with an internally hosted replication repository, you must ensure that the proxy allows access to the replication repository.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="migration-configuring-mcg_installing-3-4">
<title>Retrieving Multicloud Object Gateway credentials</title>
<simpara>You must retrieve the Multicloud Object Gateway (MCG) credentials and S3 endpoint in order to configure MCG as a replication repository for the Migration Toolkit for Containers (MTC).
You must retrieve the Multicloud Object Gateway (MCG) credentials in order to create a <literal>Secret</literal> custom resource (CR) for the OpenShift API for Data Protection (OADP).</simpara>
<simpara>MCG is a component of OpenShift Data Foundation.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must deploy OpenShift Data Foundation by using the appropriate <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.9">OpenShift Data Foundation deployment guide</link>.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Obtain the S3 endpoint, <literal>AWS_ACCESS_KEY_ID</literal>, and <literal>AWS_SECRET_ACCESS_KEY</literal> by running the <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.9/html/managing_hybrid_and_multicloud_resources/accessing-the-multicloud-object-gateway-with-your-applications_rhodf#accessing-the-Multicloud-object-gateway-from-the-terminal_rhodf"><literal>describe</literal> command</link> on the <literal>NooBaa</literal> custom resource.</simpara>
<simpara>You use these credentials to add MCG as a replication repository.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="migration-configuring-aws-s3_installing-3-4">
<title>Configuring Amazon Web Services</title>
<simpara>You configure Amazon Web Services (AWS) S3 object storage as a replication repository for the Migration Toolkit for Containers (MTC).</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have the <link xlink:href="https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-welcome.html">AWS CLI</link> installed.</simpara>
</listitem>
<listitem>
<simpara>The AWS S3 storage bucket must be accessible to the source and target clusters.</simpara>
</listitem>
<listitem>
<simpara>If you are using the snapshot copy method:</simpara>
<itemizedlist>
<listitem>
<simpara>You must have access to EC2 Elastic Block Storage (EBS).</simpara>
</listitem>
<listitem>
<simpara>The source and target clusters must be in the same region.</simpara>
</listitem>
<listitem>
<simpara>The source and target clusters must have the same storage class.</simpara>
</listitem>
<listitem>
<simpara>The storage class must be compatible with snapshots.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Set the <literal>BUCKET</literal> variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ BUCKET=&lt;your_bucket&gt;</programlisting>
</listitem>
<listitem>
<simpara>Set the <literal>REGION</literal> variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ REGION=&lt;your_region&gt;</programlisting>
</listitem>
<listitem>
<simpara>Create an AWS S3 bucket:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ aws s3api create-bucket \
    --bucket $BUCKET \
    --region $REGION \
    --create-bucket-configuration LocationConstraint=$REGION <co xml:id="CO3-1"/></programlisting>
<calloutlist>
<callout arearefs="CO3-1">
<para><literal>us-east-1</literal> does not support a <literal>LocationConstraint</literal>. If your region is <literal>us-east-1</literal>, omit <literal>--create-bucket-configuration LocationConstraint=$REGION</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create an IAM user:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ aws iam create-user --user-name velero <co xml:id="CO4-1"/></programlisting>
<calloutlist>
<callout arearefs="CO4-1">
<para>If you want to use Velero to back up multiple clusters with multiple S3 buckets, create a unique user name for each cluster.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create a <literal>velero-policy.json</literal> file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &gt; velero-policy.json &lt;&lt;EOF
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "ec2:DescribeVolumes",
                "ec2:DescribeSnapshots",
                "ec2:CreateTags",
                "ec2:CreateVolume",
                "ec2:CreateSnapshot",
                "ec2:DeleteSnapshot"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "s3:GetObject",
                "s3:DeleteObject",
                "s3:PutObject",
                "s3:AbortMultipartUpload",
                "s3:ListMultipartUploadParts"
            ],
            "Resource": [
                "arn:aws:s3:::${BUCKET}/*"
            ]
        },
        {
            "Effect": "Allow",
            "Action": [
                "s3:ListBucket",
                "s3:GetBucketLocation",
                "s3:ListBucketMultipartUploads"
            ],
            "Resource": [
                "arn:aws:s3:::${BUCKET}"
            ]
        }
    ]
}
EOF</programlisting>
</listitem>
<listitem>
<simpara>Attach the policies to give the <literal>velero</literal> user the minimum necessary permissions:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ aws iam put-user-policy \
  --user-name velero \
  --policy-name velero \
  --policy-document file://velero-policy.json</programlisting>
</listitem>
<listitem>
<simpara>Create an access key for the <literal>velero</literal> user:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ aws iam create-access-key --user-name velero</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">{
  "AccessKey": {
        "UserName": "velero",
        "Status": "Active",
        "CreateDate": "2017-07-31T22:24:41.576Z",
        "SecretAccessKey": &lt;AWS_SECRET_ACCESS_KEY&gt;,
        "AccessKeyId": &lt;AWS_ACCESS_KEY_ID&gt;
  }
}</programlisting>
</para>
</formalpara>
<simpara>Record the <literal>AWS_SECRET_ACCESS_KEY</literal> and the <literal>AWS_ACCESS_KEY_ID</literal>. You use the credentials to add AWS as a replication repository.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="migration-configuring-gcp_installing-3-4">
<title>Configuring Google Cloud Platform</title>
<simpara>You configure a Google Cloud Platform (GCP) storage bucket as a replication repository for the Migration Toolkit for Containers (MTC).</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have the <literal>gcloud</literal> and <literal>gsutil</literal> CLI tools installed. See the <link xlink:href="https://cloud.google.com/sdk/docs/">Google cloud documentation</link> for details.</simpara>
</listitem>
<listitem>
<simpara>The GCP storage bucket must be accessible to the source and target clusters.</simpara>
</listitem>
<listitem>
<simpara>If you are using the snapshot copy method:</simpara>
<itemizedlist>
<listitem>
<simpara>The source and target clusters must be in the same region.</simpara>
</listitem>
<listitem>
<simpara>The source and target clusters must have the same storage class.</simpara>
</listitem>
<listitem>
<simpara>The storage class must be compatible with snapshots.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Log in to GCP:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ gcloud auth login</programlisting>
</listitem>
<listitem>
<simpara>Set the <literal>BUCKET</literal> variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ BUCKET=&lt;bucket&gt; <co xml:id="CO5-1"/></programlisting>
<calloutlist>
<callout arearefs="CO5-1">
<para>Specify your bucket name.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the storage bucket:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ gsutil mb gs://$BUCKET/</programlisting>
</listitem>
<listitem>
<simpara>Set the <literal>PROJECT_ID</literal> variable to your active project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ PROJECT_ID=$(gcloud config get-value project)</programlisting>
</listitem>
<listitem>
<simpara>Create a service account:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ gcloud iam service-accounts create velero \
    --display-name "Velero service account"</programlisting>
</listitem>
<listitem>
<simpara>List your service accounts:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ gcloud iam service-accounts list</programlisting>
</listitem>
<listitem>
<simpara>Set the <literal>SERVICE_ACCOUNT_EMAIL</literal> variable to match its <literal>email</literal> value:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ SERVICE_ACCOUNT_EMAIL=$(gcloud iam service-accounts list \
    --filter="displayName:Velero service account" \
    --format 'value(email)')</programlisting>
</listitem>
<listitem>
<simpara>Attach the policies to give the <literal>velero</literal> user the minimum necessary permissions:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ROLE_PERMISSIONS=(
    compute.disks.get
    compute.disks.create
    compute.disks.createSnapshot
    compute.snapshots.get
    compute.snapshots.create
    compute.snapshots.useReadOnly
    compute.snapshots.delete
    compute.zones.get
    storage.objects.create
    storage.objects.delete
    storage.objects.get
    storage.objects.list
    iam.serviceAccounts.signBlob
)</programlisting>
</listitem>
<listitem>
<simpara>Create the <literal>velero.server</literal> custom role:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ gcloud iam roles create velero.server \
    --project $PROJECT_ID \
    --title "Velero Server" \
    --permissions "$(IFS=","; echo "${ROLE_PERMISSIONS[*]}")"</programlisting>
</listitem>
<listitem>
<simpara>Add IAM policy binding to the project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ gcloud projects add-iam-policy-binding $PROJECT_ID \
    --member serviceAccount:$SERVICE_ACCOUNT_EMAIL \
    --role projects/$PROJECT_ID/roles/velero.server</programlisting>
</listitem>
<listitem>
<simpara>Update the IAM service account:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ gsutil iam ch serviceAccount:$SERVICE_ACCOUNT_EMAIL:objectAdmin gs://${BUCKET}</programlisting>
</listitem>
<listitem>
<simpara>Save the IAM service account keys to the <literal>credentials-velero</literal> file in the current directory:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ gcloud iam service-accounts keys create credentials-velero \
    --iam-account $SERVICE_ACCOUNT_EMAIL</programlisting>
<simpara>You use the <literal>credentials-velero</literal> file to add GCP as a replication repository.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="migration-configuring-azure_installing-3-4">
<title>Configuring Microsoft Azure</title>
<simpara>You configure a Microsoft Azure Blob storage container as a replication repository for the Migration Toolkit for Containers (MTC).</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have the <link xlink:href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli">Azure CLI</link> installed.</simpara>
</listitem>
<listitem>
<simpara>The Azure Blob storage container must be accessible to the source and target clusters.</simpara>
</listitem>
<listitem>
<simpara>If you are using the snapshot copy method:</simpara>
<itemizedlist>
<listitem>
<simpara>The source and target clusters must be in the same region.</simpara>
</listitem>
<listitem>
<simpara>The source and target clusters must have the same storage class.</simpara>
</listitem>
<listitem>
<simpara>The storage class must be compatible with snapshots.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Log in to Azure:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ az login</programlisting>
</listitem>
<listitem>
<simpara>Set the <literal>AZURE_RESOURCE_GROUP</literal> variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ AZURE_RESOURCE_GROUP=Velero_Backups</programlisting>
</listitem>
<listitem>
<simpara>Create an Azure resource group:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ az group create -n $AZURE_RESOURCE_GROUP --location CentralUS <co xml:id="CO6-1"/></programlisting>
<calloutlist>
<callout arearefs="CO6-1">
<para>Specify your location.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Set the <literal>AZURE_STORAGE_ACCOUNT_ID</literal> variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ AZURE_STORAGE_ACCOUNT_ID="velero$(uuidgen | cut -d '-' -f5 | tr '[A-Z]' '[a-z]')"</programlisting>
</listitem>
<listitem>
<simpara>Create an Azure storage account:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ az storage account create \
    --name $AZURE_STORAGE_ACCOUNT_ID \
    --resource-group $AZURE_RESOURCE_GROUP \
    --sku Standard_GRS \
    --encryption-services blob \
    --https-only true \
    --kind BlobStorage \
    --access-tier Hot</programlisting>
</listitem>
<listitem>
<simpara>Set the <literal>BLOB_CONTAINER</literal> variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ BLOB_CONTAINER=velero</programlisting>
</listitem>
<listitem>
<simpara>Create an Azure Blob storage container:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ az storage container create \
  -n $BLOB_CONTAINER \
  --public-access off \
  --account-name $AZURE_STORAGE_ACCOUNT_ID</programlisting>
</listitem>
<listitem>
<simpara>Create a service principal and credentials for <literal>velero</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ AZURE_SUBSCRIPTION_ID=`az account list --query '[?isDefault].id' -o tsv` \
  AZURE_TENANT_ID=`az account list --query '[?isDefault].tenantId' -o tsv` \
  AZURE_CLIENT_SECRET=`az ad sp create-for-rbac --name "velero" \
  --role "Contributor" --query 'password' -o tsv` \
  AZURE_CLIENT_ID=`az ad sp list --display-name "velero" \
  --query '[0].appId' -o tsv`</programlisting>
</listitem>
<listitem>
<simpara>Save the service principal credentials in the <literal>credentials-velero</literal> file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt; EOF &gt; ./credentials-velero
AZURE_SUBSCRIPTION_ID=${AZURE_SUBSCRIPTION_ID}
AZURE_TENANT_ID=${AZURE_TENANT_ID}
AZURE_CLIENT_ID=${AZURE_CLIENT_ID}
AZURE_CLIENT_SECRET=${AZURE_CLIENT_SECRET}
AZURE_RESOURCE_GROUP=${AZURE_RESOURCE_GROUP}
AZURE_CLOUD_NAME=AzurePublicCloud
EOF</programlisting>
<simpara>You use the <literal>credentials-velero</literal> file to add Azure as a replication repository.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="installing-3-4_configuring-replication-repository-additional-resources" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="migration-mtc-workflow_about-mtc-3-4">MTC workflow</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="migration-understanding-data-copy-methods_about-mtc-3-4">About data copy methods</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="migration-adding-replication-repository-to-cam_migrating-applications-3-4">Adding a replication repository to the MTC web console</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="migration-uninstalling-mtc-clean-up_installing-3-4">
<title>Uninstalling MTC and deleting resources</title>
<simpara>You can uninstall the Migration Toolkit for Containers (MTC) and delete its resources to clean up the cluster.</simpara>
<note>
<simpara>Deleting the <literal>velero</literal> CRDs removes Velero from the cluster.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must be logged in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Delete the <literal>MigrationController</literal> custom resource (CR) on all clusters:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete migrationcontroller &lt;migration_controller&gt;</programlisting>
</listitem>
<listitem>
<simpara>Uninstall the Migration Toolkit for Containers Operator on OpenShift Container Platform 4 by using the Operator Lifecycle Manager.</simpara>
</listitem>
<listitem>
<simpara>Delete cluster-scoped resources on all clusters by running the following commands:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>migration</literal> custom resource definitions (CRDs):</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete $(oc get crds -o name | grep 'migration.openshift.io')</programlisting>
</listitem>
<listitem>
<simpara><literal>velero</literal> CRDs:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete $(oc get crds -o name | grep 'velero')</programlisting>
</listitem>
<listitem>
<simpara><literal>migration</literal> cluster roles:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete $(oc get clusterroles -o name | grep 'migration.openshift.io')</programlisting>
</listitem>
<listitem>
<simpara><literal>migration-operator</literal> cluster role:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete clusterrole migration-operator</programlisting>
</listitem>
<listitem>
<simpara><literal>velero</literal> cluster roles:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete $(oc get clusterroles -o name | grep 'velero')</programlisting>
</listitem>
<listitem>
<simpara><literal>migration</literal> cluster role bindings:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete $(oc get clusterrolebindings -o name | grep 'migration.openshift.io')</programlisting>
</listitem>
<listitem>
<simpara><literal>migration-operator</literal> cluster role bindings:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete clusterrolebindings migration-operator</programlisting>
</listitem>
<listitem>
<simpara><literal>velero</literal> cluster role bindings:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete $(oc get clusterrolebindings -o name | grep 'velero')</programlisting>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</section>
</chapter>
<chapter xml:id="installing-restricted-3-4">
<title>Installing the Migration Toolkit for Containers in a restricted network environment</title>
<simpara>You can install the Migration Toolkit for Containers (MTC) on OpenShift Container Platform 3 and 4 in a restricted network environment by performing the following procedures:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Create a <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/operators/#olm-mirror-catalog_olm-restricted-networks">mirrored Operator catalog</link>.</simpara>
<simpara>This process creates a <literal>mapping.txt</literal> file, which contains the mapping between the <literal>registry.redhat.io</literal> image and your mirror registry image. The <literal>mapping.txt</literal> file is required for installing the Operator on the source cluster.</simpara>
</listitem>
<listitem>
<simpara>Install the Migration Toolkit for Containers Operator on the OpenShift Container Platform 4.14 target cluster by using Operator Lifecycle Manager.</simpara>
<simpara>By default, the MTC web console and the <literal>Migration Controller</literal> pod run on the target cluster. You can configure the <literal>Migration Controller</literal> custom resource manifest to run the MTC web console and the <literal>Migration Controller</literal> pod on a <link xlink:href="https://access.redhat.com/articles/5064151">source cluster or on a remote cluster</link>.</simpara>
</listitem>
<listitem>
<simpara>Install the <emphasis>legacy</emphasis> Migration Toolkit for Containers Operator on the OpenShift Container Platform 3 source cluster from the command line interface.</simpara>
</listitem>
<listitem>
<simpara>Configure object storage to use as a replication repository.</simpara>
</listitem>
</orderedlist>
<simpara>To uninstall MTC, see <link linkend="migration-uninstalling-mtc-clean-up_installing-restricted-3-4">Uninstalling MTC and deleting resources</link>.</simpara>
<section xml:id="migration-compatibility-guidelines_installing-restricted-3-4">
<title>Compatibility guidelines</title>
<simpara>You must install the Migration Toolkit for Containers (MTC) Operator that is compatible with your OpenShift Container Platform version.</simpara>
<variablelist>
<title>Definitions</title>
<varlistentry>
<term>legacy platform</term>
<listitem>
<simpara>OpenShift Container Platform 4.5 and earlier.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>modern platform</term>
<listitem>
<simpara>OpenShift Container Platform 4.6 and later.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>legacy operator</term>
<listitem>
<simpara>The MTC Operator designed for legacy platforms.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>modern operator</term>
<listitem>
<simpara>The MTC Operator designed for modern platforms.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>control cluster</term>
<listitem>
<simpara>The cluster that runs the MTC controller and GUI.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>remote cluster</term>
<listitem>
<simpara>A source or destination cluster for a migration that runs Velero. The Control Cluster communicates with Remote clusters via the Velero API to drive migrations.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>You must use the compatible MTC version for migrating your OpenShift Container Platform clusters. For the migration to succeed both your source cluster and the destination cluster must use the same version of MTC.</simpara>
<simpara>MTC 1.7 supports migrations from OpenShift Container Platform 3.11 to 4.8.</simpara>
<simpara>MTC 1.8 only supports migrations from OpenShift Container Platform 4.9 and later.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>MTC compatibility: Migrating from a legacy or a modern platform</title>
<tgroup cols="5">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="20*"/>
<colspec colname="col_4" colwidth="20*"/>
<colspec colname="col_5" colwidth="20*"/>
<thead>
<row>
<entry align="left" valign="top">Details</entry>
<entry align="left" valign="top">OpenShift Container Platform 3.11</entry>
<entry align="left" valign="top">OpenShift Container Platform 4.0 to 4.5</entry>
<entry align="left" valign="top">OpenShift Container Platform 4.6 to 4.8</entry>
<entry align="left" valign="top">OpenShift Container Platform 4.9 or later</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Stable MTC version</simpara></entry>
<entry align="left" valign="top"><simpara>MTC v.1.7.<emphasis>z</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>MTC v.1.7.<emphasis>z</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>MTC v.1.7.<emphasis>z</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>MTC v.1.8.<emphasis>z</emphasis></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Installation</simpara></entry>
<entry align="left" valign="top"></entry>
<entry align="left" valign="top"><simpara>Legacy MTC v.1.7.<emphasis>z</emphasis> operator: Install manually with the <literal>operator.yml</literal> file.</simpara><simpara>[<emphasis role="strong">IMPORTANT</emphasis>]
This cluster cannot be the control cluster.</simpara></entry>
<entry align="left" valign="top"><simpara>Install with OLM, release channel <literal>release-v1.7</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Install with OLM, release channel <literal>release-v1.8</literal></simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara>Edge cases exist in which network restrictions prevent modern clusters from connecting to other clusters involved in the migration. For example, when migrating from an OpenShift Container Platform 3.11 cluster on premises to a modern OpenShift Container Platform cluster in the cloud, where the modern cluster cannot connect to the OpenShift Container Platform 3.11 cluster.</simpara>
<simpara>With MTC v.1.7.<emphasis>z</emphasis>, if one of the remote clusters is unable to communicate with the control cluster because of network restrictions, use the <literal>crane tunnel-api</literal> command.</simpara>
<simpara>With the stable MTC release, although you should always designate the most modern cluster as the control cluster, in this specific case it is possible to designate the legacy cluster as the control cluster and push workloads to the remote cluster.</simpara>
</section>
<section xml:id="migration-installing-mtc-on-ocp-4_installing-restricted-3-4">
<title>Installing the Migration Toolkit for Containers Operator on OpenShift Container Platform 4.14</title>
<simpara>You install the Migration Toolkit for Containers Operator on OpenShift Container Platform 4.14 by using the Operator Lifecycle Manager.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must be logged in as a user with <literal>cluster-admin</literal> privileges on all clusters.</simpara>
</listitem>
<listitem>
<simpara>You must create an Operator catalog from a mirror image in a local registry.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the OpenShift Container Platform web console, click <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">OperatorHub</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Use the <emphasis role="strong">Filter by keyword</emphasis> field to find the <emphasis role="strong">Migration Toolkit for Containers Operator</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select the <emphasis role="strong">Migration Toolkit for Containers Operator</emphasis> and click <emphasis role="strong">Install</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Install</emphasis>.</simpara>
<simpara>On the <emphasis role="strong">Installed Operators</emphasis> page, the <emphasis role="strong">Migration Toolkit for Containers Operator</emphasis> appears in the <emphasis role="strong">openshift-migration</emphasis> project with the status <emphasis role="strong">Succeeded</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Migration Toolkit for Containers Operator</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Under <emphasis role="strong">Provided APIs</emphasis>, locate the <emphasis role="strong">Migration Controller</emphasis> tile, and click <emphasis role="strong">Create Instance</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Workloads</emphasis> &#8594; <emphasis role="strong">Pods</emphasis> to verify that the MTC pods are running.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="migration-installing-legacy-operator_installing-restricted-3-4">
<title>Installing the legacy Migration Toolkit for Containers Operator on OpenShift Container Platform 3</title>
<simpara>You can install the legacy Migration Toolkit for Containers Operator manually on OpenShift Container Platform 3.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must be logged in as a user with <literal>cluster-admin</literal> privileges on all clusters.</simpara>
</listitem>
<listitem>
<simpara>You must have access to <literal>registry.redhat.io</literal>.</simpara>
</listitem>
<listitem>
<simpara>You must have <literal>podman</literal> installed.</simpara>
</listitem>
<listitem>
<simpara>You must create an <link xlink:href="https://access.redhat.com/solutions/3772061">image stream secret</link> and copy it to each node in the cluster.</simpara>
</listitem>
<listitem>
<simpara>You must have a Linux workstation with network access in order to download files from <literal>registry.redhat.io</literal>.</simpara>
</listitem>
<listitem>
<simpara>You must create a mirror image of the Operator catalog.</simpara>
</listitem>
<listitem>
<simpara>You must install the Migration Toolkit for Containers Operator from the mirrored Operator catalog on OpenShift Container Platform 4.14.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Log in to <literal>registry.redhat.io</literal> with your Red Hat Customer Portal credentials:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ podman login registry.redhat.io</programlisting>
</listitem>
<listitem>
<simpara>Download the <literal>operator.yml</literal> file by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">podman cp $(podman create registry.redhat.io/rhmtc/openshift-migration-legacy-rhel8-operator:v1.7):/operator.yml ./</programlisting>
</listitem>
<listitem>
<simpara>Download the <literal>controller.yml</literal> file by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">podman cp $(podman create registry.redhat.io/rhmtc/openshift-migration-legacy-rhel8-operator:v1.7):/controller.yml ./</programlisting>
</listitem>
<listitem>
<simpara>Obtain the Operator image mapping by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ grep openshift-migration-legacy-rhel8-operator ./mapping.txt | grep rhmtc</programlisting>
<simpara>The <literal>mapping.txt</literal> file was created when you mirrored the Operator catalog. The output shows the mapping between the <literal>registry.redhat.io</literal> image and your mirror registry image.</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">registry.redhat.io/rhmtc/openshift-migration-legacy-rhel8-operator@sha256:468a6126f73b1ee12085ca53a312d1f96ef5a2ca03442bcb63724af5e2614e8a=&lt;registry.apps.example.com&gt;/rhmtc/openshift-migration-legacy-rhel8-operator</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Update the <literal>image</literal> values for the <literal>ansible</literal> and <literal>operator</literal> containers and the <literal>REGISTRY</literal> value in the <literal>operator.yml</literal> file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">containers:
  - name: ansible
    image: &lt;registry.apps.example.com&gt;/rhmtc/openshift-migration-legacy-rhel8-operator@sha256:&lt;468a6126f73b1ee12085ca53a312d1f96ef5a2ca03442bcb63724af5e2614e8a&gt; <co xml:id="CO7-1"/>
...
  - name: operator
    image: &lt;registry.apps.example.com&gt;/rhmtc/openshift-migration-legacy-rhel8-operator@sha256:&lt;468a6126f73b1ee12085ca53a312d1f96ef5a2ca03442bcb63724af5e2614e8a&gt; <co xml:id="CO7-2"/>
...
    env:
    - name: REGISTRY
      value: &lt;registry.apps.example.com&gt; <co xml:id="CO7-3"/></programlisting>
<calloutlist>
<callout arearefs="CO7-1 CO7-2">
<para>Specify your mirror registry and the <literal>sha256</literal> value of the Operator image.</para>
</callout>
<callout arearefs="CO7-3">
<para>Specify your mirror registry.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Log in to your OpenShift Container Platform source cluster.</simpara>
</listitem>
<listitem>
<simpara>Create the Migration Toolkit for Containers Operator object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f operator.yml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">namespace/openshift-migration created
rolebinding.rbac.authorization.k8s.io/system:deployers created
serviceaccount/migration-operator created
customresourcedefinition.apiextensions.k8s.io/migrationcontrollers.migration.openshift.io created
role.rbac.authorization.k8s.io/migration-operator created
rolebinding.rbac.authorization.k8s.io/migration-operator created
clusterrolebinding.rbac.authorization.k8s.io/migration-operator created
deployment.apps/migration-operator created
Error from server (AlreadyExists): error when creating "./operator.yml":
rolebindings.rbac.authorization.k8s.io "system:image-builders" already exists <co xml:id="CO8-1"/>
Error from server (AlreadyExists): error when creating "./operator.yml":
rolebindings.rbac.authorization.k8s.io "system:image-pullers" already exists</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO8-1">
<para>You can ignore <literal>Error from server (AlreadyExists)</literal> messages. They are caused by the Migration Toolkit for Containers Operator creating resources for earlier versions of OpenShift Container Platform 4 that are provided in later releases.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the <literal>MigrationController</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f controller.yml</programlisting>
</listitem>
<listitem>
<simpara>Verify that the MTC pods are running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -n openshift-migration</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="migration-about-configuring-proxies_installing-restricted-3-4">
<title>Proxy configuration</title>
<simpara>For OpenShift Container Platform 4.1 and earlier versions, you must configure proxies in the <literal>MigrationController</literal> custom resource (CR) manifest after you install the Migration Toolkit for Containers Operator because these versions do not support a cluster-wide <literal>proxy</literal> object.</simpara>
<simpara>For OpenShift Container Platform 4.2 to 4.14, the Migration Toolkit for Containers (MTC) inherits the cluster-wide proxy settings. You can change the proxy parameters if you want to override the cluster-wide proxy settings.</simpara>
<section xml:id="direct-volume-migration_installing-restricted-3-4">
<title>Direct volume migration</title>
<simpara>Direct Volume Migration (DVM) was introduced in MTC 1.4.2. DVM supports only one proxy. The source cluster cannot access the route of the target cluster if the target cluster is also behind a proxy.</simpara>
<simpara>If you want to perform a DVM from a source cluster behind a proxy, you must configure a TCP proxy that works at the transport layer and forwards the SSL connections transparently without decrypting and re-encrypting them with their own SSL certificates. A Stunnel proxy is an example of such a proxy.</simpara>
<section xml:id="tcp-proxy-setup-for-dvm_installing-restricted-3-4">
<title>TCP proxy setup for DVM</title>
<simpara>You can set up a direct connection between the source and the target cluster through a TCP proxy and configure the <literal>stunnel_tcp_proxy</literal> variable in the <literal>MigrationController</literal> CR to use the proxy:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: migration.openshift.io/v1alpha1
kind: MigrationController
metadata:
  name: migration-controller
  namespace: openshift-migration
spec:
  [...]
  stunnel_tcp_proxy: http://username:password@ip:port</programlisting>
<simpara>Direct volume migration (DVM) supports only basic authentication for the proxy. Moreover, DVM works only from behind proxies that can tunnel a TCP connection transparently. HTTP/HTTPS proxies in man-in-the-middle mode do not work. The existing cluster-wide proxies might not support this behavior. As a result, the proxy settings for DVM are intentionally kept different from the usual proxy configuration in MTC.</simpara>
</section>
<section xml:id="why-tcp-proxy-instead-of-an-http-https-proxy_installing-restricted-3-4">
<title>Why use a TCP proxy instead of an HTTP/HTTPS proxy?</title>
<simpara>You can enable DVM by running Rsync between the source and the target cluster over an OpenShift route.  Traffic is encrypted using Stunnel, a TCP proxy. The Stunnel running on the source cluster initiates a TLS connection with the target Stunnel and transfers data over an encrypted channel.</simpara>
<simpara>Cluster-wide HTTP/HTTPS proxies in OpenShift are usually configured in man-in-the-middle mode where they negotiate their own TLS session with the outside servers. However, this does not work with Stunnel. Stunnel requires that its TLS session be untouched by the proxy, essentially making the proxy a transparent tunnel which simply forwards the TCP connection as-is. Therefore, you must use a TCP proxy.</simpara>
</section>
<section xml:id="dvm-known-issues_installing-restricted-3-4">
<title>Known issue</title>
<formalpara>
<title>Migration fails with error <literal>Upgrade request required</literal></title>
<para>The migration Controller uses the SPDY protocol to execute commands within remote pods. If the remote cluster is behind a proxy or a firewall that does not support the SPDY protocol, the migration controller fails to execute remote commands. The migration fails with the error message <literal>Upgrade request required</literal>.
Workaround: Use a proxy that supports the SPDY protocol.</para>
</formalpara>
<simpara>In addition to supporting the SPDY protocol, the proxy or firewall also must pass the <literal>Upgrade</literal> HTTP header to the API server. The client uses this header to open a websocket connection with the API server. If the <literal>Upgrade</literal> header is blocked by the proxy or firewall, the migration fails with the error message <literal>Upgrade request required</literal>.
Workaround: Ensure that the proxy forwards the <literal>Upgrade</literal> header.</simpara>
</section>
</section>
<section xml:id="tuning-network-policies-for-migrations_installing-restricted-3-4">
<title>Tuning network policies for migrations</title>
<simpara>OpenShift supports restricting traffic to or from pods using <emphasis>NetworkPolicy</emphasis> or <emphasis>EgressFirewalls</emphasis> based on the network plugin used by the cluster. If any of the source namespaces involved in a migration use such mechanisms to restrict network traffic to pods, the restrictions might inadvertently stop traffic to Rsync pods during migration.</simpara>
<simpara>Rsync pods running on both the source and the target clusters must connect to each other over an OpenShift Route. Existing <emphasis>NetworkPolicy</emphasis> or <emphasis>EgressNetworkPolicy</emphasis> objects can be configured to automatically exempt Rsync pods from these traffic restrictions.</simpara>
<section xml:id="dvm-network-policy-configuration_installing-restricted-3-4">
<title>NetworkPolicy configuration</title>
<section xml:id="egress-traffic-from-rsync-pods_installing-restricted-3-4">
<title>Egress traffic from Rsync pods</title>
<simpara>You can use the unique labels of Rsync pods to allow egress traffic to pass from them if the <literal>NetworkPolicy</literal> configuration in the source or destination namespaces blocks this type of traffic. The following policy allows <emphasis role="strong">all</emphasis> egress traffic from Rsync pods in the namespace:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-all-egress-from-rsync-pods
spec:
  podSelector:
    matchLabels:
      owner: directvolumemigration
      app: directvolumemigration-rsync-transfer
  egress:
  - {}
  policyTypes:
  - Egress</programlisting>
</section>
<section xml:id="ingress-traffic-to-rsync-pods_installing-restricted-3-4">
<title>Ingress traffic to Rsync pods</title>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-all-egress-from-rsync-pods
spec:
  podSelector:
    matchLabels:
      owner: directvolumemigration
      app: directvolumemigration-rsync-transfer
  ingress:
  - {}
  policyTypes:
  - Ingress</programlisting>
</section>
</section>
<section xml:id="egressnetworkpolicy-config_installing-restricted-3-4">
<title>EgressNetworkPolicy configuration</title>
<simpara>The <literal>EgressNetworkPolicy</literal> object or <emphasis>Egress Firewalls</emphasis> are OpenShift constructs designed to block egress traffic leaving the cluster.</simpara>
<simpara>Unlike the <literal>NetworkPolicy</literal> object, the Egress Firewall works at a project level because it applies to all pods in the namespace. Therefore, the unique labels of Rsync pods do not exempt only Rsync pods from the restrictions. However, you can add the CIDR ranges of the source or target cluster to the <emphasis>Allow</emphasis> rule of the policy so that a direct connection can be setup between two clusters.</simpara>
<simpara>Based on which cluster the Egress Firewall is present in, you can add the CIDR range of the other cluster to allow egress traffic between the two:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: network.openshift.io/v1
kind: EgressNetworkPolicy
metadata:
  name: test-egress-policy
  namespace: &lt;namespace&gt;
spec:
  egress:
  - to:
      cidrSelector: &lt;cidr_of_source_or_target_cluster&gt;
    type: Deny</programlisting>
</section>
<section xml:id="choosing-alternate-endpoints-for-data-transfer_installing-restricted-3-4">
<title>Choosing alternate endpoints for data transfer</title>
<simpara>By default, DVM uses an OpenShift Container Platform route as an endpoint to transfer PV data to destination clusters. You can choose another type of supported endpoint, if cluster topologies allow.</simpara>
<simpara>For each cluster, you can configure an endpoint by setting the <literal>rsync_endpoint_type</literal> variable on the appropriate <emphasis role="strong">destination</emphasis> cluster in your <literal>MigrationController</literal> CR:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: migration.openshift.io/v1alpha1
kind: MigrationController
metadata:
  name: migration-controller
  namespace: openshift-migration
spec:
  [...]
  rsync_endpoint_type: [NodePort|ClusterIP|Route]</programlisting>
</section>
<section xml:id="configuring-supplemental-groups-for-rsync-pods_installing-restricted-3-4">
<title>Configuring supplemental groups for Rsync pods</title>
<simpara>When your PVCs use a shared storage, you can configure the access to that storage by adding supplemental groups to Rsync pod definitions in order for the pods to allow access:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Supplementary groups for Rsync pods</title>
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<thead>
<row>
<entry align="left" valign="top">Variable</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Default</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>src_supplemental_groups</literal></simpara></entry>
<entry align="left" valign="top"><simpara>string</simpara></entry>
<entry align="left" valign="top"><simpara>Not set</simpara></entry>
<entry align="left" valign="top"><simpara>Comma-separated list of supplemental groups for source Rsync pods</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>target_supplemental_groups</literal></simpara></entry>
<entry align="left" valign="top"><simpara>string</simpara></entry>
<entry align="left" valign="top"><simpara>Not set</simpara></entry>
<entry align="left" valign="top"><simpara>Comma-separated list of supplemental groups for target Rsync pods</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<formalpara>
<title>Example usage</title>
<para>The <literal>MigrationController</literal> CR can be updated to set values for these supplemental groups:</para>
</formalpara>
<programlisting language="yaml" linenumbering="unnumbered">spec:
  src_supplemental_groups: "1000,2000"
  target_supplemental_groups: "2000,3000"</programlisting>
</section>
</section>
<section xml:id="migration-configuring-proxies_installing-restricted-3-4">
<title>Configuring proxies</title>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must be logged in as a user with <literal>cluster-admin</literal> privileges on all clusters.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Get the <literal>MigrationController</literal> CR manifest:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get migrationcontroller &lt;migration_controller&gt; -n openshift-migration</programlisting>
</listitem>
<listitem>
<simpara>Update the proxy parameters:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: migration.openshift.io/v1alpha1
kind: MigrationController
metadata:
  name: &lt;migration_controller&gt;
  namespace: openshift-migration
...
spec:
  stunnel_tcp_proxy: http://&lt;username&gt;:&lt;password&gt;@&lt;ip&gt;:&lt;port&gt; <co xml:id="CO9-1"/>
  noProxy: example.com <co xml:id="CO9-2"/></programlisting>
<calloutlist>
<callout arearefs="CO9-1">
<para>Stunnel proxy URL for direct volume migration.</para>
</callout>
<callout arearefs="CO9-2">
<para>Comma-separated list of destination domain names, domains, IP addresses, or other network CIDRs to exclude proxying.</para>
</callout>
</calloutlist>
<simpara>Preface a domain with <literal>.</literal> to match subdomains only. For example, <literal>.y.com</literal> matches <literal>x.y.com</literal>, but not <literal>y.com</literal>. Use <literal>*</literal> to bypass proxy for all destinations.
If you scale up workers that are not included in the network defined by the <literal>networking.machineNetwork[].cidr</literal> field from the installation configuration, you must add them to this list to prevent connection issues.</simpara>
<simpara>This field is ignored if neither the <literal>httpProxy</literal> nor the <literal>httpsProxy</literal> field is set.</simpara>
</listitem>
<listitem>
<simpara>Save the manifest as <literal>migration-controller.yaml</literal>.</simpara>
</listitem>
<listitem>
<simpara>Apply the updated manifest:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc replace -f migration-controller.yaml -n openshift-migration</programlisting>
</listitem>
</orderedlist>
<simpara>For more information, see <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/networking/#nw-proxy-configure-object_config-cluster-wide-proxy">Configuring the cluster-wide proxy</link>.</simpara>
</section>
</section>
<section xml:id="configuring-replication-repository_installing-restricted-3-4">
<title>Configuring a replication repository</title>
<simpara>The Multicloud Object Gateway is the only supported option for a restricted network environment.</simpara>
<simpara>MTC supports the <link linkend="migration-understanding-data-copy-methods_about-mtc-3-4">file system and snapshot data copy methods</link> for migrating data from the source cluster to the target cluster. You can select a method that is suited for your environment and is supported by your storage provider.</simpara>
<section xml:id="replication-repository-prerequisites_installing-restricted-3-4">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>All clusters must have uninterrupted network access to the replication repository.</simpara>
</listitem>
<listitem>
<simpara>If you use a proxy server with an internally hosted replication repository, you must ensure that the proxy allows access to the replication repository.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="migration-configuring-mcg_installing-restricted-3-4">
<title>Retrieving Multicloud Object Gateway credentials</title>
<simpara>You must retrieve the Multicloud Object Gateway (MCG) credentials in order to create a <literal>Secret</literal> custom resource (CR) for the OpenShift API for Data Protection (OADP).</simpara>
<simpara>MCG is a component of OpenShift Data Foundation.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must deploy OpenShift Data Foundation by using the appropriate <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.9">OpenShift Data Foundation deployment guide</link>.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Obtain the S3 endpoint, <literal>AWS_ACCESS_KEY_ID</literal>, and <literal>AWS_SECRET_ACCESS_KEY</literal> by running the <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.9/html/managing_hybrid_and_multicloud_resources/accessing-the-multicloud-object-gateway-with-your-applications_rhodf#accessing-the-Multicloud-object-gateway-from-the-terminal_rhodf"><literal>describe</literal> command</link> on the <literal>NooBaa</literal> custom resource.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="installing-restricted-3-4_configuring-replication-repository-additional-resources" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.9/html/planning_your_deployment/disconnected-environment_rhodf">Disconnected environment</link> in the Red Hat OpenShift Data Foundation documentation.</simpara>
</listitem>
<listitem>
<simpara><link linkend="migration-mtc-workflow_about-mtc-3-4">MTC workflow</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="migration-understanding-data-copy-methods_about-mtc-3-4">About data copy methods</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="migration-adding-replication-repository-to-cam_migrating-applications-3-4">Adding a replication repository to the MTC web console</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="migration-uninstalling-mtc-clean-up_installing-restricted-3-4">
<title>Uninstalling MTC and deleting resources</title>
<simpara>You can uninstall the Migration Toolkit for Containers (MTC) and delete its resources to clean up the cluster.</simpara>
<note>
<simpara>Deleting the <literal>velero</literal> CRDs removes Velero from the cluster.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must be logged in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Delete the <literal>MigrationController</literal> custom resource (CR) on all clusters:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete migrationcontroller &lt;migration_controller&gt;</programlisting>
</listitem>
<listitem>
<simpara>Uninstall the Migration Toolkit for Containers Operator on OpenShift Container Platform 4 by using the Operator Lifecycle Manager.</simpara>
</listitem>
<listitem>
<simpara>Delete cluster-scoped resources on all clusters by running the following commands:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>migration</literal> custom resource definitions (CRDs):</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete $(oc get crds -o name | grep 'migration.openshift.io')</programlisting>
</listitem>
<listitem>
<simpara><literal>velero</literal> CRDs:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete $(oc get crds -o name | grep 'velero')</programlisting>
</listitem>
<listitem>
<simpara><literal>migration</literal> cluster roles:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete $(oc get clusterroles -o name | grep 'migration.openshift.io')</programlisting>
</listitem>
<listitem>
<simpara><literal>migration-operator</literal> cluster role:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete clusterrole migration-operator</programlisting>
</listitem>
<listitem>
<simpara><literal>velero</literal> cluster roles:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete $(oc get clusterroles -o name | grep 'velero')</programlisting>
</listitem>
<listitem>
<simpara><literal>migration</literal> cluster role bindings:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete $(oc get clusterrolebindings -o name | grep 'migration.openshift.io')</programlisting>
</listitem>
<listitem>
<simpara><literal>migration-operator</literal> cluster role bindings:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete clusterrolebindings migration-operator</programlisting>
</listitem>
<listitem>
<simpara><literal>velero</literal> cluster role bindings:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete $(oc get clusterrolebindings -o name | grep 'velero')</programlisting>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</section>
</chapter>
<chapter xml:id="upgrading-3-4">
<title>Upgrading the Migration Toolkit for Containers</title>
<simpara>You can upgrade the Migration Toolkit for Containers (MTC) on OpenShift Container Platform 4.14 by using Operator Lifecycle Manager.</simpara>
<simpara>You can upgrade MTC on OpenShift Container Platform 3 by reinstalling the legacy Migration Toolkit for Containers Operator.</simpara>
<important>
<simpara>If you are upgrading from MTC version 1.3, you must perform an additional procedure to update the <literal>MigPlan</literal> custom resource (CR).</simpara>
</important>
<section xml:id="migration-upgrading-mtc-on-ocp-4_upgrading-3-4">
<title>Upgrading the Migration Toolkit for Containers on OpenShift Container Platform 4.14</title>
<simpara>You can upgrade the Migration Toolkit for Containers (MTC) on OpenShift Container Platform 4.14 by using the Operator Lifecycle Manager.</simpara>
<important>
<simpara>When upgrading the MTC by using the Operator Lifecycle Manager, you must use a supported migration path.</simpara>
</important>
<itemizedlist>
<title>Migration paths</title>
<listitem>
<simpara>Migrating from OpenShift Container Platform 3 to OpenShift Container Platform 4 requires a legacy MTC Operator and MTC 1.7.x.</simpara>
</listitem>
<listitem>
<simpara>Migrating from MTC 1.7.x to MTC 1.8.x is not supported.</simpara>
</listitem>
<listitem>
<simpara>You must use MTC 1.7.x to migrate anything with a source of OpenShift Container Platform 4.9 or earlier.</simpara>
<itemizedlist>
<listitem>
<simpara>MTC 1.7.x must be used on both source and destination.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>MTC 1.8.x only supports migrations from OpenShift Container Platform 4.10 or later to OpenShift Container Platform 4.10 or later. For migrations only involving cluster versions 4.10 and later, either 1.7.x or 1.8.x may be used. However, it must be the same MTC version on both source &amp; destination.</simpara>
<itemizedlist>
<listitem>
<simpara>Migration from source MTC 1.7.x to destination MTC 1.8.x is unsupported.</simpara>
</listitem>
<listitem>
<simpara>Migration from source MTC 1.8.x to destination MTC 1.7.x is unsupported.</simpara>
</listitem>
<listitem>
<simpara>Migration from source MTC 1.7.x to destination MTC 1.7.x is supported.</simpara>
</listitem>
<listitem>
<simpara>Migration from source MTC 1.8.x to destination MTC 1.8.x is supported</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must be logged in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the OpenShift Container Platform console, navigate to <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">Installed Operators</emphasis>.</simpara>
<simpara>Operators that have a pending upgrade display an <emphasis role="strong">Upgrade available</emphasis> status.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Migration Toolkit for Containers Operator</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">Subscription</emphasis> tab. Any upgrades requiring approval are displayed next to <emphasis role="strong">Upgrade Status</emphasis>. For example, it might display <emphasis role="strong">1 requires approval</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">1 requires approval</emphasis>, then click <emphasis role="strong">Preview Install Plan</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Review the resources that are listed as available for upgrade and click <emphasis role="strong">Approve</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Navigate back to the <emphasis role="strong">Operators &#8594; Installed Operators</emphasis> page to monitor the progress of the upgrade. When complete, the status changes to <emphasis role="strong">Succeeded</emphasis> and <emphasis role="strong">Up to date</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Workloads</emphasis> &#8594; <emphasis role="strong">Pods</emphasis> to verify that the MTC pods are running.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="migration-upgrading-mtc-with-legacy-operator_upgrading-3-4">
<title>Upgrading the Migration Toolkit for Containers on OpenShift Container Platform 3</title>
<simpara>You can upgrade Migration Toolkit for Containers (MTC) on OpenShift Container Platform 3 by manually installing the legacy Migration Toolkit for Containers Operator.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must be logged in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You must have access to <literal>registry.redhat.io</literal>.</simpara>
</listitem>
<listitem>
<simpara>You must have <literal>podman</literal> installed.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Log in to <literal>registry.redhat.io</literal> with your Red Hat Customer Portal credentials by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ podman login registry.redhat.io</programlisting>
</listitem>
<listitem>
<simpara>Download the <literal>operator.yml</literal> file by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ podman cp $(podman create \
  registry.redhat.io/rhmtc/openshift-migration-legacy-rhel8-operator:v1.8):/operator.yml ./</programlisting>
</listitem>
<listitem>
<simpara>Replace the Migration Toolkit for Containers Operator by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc replace --force -f operator.yml</programlisting>
</listitem>
<listitem>
<simpara>Scale the <literal>migration-operator</literal> deployment to <literal>0</literal> to stop the deployment by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc scale -n openshift-migration --replicas=0 deployment/migration-operator</programlisting>
</listitem>
<listitem>
<simpara>Scale the <literal>migration-operator</literal> deployment to <literal>1</literal> to start the deployment and apply the changes by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc scale -n openshift-migration --replicas=1 deployment/migration-operator</programlisting>
</listitem>
<listitem>
<simpara>Verify that the <literal>migration-operator</literal> was upgraded by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -o yaml -n openshift-migration get deployment/migration-operator | grep image: | awk -F ":" '{ print $NF }'</programlisting>
</listitem>
<listitem>
<simpara>Download the <literal>controller.yml</literal> file by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ podman cp $(podman create \
  registry.redhat.io/rhmtc/openshift-migration-legacy-rhel8-operator:v1.8):/controller.yml ./</programlisting>
</listitem>
<listitem>
<simpara>Create the <literal>migration-controller</literal> object by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f controller.yml</programlisting>
</listitem>
<listitem>
<simpara>If you have previously added the OpenShift Container Platform 3 cluster to the MTC web console, you must update the service account token in the web console because the upgrade process deletes and restores the <literal>openshift-migration</literal> namespace:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Obtain the service account token by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc sa get-token migration-controller -n openshift-migration</programlisting>
</listitem>
<listitem>
<simpara>In the MTC web console, click <emphasis role="strong">Clusters</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click the Options menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> next to the cluster and select <emphasis role="strong">Edit</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Enter the new service account token in the <emphasis role="strong">Service account token</emphasis> field.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Update cluster</emphasis> and then click <emphasis role="strong">Close</emphasis>.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Verify that the MTC pods are running by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -n openshift-migration</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="migration-upgrading-from-mtc-1-3_upgrading-3-4">
<title>Upgrading MTC 1.3 to 1.8</title>
<simpara>If you are upgrading Migration Toolkit for Containers (MTC) version 1.3.x to 1.8, you must update the <literal>MigPlan</literal> custom resource (CR) manifest on the cluster on which the <literal>MigrationController</literal> pod is running.</simpara>
<simpara>Because the <literal>indirectImageMigration</literal> and <literal>indirectVolumeMigration</literal> parameters do not exist in MTC 1.3, their default value in version 1.4 is <literal>false</literal>, which means that direct image migration and direct volume migration are enabled. Because the direct migration requirements are not fulfilled, the migration plan cannot reach a <literal>Ready</literal> state unless these parameter values are changed to <literal>true</literal>.</simpara>
<important>
<itemizedlist>
<listitem>
<simpara>Migrating from OpenShift Container Platform 3 to OpenShift Container Platform 4 requires a legacy MTC Operator and MTC 1.7.x.</simpara>
</listitem>
<listitem>
<simpara>Upgrading MTC 1.7.x to 1.8.x requires manually updating the OADP channel from <literal>stable-1.0</literal> to <literal>stable-1.2</literal> in order to successfully complete the upgrade from 1.7.x to 1.8.x.</simpara>
</listitem>
</itemizedlist>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must be logged in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Log in to the cluster on which the <literal>MigrationController</literal> pod is running.</simpara>
</listitem>
<listitem>
<simpara>Get the <literal>MigPlan</literal> CR manifest:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get migplan &lt;migplan&gt; -o yaml -n openshift-migration</programlisting>
</listitem>
<listitem>
<simpara>Update the following parameter values and save the file as <literal>migplan.yaml</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">...
spec:
  indirectImageMigration: true
  indirectVolumeMigration: true</programlisting>
</listitem>
<listitem>
<simpara>Replace the <literal>MigPlan</literal> CR manifest to apply the changes:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc replace -f migplan.yaml -n openshift-migration</programlisting>
</listitem>
<listitem>
<simpara>Get the updated <literal>MigPlan</literal> CR manifest to verify the changes:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get migplan &lt;migplan&gt; -o yaml -n openshift-migration</programlisting>
</listitem>
</orderedlist>
</section>
</chapter>
<chapter xml:id="premigration-checklists-3-4">
<title>Premigration checklists</title>
<simpara>Before you migrate your application workloads with the Migration Toolkit for Containers (MTC), review the following checklists.</simpara>
<section xml:id="resources_premigration-checklists-3-4">
<title>Resources</title>
<itemizedlist mark="none">
<listitem>
<simpara>&#10063; If your application uses an internal service network or an external route for communicating with services, the relevant route exists.</simpara>
</listitem>
<listitem>
<simpara>&#10063; If your application uses cluster-level resources, you have re-created them on the target cluster.</simpara>
</listitem>
<listitem>
<simpara>&#10063; You have <link linkend="migration-excluding-resources_advanced-migration-options-3-4">excluded</link> persistent volumes (PVs), image streams, and other resources that you do not want to migrate.</simpara>
</listitem>
<listitem>
<simpara>&#10063; PV data has been backed up in case an application displays unexpected behavior after migration and corrupts the data.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="source-cluster_premigration-checklists-3-4">
<title>Source cluster</title>
<itemizedlist mark="none">
<listitem>
<simpara>&#10063; The cluster meets the <link xlink:href="https://docs.openshift.com/container-platform/3.11/install/prerequisites.html#hardware">minimum hardware requirements</link>.</simpara>
</listitem>
<listitem>
<simpara>&#10063; You have installed the correct legacy Migration Toolkit for Containers Operator version:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>operator-3.7.yml</literal> on OpenShift Container Platform version 3.7.</simpara>
</listitem>
<listitem>
<simpara><literal>operator.yml</literal> on OpenShift Container Platform versions 3.9 to 4.5.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>&#10063; All nodes have an active OpenShift Container Platform subscription.</simpara>
</listitem>
<listitem>
<simpara>&#10063; You have performed all the <link xlink:href="https://docs.openshift.com/container-platform/3.11/day_two_guide/run_once_tasks.html#day-two-guide-default-storage-class">run-once tasks</link>.</simpara>
</listitem>
<listitem>
<simpara>&#10063; You have performed all the <link xlink:href="https://docs.openshift.com/container-platform/3.11/day_two_guide/environment_health_checks.html">environment health checks</link>.</simpara>
</listitem>
<listitem>
<simpara>&#10063; You have checked for PVs with abnormal configurations  stuck in a <emphasis role="strong">Terminating</emphasis> state by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pv</programlisting>
</listitem>
<listitem>
<simpara>&#10063; You have checked for pods whose status is other than <emphasis role="strong">Running</emphasis> or <emphasis role="strong">Completed</emphasis> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods --all-namespaces | egrep -v 'Running | Completed'</programlisting>
</listitem>
<listitem>
<simpara>&#10063; You have checked for pods with a high restart count by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods --all-namespaces --field-selector=status.phase=Running \
  -o json | jq '.items[]|select(any( .status.containerStatuses[]; \
  .restartCount &gt; 3))|.metadata.name'</programlisting>
<simpara>Even if the pods are in a <emphasis role="strong">Running</emphasis> state, a high restart count might indicate underlying problems.</simpara>
</listitem>
<listitem>
<simpara>&#10063; You have removed old builds, deployments, and images from each namespace to be migrated by <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/building_applications/#pruning-objects">pruning</link>.</simpara>
</listitem>
<listitem>
<simpara>&#10063; The OpenShift image registry uses a <link xlink:href="https://docs.openshift.com/container-platform/3.11/scaling_performance/optimizing_storage.html#registry">supported storage type</link>.</simpara>
</listitem>
<listitem>
<simpara>&#10063; Direct image migration only: The OpenShift image registry is <link xlink:href="https://docs.openshift.com/container-platform/3.11/install_config/registry/securing_and_exposing_registry.html#exposing-the-registry">exposed</link> to external traffic.</simpara>
</listitem>
<listitem>
<simpara>&#10063; You can read and write images to the registry.</simpara>
</listitem>
<listitem>
<simpara>&#10063; The <link xlink:href="https://access.redhat.com/articles/3093761">etcd cluster</link> is healthy.</simpara>
</listitem>
<listitem>
<simpara>&#10063; The <link xlink:href="https://docs.openshift.com/container-platform/3.11/install_config/master_node_configuration.html#master-node-configuration-node-qps-burst">average API server response time</link> on the source cluster is less than 50 ms.</simpara>
</listitem>
<listitem>
<simpara>&#10063; The cluster certificates are <link xlink:href="https://docs.openshift.com/container-platform/3.11/install_config/redeploying_certificates.html#install-config-cert-expiry">valid</link> for the duration of the migration process.</simpara>
</listitem>
<listitem>
<simpara>&#10063; You have checked for pending certificate-signing requests by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get csr -A | grep pending -i</programlisting>
</listitem>
<listitem>
<simpara>&#10063; The <link xlink:href="https://docs.openshift.com/container-platform/3.11/install_config/configuring_authentication.html#overview">identity provider</link> is working.</simpara>
</listitem>
<listitem>
<simpara>&#10063; You have set the value of the <literal>openshift.io/host.generated</literal> annotation parameter to <literal>true</literal> for each OpenShift Container Platform route, which updates the host name of the route for the target cluster. Otherwise, the migrated routes retain the source cluster host name.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="target-cluster_premigration-checklists-3-4">
<title>Target cluster</title>
<itemizedlist mark="none">
<listitem>
<simpara>&#10063; You have installed Migration Toolkit for Containers Operator version 1.5.1.</simpara>
</listitem>
<listitem>
<simpara>&#10063; All <link linkend="migration-prerequisites_migrating-applications-3-4">MTC prerequisites</link> are met.</simpara>
</listitem>
<listitem>
<simpara>&#10063; The cluster meets the minimum hardware requirements for the specific platform and installation method, for example, on <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/installing/#minimum-resource-requirements_installing-bare-metal">bare metal</link>.</simpara>
</listitem>
<listitem>
<simpara>&#10063; The cluster has <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/storage/#defining-storage-classes_dynamic-provisioning">storage classes</link> defined for the storage types used by the source cluster, for example, block volume, file system, or object storage.</simpara>
<note>
<simpara>NFS does not require a defined storage class.</simpara>
</note>
</listitem>
<listitem>
<simpara>&#10063; The cluster has the correct network configuration and permissions to access external services, for example, databases, source code repositories, container image registries, and CI/CD tools.</simpara>
</listitem>
<listitem>
<simpara>&#10063; External applications and services that use services provided by the cluster have the correct network configuration and permissions to access the cluster.</simpara>
</listitem>
<listitem>
<simpara>&#10063; Internal container image dependencies are met.</simpara>
<simpara>If an application uses an internal image in the <literal>openshift</literal> namespace that is not supported by OpenShift Container Platform 4.14, you can manually update the <link linkend="migration-updating-deprecated-internal-images_troubleshooting-3-4">OpenShift Container Platform 3 image stream tag</link> with <literal>podman</literal>.</simpara>
</listitem>
<listitem>
<simpara>&#10063; The target cluster and the replication repository have sufficient storage space.</simpara>
</listitem>
<listitem>
<simpara>&#10063; The <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/authentication_and_authorization/#supported-identity-providers">identity provider</link> is working.</simpara>
</listitem>
<listitem>
<simpara>&#10063; DNS records for your application exist on the target cluster.</simpara>
</listitem>
<listitem>
<simpara>&#10063; Certificates that your application uses exist on the target cluster.</simpara>
</listitem>
<listitem>
<simpara>&#10063; You have configured appropriate firewall rules on the target cluster.</simpara>
</listitem>
<listitem>
<simpara>&#10063; You have correctly configured load balancing on the target cluster.</simpara>
</listitem>
<listitem>
<simpara>&#10063; If you migrate objects to an existing namespace on the target cluster that has the same name as the namespace being migrated from the source, the target namespace contains no objects of the same name and type as the objects being migrated.</simpara>
<note>
<simpara>Do not create namespaces for your application on the target cluster before migration because this might cause quotas to change.</simpara>
</note>
</listitem>
</itemizedlist>
</section>
<section xml:id="performance_premigration-checklists-3-4">
<title>Performance</title>
<itemizedlist mark="none">
<listitem>
<simpara>&#10063; The migration network has a minimum throughput of 10 Gbps.</simpara>
</listitem>
<listitem>
<simpara>&#10063; The clusters have sufficient resources for migration.</simpara>
<note>
<simpara>Clusters require additional memory, CPUs, and storage in order to run a migration on top of normal workloads. Actual resource requirements depend on the number of Kubernetes resources being migrated in a single migration plan. You must test migrations in a non-production environment in order to estimate the resource requirements.</simpara>
</note>
</listitem>
<listitem>
<simpara>&#10063; The <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/support/#reviewing-node-status-use-and-configuration_verifying-node-health">memory and CPU usage</link> of the nodes are healthy.</simpara>
</listitem>
<listitem>
<simpara>&#10063; The <link xlink:href="https://access.redhat.com/solutions/4885641">etcd disk performance</link> of the clusters has been checked with <literal>fio</literal>.</simpara>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="migrating-applications-3-4">
<title>Migrating your applications</title>
<simpara>You can migrate your applications by using the Migration Toolkit for Containers (MTC) web console or from the <link linkend="migrating-applications-cli_advanced-migration-options-3-4">command line</link>.</simpara>
<simpara>You can use stage migration and cutover migration to migrate an application between clusters:</simpara>
<itemizedlist>
<listitem>
<simpara>Stage migration copies data from the source cluster to the target cluster without stopping the application. You can run a stage migration multiple times to reduce the duration of the cutover migration.</simpara>
</listitem>
<listitem>
<simpara>Cutover migration stops the transactions on the source cluster and moves the resources to the target cluster.</simpara>
</listitem>
</itemizedlist>
<simpara>You can use state migration to migrate an application&#8217;s state:</simpara>
<itemizedlist>
<listitem>
<simpara>State migration copies selected persistent volume claims (PVCs).</simpara>
</listitem>
<listitem>
<simpara>You can use state migration to migrate a namespace within the same cluster.</simpara>
</listitem>
</itemizedlist>
<simpara>Most cluster-scoped resources are not yet handled by MTC. If your applications require cluster-scoped resources, you might have to create them manually on the target cluster.</simpara>
<simpara>During migration, MTC preserves the following namespace annotations:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>openshift.io/sa.scc.mcs</literal></simpara>
</listitem>
<listitem>
<simpara><literal>openshift.io/sa.scc.supplemental-groups</literal></simpara>
</listitem>
<listitem>
<simpara><literal>openshift.io/sa.scc.uid-range</literal></simpara>
</listitem>
</itemizedlist>
<simpara>These annotations preserve the UID range, ensuring that the containers retain their file system permissions on the target cluster. There is a risk that the migrated UIDs could duplicate UIDs within an existing or future namespace on the target cluster.</simpara>
<section xml:id="migration-prerequisites_migrating-applications-3-4">
<title>Migration prerequisites</title>
<itemizedlist>
<listitem>
<simpara>You must be logged in as a user with <literal>cluster-admin</literal> privileges on all clusters.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Direct image migration</title>
<listitem>
<simpara>You must ensure that the secure OpenShift image registry of the source cluster is exposed.</simpara>
</listitem>
<listitem>
<simpara>You must create a route to the exposed registry.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Direct volume migration</title>
<listitem>
<simpara>If your clusters use proxies, you must configure an Stunnel TCP proxy.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Internal images</title>
<listitem>
<simpara>If your application uses internal images from the <literal>openshift</literal> namespace, you must ensure that the required versions of the images are present on the target cluster.</simpara>
<simpara>You can manually update an image stream tag in order to use a deprecated OpenShift Container Platform 3 image on an OpenShift Container Platform 4.14 cluster.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Clusters</title>
<listitem>
<simpara>The source cluster must be upgraded to the latest MTC z-stream release.</simpara>
</listitem>
<listitem>
<simpara>The MTC version must be the same on all clusters.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Network</title>
<listitem>
<simpara>The clusters have unrestricted network access to each other and to the replication repository.</simpara>
</listitem>
<listitem>
<simpara>If you copy the persistent volumes with <literal>move</literal>, the clusters must have unrestricted network access to the remote volumes.</simpara>
</listitem>
<listitem>
<simpara>You must enable the following ports on an OpenShift Container Platform 3 cluster:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>8443</literal> (API server)</simpara>
</listitem>
<listitem>
<simpara><literal>443</literal> (routes)</simpara>
</listitem>
<listitem>
<simpara><literal>53</literal> (DNS)</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>You must enable the following ports on an OpenShift Container Platform 4 cluster:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>6443</literal> (API server)</simpara>
</listitem>
<listitem>
<simpara><literal>443</literal> (routes)</simpara>
</listitem>
<listitem>
<simpara><literal>53</literal> (DNS)</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>You must enable port <literal>443</literal> on the replication repository if you are using TLS.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Persistent volumes (PVs)</title>
<listitem>
<simpara>The PVs must be valid.</simpara>
</listitem>
<listitem>
<simpara>The PVs must be bound to persistent volume claims.</simpara>
</listitem>
<listitem>
<simpara>If you use snapshots to copy the PVs, the following additional prerequisites apply:</simpara>
<itemizedlist>
<listitem>
<simpara>The cloud provider must support snapshots.</simpara>
</listitem>
<listitem>
<simpara>The PVs must have the same cloud provider.</simpara>
</listitem>
<listitem>
<simpara>The PVs must be located in the same geographic region.</simpara>
</listitem>
<listitem>
<simpara>The PVs must have the same storage class.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<bridgehead xml:id="additional-resources-for-migration-prerequisites_migrating-applications-3-4" role="_additional-resources" renderas="sect3">Additional resources for migration prerequisites</bridgehead>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://docs.openshift.com/container-platform/3.11/install_config/registry/securing_and_exposing_registry.html#exposing-the-registry">Manually exposing a secure registry for OpenShift Container Platform 3</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="migration-updating-deprecated-internal-images_troubleshooting-3-4">Updating deprecated internal images</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="migrating-applications-mtc-web-console_migrating-applications-3-4">
<title>Migrating your applications by using the MTC web console</title>
<simpara>You can configure clusters and a replication repository by using the MTC web console. Then, you can create and run a migration plan.</simpara>
<section xml:id="migration-launching-cam_migrating-applications-3-4">
<title>Launching the MTC web console</title>
<simpara>You can launch the Migration Toolkit for Containers (MTC) web console in a browser.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The MTC web console must have network access to the OpenShift Container Platform web console.</simpara>
</listitem>
<listitem>
<simpara>The MTC web console must have network access to the OAuth authorization server.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Log in to the OpenShift Container Platform cluster on which you have installed MTC.</simpara>
</listitem>
<listitem>
<simpara>Obtain the MTC web console URL by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get -n openshift-migration route/migration -o go-template='https://{{ .spec.host }}'</programlisting>
<simpara>The output resembles the following: <literal>https://migration-openshift-migration.apps.cluster.openshift.com</literal>.</simpara>
</listitem>
<listitem>
<simpara>Launch a browser and navigate to the MTC web console.</simpara>
<note>
<simpara>If you try to access the MTC web console immediately after installing the Migration Toolkit for Containers Operator, the console might not load because the Operator is still configuring the cluster. Wait a few minutes and retry.</simpara>
</note>
</listitem>
<listitem>
<simpara>If you are using self-signed CA certificates, you will be prompted to accept the CA certificate of the source cluster API server. The web page guides you through the process of accepting the remaining certificates.</simpara>
</listitem>
<listitem>
<simpara>Log in with your OpenShift Container Platform <emphasis role="strong">username</emphasis> and <emphasis role="strong">password</emphasis>.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="migration-adding-cluster-to-cam_migrating-applications-3-4">
<title>Adding a cluster to the MTC web console</title>
<simpara>You can add a cluster to the Migration Toolkit for Containers (MTC) web console.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>If you are using Azure snapshots to copy data:</simpara>
<itemizedlist>
<listitem>
<simpara>You must specify the Azure resource group name for the cluster.</simpara>
</listitem>
<listitem>
<simpara>The clusters must be in the same Azure resource group.</simpara>
</listitem>
<listitem>
<simpara>The clusters must be in the same geographic location.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>If you are using direct image migration, you must expose a route to the image registry of the source cluster.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Log in to the cluster.</simpara>
</listitem>
<listitem>
<simpara>Obtain the <literal>migration-controller</literal> service account token:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc sa get-token migration-controller -n openshift-migration</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJtaWciLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlY3JldC5uYW1lIjoibWlnLXRva2VuLWs4dDJyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6Im1pZyIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImE1YjFiYWMwLWMxYmYtMTFlOS05Y2NiLTAyOWRmODYwYjMwOCIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDptaWc6bWlnIn0.xqeeAINK7UXpdRqAtOj70qhBJPeMwmgLomV9iFxr5RoqUgKchZRG2J2rkqmPm6vr7K-cm7ibD1IBpdQJCcVDuoHYsFgV4mp9vgOfn9osSDp2TGikwNz4Az95e81xnjVUmzh-NjDsEpw71DH92iHV_xt2sTwtzftS49LpPW2LjrV0evtNBP_t_RfskdArt5VSv25eORl7zScqfe1CiMkcVbf2UqACQjo3LbkpfN26HAioO2oH0ECPiRzT0Xyh-KwFutJLS9Xgghyw-LD9kPKcE_xbbJ9Y4Rqajh7WdPYuB0Jd9DPVrslmzK-F6cgHHYoZEv0SvLQi-PO0rpDrcjOEQQ</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>In the MTC web console, click <emphasis role="strong">Clusters</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Add cluster</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Fill in the following fields:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Cluster name</emphasis>: The cluster name can contain lower-case letters (<literal>a-z</literal>) and numbers (<literal>0-9</literal>). It must not contain spaces or international characters.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">URL</emphasis>: Specify the API server URL, for example, <literal>https://&lt;www.example.com&gt;:8443</literal>.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Service account token</emphasis>: Paste the <literal>migration-controller</literal> service account token.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Exposed route host to image registry</emphasis>: If you are using direct image migration, specify the exposed route to the image registry of the source cluster.</simpara>
<simpara>To create the route, run the following command:</simpara>
<itemizedlist>
<listitem>
<simpara>For OpenShift Container Platform 3:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create route passthrough --service=docker-registry --port=5000 -n default</programlisting>
</listitem>
<listitem>
<simpara>For OpenShift Container Platform 4:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create route passthrough --service=image-registry --port=5000 -n openshift-image-registry</programlisting>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">Azure cluster</emphasis>: You must select this option if you use Azure snapshots to copy your data.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Azure resource group</emphasis>: This field is displayed if <emphasis role="strong">Azure cluster</emphasis> is selected. Specify the Azure resource group.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Require SSL verification</emphasis>: Optional: Select this option to verify SSL connections to the cluster.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">CA bundle file</emphasis>: This field is displayed if <emphasis role="strong">Require SSL verification</emphasis> is selected. If you created a custom CA certificate bundle file for self-signed certificates, click <emphasis role="strong">Browse</emphasis>, select the CA bundle file, and upload it.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Add cluster</emphasis>.</simpara>
<simpara>The cluster appears in the <emphasis role="strong">Clusters</emphasis> list.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="migration-adding-replication-repository-to-cam_migrating-applications-3-4">
<title>Adding a replication repository to the MTC web console</title>
<simpara>You can add an object storage as a replication repository to the Migration Toolkit for Containers (MTC) web console.</simpara>
<simpara>MTC supports the following storage providers:</simpara>
<itemizedlist>
<listitem>
<simpara>Amazon Web Services (AWS) S3</simpara>
</listitem>
<listitem>
<simpara>Multi-Cloud Object Gateway (MCG)</simpara>
</listitem>
<listitem>
<simpara>Generic S3 object storage, for example, Minio or Ceph S3</simpara>
</listitem>
<listitem>
<simpara>Google Cloud Provider (GCP)</simpara>
</listitem>
<listitem>
<simpara>Microsoft Azure Blob</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must configure the object storage as a replication repository.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the MTC web console, click <emphasis role="strong">Replication repositories</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Add repository</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select a <emphasis role="strong">Storage provider type</emphasis> and fill in the following fields:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">AWS</emphasis> for S3 providers, including AWS and MCG:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Replication repository name</emphasis>: Specify the replication repository name in the MTC web console.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">S3 bucket name</emphasis>: Specify the name of the S3 bucket.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">S3 bucket region</emphasis>: Specify the S3 bucket region. <emphasis role="strong">Required</emphasis> for AWS S3. <emphasis role="strong">Optional</emphasis> for some S3 providers. Check the product documentation of your S3 provider for expected values.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">S3 endpoint</emphasis>: Specify the URL of the S3 service, not the bucket, for example, <literal>https://&lt;s3-storage.apps.cluster.com&gt;</literal>. <emphasis role="strong">Required</emphasis> for a generic S3 provider. You must use the <literal>https://</literal> prefix.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">S3 provider access key</emphasis>: Specify the <literal>&lt;AWS_SECRET_ACCESS_KEY&gt;</literal> for AWS or the S3 provider access key for MCG and other S3 providers.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">S3 provider secret access key</emphasis>: Specify the <literal>&lt;AWS_ACCESS_KEY_ID&gt;</literal> for AWS or the S3 provider secret access key for MCG and other S3 providers.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Require SSL verification</emphasis>: Clear this checkbox if you are using a generic S3 provider.</simpara>
</listitem>
<listitem>
<simpara>If you created a custom CA certificate bundle for self-signed certificates, click <emphasis role="strong">Browse</emphasis> and browse to the Base64-encoded file.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">GCP</emphasis>:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Replication repository name</emphasis>: Specify the replication repository name in the MTC web console.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">GCP bucket name</emphasis>: Specify the name of the GCP bucket.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">GCP credential JSON blob</emphasis>: Specify the string in the <literal>credentials-velero</literal> file.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">Azure</emphasis>:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Replication repository name</emphasis>: Specify the replication repository name in the MTC web console.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Azure resource group</emphasis>: Specify the resource group of the Azure Blob storage.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Azure storage account name</emphasis>: Specify the Azure Blob storage account name.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Azure credentials - INI file contents</emphasis>: Specify the string in the <literal>credentials-velero</literal> file.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Add repository</emphasis> and wait for connection validation.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Close</emphasis>.</simpara>
<simpara>The new repository appears in the <emphasis role="strong">Replication repositories</emphasis> list.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="migration-creating-migration-plan-cam_migrating-applications-3-4">
<title>Creating a migration plan in the MTC web console</title>
<simpara>You can create a migration plan in the Migration Toolkit for Containers (MTC) web console.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must be logged in as a user with <literal>cluster-admin</literal> privileges on all clusters.</simpara>
</listitem>
<listitem>
<simpara>You must ensure that the same MTC version is installed on all clusters.</simpara>
</listitem>
<listitem>
<simpara>You must add the clusters and the replication repository to the MTC web console.</simpara>
</listitem>
<listitem>
<simpara>If you want to use the <emphasis>move</emphasis> data copy method to migrate a persistent volume (PV), the source and target clusters must have uninterrupted network access to the remote volume.</simpara>
</listitem>
<listitem>
<simpara>If you want to use direct image migration, you must specify the exposed route to the image registry of the source cluster. This can be done by using the MTC web console or by updating the <literal>MigCluster</literal> custom resource manifest.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the MTC web console, click <emphasis role="strong">Migration plans</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Add migration plan</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Enter the <emphasis role="strong">Plan name</emphasis>.</simpara>
<simpara>The migration plan name must not exceed 253 lower-case alphanumeric characters (<literal>a-z, 0-9</literal>) and must not contain spaces or underscores (<literal>_</literal>).</simpara>
</listitem>
<listitem>
<simpara>Select a <emphasis role="strong">Source cluster</emphasis>, a <emphasis role="strong">Target cluster</emphasis>, and a <emphasis role="strong">Repository</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Next</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select the projects for migration.</simpara>
</listitem>
<listitem>
<simpara>Optional: Click the edit icon beside a project to change the target namespace.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Next</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select a <emphasis role="strong">Migration type</emphasis> for each PV:</simpara>
<itemizedlist>
<listitem>
<simpara>The <emphasis role="strong">Copy</emphasis> option copies the data from the PV of a source cluster to the replication repository and then restores the data on a newly created PV, with similar characteristics, in the target cluster.</simpara>
</listitem>
<listitem>
<simpara>The <emphasis role="strong">Move</emphasis> option unmounts a remote volume, for example, NFS, from the source cluster, creates a PV resource on the target cluster pointing to the remote volume, and then mounts the remote volume on the target cluster. Applications running on the target cluster use the same remote volume that the source cluster was using.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Next</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select a <emphasis role="strong">Copy method</emphasis> for each PV:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Snapshot copy</emphasis> backs up and restores data using the cloud provider&#8217;s snapshot functionality. It is significantly faster than <emphasis role="strong">Filesystem copy</emphasis>.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Filesystem copy</emphasis> backs up the files on the source cluster and restores them on the target cluster.</simpara>
<simpara>The file system copy method is required for direct volume migration.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>You can select <emphasis role="strong">Verify copy</emphasis> to verify data migrated with <emphasis role="strong">Filesystem copy</emphasis>. Data is verified by generating a checksum for each source file and checking the checksum after restoration. Data verification significantly reduces performance.</simpara>
</listitem>
<listitem>
<simpara>Select a <emphasis role="strong">Target storage class</emphasis>.</simpara>
<simpara>If you selected <emphasis role="strong">Filesystem copy</emphasis>, you can change the target storage class.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Next</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">Migration options</emphasis> page, the <emphasis role="strong">Direct image migration</emphasis> option is selected if you specified an exposed image registry route for the source cluster. The <emphasis role="strong">Direct PV migration</emphasis> option is selected if you are migrating data with <emphasis role="strong">Filesystem copy</emphasis>.</simpara>
<simpara>The direct migration options copy images and files directly from the source cluster to the target cluster. This option is much faster than copying images and files from the source cluster to the replication repository and then from the replication repository to the target cluster.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Next</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Optional: Click <emphasis role="strong">Add Hook</emphasis> to add a hook to the migration plan.</simpara>
<simpara>A hook runs custom code. You can add up to four hooks to a single migration plan. Each hook runs during a different migration step.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Enter the name of the hook to display in the web console.</simpara>
</listitem>
<listitem>
<simpara>If the hook is an Ansible playbook, select <emphasis role="strong">Ansible playbook</emphasis> and click <emphasis role="strong">Browse</emphasis> to upload the playbook or paste the contents of the playbook in the field.</simpara>
</listitem>
<listitem>
<simpara>Optional: Specify an Ansible runtime image if you are not using the default hook image.</simpara>
</listitem>
<listitem>
<simpara>If the hook is not an Ansible playbook, select <emphasis role="strong">Custom container image</emphasis> and specify the image name and path.</simpara>
<simpara>A custom container image can include Ansible playbooks.</simpara>
</listitem>
<listitem>
<simpara>Select <emphasis role="strong">Source cluster</emphasis> or <emphasis role="strong">Target cluster</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Enter the <emphasis role="strong">Service account name</emphasis> and the <emphasis role="strong">Service account namespace</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select the migration step for the hook:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">preBackup</emphasis>: Before the application workload is backed up on the source cluster</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">postBackup</emphasis>: After the application workload is backed up on the source cluster</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">preRestore</emphasis>: Before the application workload is restored on the target cluster</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">postRestore</emphasis>: After the application workload is restored on the target cluster</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Add</emphasis>.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Finish</emphasis>.</simpara>
<simpara>The migration plan is displayed in the <emphasis role="strong">Migration plans</emphasis> list.</simpara>
</listitem>
</orderedlist>
<bridgehead xml:id="additional-resources-for-persistent-volume-copy-methods_migrating-applications-3-4" role="_additional-resources" renderas="sect3">Additional resources</bridgehead>
<itemizedlist>
<listitem>
<simpara><link linkend="file-system-copy-method_about-mtc-3-4">MTC file system copy method</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="snapshot-copy-method_about-mtc-3-4">MTC snapshot copy method</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="migration-running-migration-plan-cam_migrating-applications-3-4">
<title>Running a migration plan in the MTC web console</title>
<simpara>You can migrate applications and data with the migration plan you created in the Migration Toolkit for Containers (MTC) web console.</simpara>
<note>
<simpara>During migration, MTC sets the reclaim policy of migrated persistent volumes (PVs) to <literal>Retain</literal> on the target cluster.</simpara>
<simpara>The <literal>Backup</literal> custom resource contains a <literal>PVOriginalReclaimPolicy</literal> annotation that indicates the original reclaim policy. You can manually restore the reclaim policy of the migrated PVs.</simpara>
</note>
<formalpara>
<title>Prerequisites</title>
<para>The MTC web console must contain the following:</para>
</formalpara>
<itemizedlist>
<listitem>
<simpara>Source cluster in a <literal>Ready</literal> state</simpara>
</listitem>
<listitem>
<simpara>Target cluster in a <literal>Ready</literal> state</simpara>
</listitem>
<listitem>
<simpara>Replication repository</simpara>
</listitem>
<listitem>
<simpara>Valid migration plan</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Log in to the MTC web console and click <emphasis role="strong">Migration plans</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click the Options menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> next to a migration plan and select one of the following options under <emphasis role="strong">Migration</emphasis>:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Stage</emphasis> copies data from the source cluster to the target cluster without stopping the application.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Cutover</emphasis> stops the transactions on the source cluster and moves the resources to the target cluster.</simpara>
<simpara>Optional: In the <emphasis role="strong">Cutover migration</emphasis> dialog, you can clear the <emphasis role="strong">Halt transactions on the source cluster during migration</emphasis> checkbox.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">State</emphasis> copies selected persistent volume claims (PVCs).</simpara>
<important>
<simpara>Do not use state migration to migrate a namespace between clusters. Use stage or cutover migration instead.</simpara>
</important>
<itemizedlist>
<listitem>
<simpara>Select one or more PVCs in the <emphasis role="strong">State migration</emphasis> dialog and click <emphasis role="strong">Migrate</emphasis>.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>When the migration is complete, verify that the application migrated successfully in the OpenShift Container Platform web console:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Click <emphasis role="strong">Home</emphasis> &#8594; <emphasis role="strong">Projects</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click the migrated project to view its status.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">Routes</emphasis> section, click <emphasis role="strong">Location</emphasis> to verify that the application is functioning, if applicable.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Workloads</emphasis> &#8594; <emphasis role="strong">Pods</emphasis> to verify that the pods are running in the migrated namespace.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Storage</emphasis> &#8594; <emphasis role="strong">Persistent volumes</emphasis> to verify that the migrated persistent volumes are correctly provisioned.</simpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
</section>
</chapter>
<chapter xml:id="advanced-migration-options-3-4">
<title>Advanced migration options</title>
<simpara>You can automate your migrations and modify the <literal>MigPlan</literal> and <literal>MigrationController</literal> custom resources in order to perform large-scale migrations and to improve performance.</simpara>
<section xml:id="migration-terminology_advanced-migration-options-3-4">
<title>Terminology</title>
<table frame="all" rowsep="1" colsep="1">
<title>MTC terminology</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Term</entry>
<entry align="left" valign="top">Definition</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Source cluster</simpara></entry>
<entry align="left" valign="top"><simpara>Cluster from which the applications are migrated.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Destination cluster<superscript>[1]</superscript></simpara></entry>
<entry align="left" valign="top"><simpara>Cluster to which the applications are migrated.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Replication repository</simpara></entry>
<entry align="left" valign="top"><simpara>Object storage used for copying images, volumes, and Kubernetes objects during indirect migration or for Kubernetes objects during direct volume migration or direct image migration.</simpara>
<simpara>The replication repository must be accessible to all clusters.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Host cluster</simpara></entry>
<entry align="left" valign="top"><simpara>Cluster on which the <literal>migration-controller</literal> pod and the web console are running. The host cluster is usually the destination cluster but this is not required.</simpara>
<simpara>The host cluster does not require an exposed registry route for direct image migration.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Remote cluster</simpara></entry>
<entry align="left" valign="top"><simpara>A remote cluster is usually the source cluster but this is not required.</simpara>
<simpara>A remote cluster requires a <literal>Secret</literal> custom resource that contains the <literal>migration-controller</literal> service account token.</simpara>
<simpara>A remote cluster requires an exposed secure registry route for direct image migration.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Indirect migration</simpara></entry>
<entry align="left" valign="top"><simpara>Images, volumes, and Kubernetes objects are copied from the source cluster to the replication repository and then from the replication repository to the destination cluster.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Direct volume migration</simpara></entry>
<entry align="left" valign="top"><simpara>Persistent volumes are copied directly from the source cluster to the destination cluster.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Direct image migration</simpara></entry>
<entry align="left" valign="top"><simpara>Images are copied directly from the source cluster to the destination cluster.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Stage migration</simpara></entry>
<entry align="left" valign="top"><simpara>Data is copied to the destination cluster without stopping the application.</simpara>
<simpara>Running a stage migration multiple times reduces the duration of the cutover migration.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Cutover migration</simpara></entry>
<entry align="left" valign="top"><simpara>The application is stopped on the source cluster and its resources are migrated to the destination cluster.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>State migration</simpara></entry>
<entry align="left" valign="top"><simpara>Application state is migrated by copying specific persistent volume claims to the destination cluster.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Rollback migration</simpara></entry>
<entry align="left" valign="top"><simpara>Rollback migration rolls back a completed migration.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara><superscript>1</superscript>  Called the <emphasis>target</emphasis> cluster in the MTC web console.</simpara>
</section>
<section xml:id="migration-migrating-applications-on-prem-to-cloud_advanced-migration-options-3-4">
<title>Migrating an application from on-premises to a cloud-based cluster</title>
<simpara>You can migrate from a source cluster that is behind a firewall to a cloud-based destination cluster by establishing a network tunnel between the two clusters. The <literal>crane tunnel-api</literal> command establishes such a tunnel by creating a VPN tunnel on the source cluster and then connecting to a VPN server running on the destination cluster. The VPN server is exposed to the client using a load balancer address on the destination cluster.</simpara>
<simpara>A service created on the destination cluster exposes the source cluster&#8217;s API to MTC, which is running on the destination cluster.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The system that creates the VPN tunnel must have access and be logged in to both clusters.</simpara>
</listitem>
<listitem>
<simpara>It must be possible to create a load balancer on the destination cluster. Refer to your cloud provider to ensure this is possible.</simpara>
</listitem>
<listitem>
<simpara>Have names prepared to assign to namespaces, on both the source cluster and the destination cluster, in which to run the VPN tunnel. These namespaces should not be created in advance. For information about namespace rules, see https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names.</simpara>
</listitem>
<listitem>
<simpara>When connecting multiple firewall-protected source clusters to the cloud cluster, each source cluster requires its own namespace.</simpara>
</listitem>
<listitem>
<simpara>OpenVPN server is installed on the destination cluster.</simpara>
</listitem>
<listitem>
<simpara>OpenVPN client is installed on the source cluster.</simpara>
</listitem>
<listitem>
<simpara>When configuring the source cluster in MTC, the API URL takes the form of <literal>https://proxied-cluster.&lt;namespace&gt;.svc.cluster.local:8443</literal>.</simpara>
<itemizedlist>
<listitem>
<simpara>If you use the API, see <emphasis>Create a MigCluster CR manifest for each remote cluster</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>If you use the MTC web console, see <emphasis>Migrating your applications using the MTC web console</emphasis>.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>The MTC web console and Migration Controller must be installed on the target cluster.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Install the <literal>crane</literal> utility:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ podman cp $(podman create registry.redhat.io/rhmtc/openshift-migration-controller-rhel8:v1.8):/crane ./</programlisting>
</listitem>
<listitem>
<simpara>Log in remotely to a node on the source cluster and a node on the destination cluster.</simpara>
</listitem>
<listitem>
<simpara>Obtain the cluster context for both clusters after logging in:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc config view</programlisting>
</listitem>
<listitem>
<simpara>Establish a tunnel by entering the following command on the command system:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ crane tunnel-api [--namespace &lt;namespace&gt;] \
      --destination-context &lt;destination-cluster&gt; \
      --source-context &lt;source-cluster&gt;</programlisting>
<simpara>If you don&#8217;t specify a namespace, the command uses the default value <literal>openvpn</literal>.</simpara>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ crane tunnel-api --namespace my_tunnel \
      --destination-context openshift-migration/c131-e-us-east-containers-cloud-ibm-com/admin \
      --source-context default/192-168-122-171-nip-io:8443/admin</programlisting>
<tip>
<simpara>See all available parameters for the <literal>crane tunnel-api</literal> command by entering <literal>crane tunnel-api --help</literal>.</simpara>
</tip>
<simpara>The command generates TSL/SSL Certificates. This process might take several minutes. A message appears when the process completes.</simpara>
<simpara>The OpenVPN server starts on the destination cluster and the OpenVPN client starts on the source cluster.</simpara>
<simpara>After a few minutes, the load balancer resolves on the source node.</simpara>
<tip>
<simpara>You can view the log for the OpenVPN pods to check the status of this process by entering the following commands with root privileges:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># oc get po -n &lt;namespace&gt;</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME            READY     STATUS      RESTARTS    AGE
&lt;pod_name&gt;    2/2       Running     0           44s</programlisting>
</para>
</formalpara>
<programlisting language="terminal" linenumbering="unnumbered"># oc logs -f -n &lt;namespace&gt; &lt;pod_name&gt; -c openvpn</programlisting>
<simpara>When the address of the load balancer is resolved, the message <literal>Initialization Sequence Completed</literal> appears at the end of the log.</simpara>
</tip>
</listitem>
<listitem>
<simpara>On the OpenVPN server, which is on a destination control node, verify that the <literal>openvpn</literal> service and the <literal>proxied-cluster</literal> service are running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get service -n &lt;namespace&gt;</programlisting>
</listitem>
<listitem>
<simpara>On the source node, get the service account (SA) token for the migration controller:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># oc sa get-token -n openshift-migration migration-controller</programlisting>
</listitem>
<listitem>
<simpara>Open the MTC web console and add the source cluster, using the following values:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Cluster name</emphasis>: The source cluster name.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">URL</emphasis>: <literal>proxied-cluster.&lt;namespace&gt;.svc.cluster.local:8443</literal>. If you did not define a value for <literal>&lt;namespace&gt;</literal>, use <literal>openvpn</literal>.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Service account token</emphasis>: The token of the migration controller service account.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Exposed route host to image registry</emphasis>: <literal>proxied-cluster.&lt;namespace&gt;.svc.cluster.local:5000</literal>. If you did not define a value for <literal>&lt;namespace&gt;</literal>, use <literal>openvpn</literal>.</simpara>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
<simpara>After MTC has successfully validated the connection, you can proceed to create and run a migration plan. The namespace for the source cluster should appear in the list of namespaces.</simpara>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara>For information about creating a MigCluster CR manifest for each remote cluster, see <link linkend="migration-migrating-applications-api_advanced-migration-options-3-4">Migrating an application by using the MTC API</link>.</simpara>
</listitem>
<listitem>
<simpara>For information about adding a cluster using the web console, see <link linkend="migrating-applications-mtc-web-console_migrating-applications-3-4">Migrating your applications by using the MTC web console</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="migrating-applications-cli_advanced-migration-options-3-4">
<title>Migrating applications by using the command line</title>
<simpara>You can migrate applications with the MTC API by using the command line interface (CLI) in order to automate the migration.</simpara>
<section xml:id="migration-prerequisites_advanced-migration-options-3-4">
<title>Migration prerequisites</title>
<itemizedlist>
<listitem>
<simpara>You must be logged in as a user with <literal>cluster-admin</literal> privileges on all clusters.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Direct image migration</title>
<listitem>
<simpara>You must ensure that the secure OpenShift image registry of the source cluster is exposed.</simpara>
</listitem>
<listitem>
<simpara>You must create a route to the exposed registry.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Direct volume migration</title>
<listitem>
<simpara>If your clusters use proxies, you must configure an Stunnel TCP proxy.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Internal images</title>
<listitem>
<simpara>If your application uses internal images from the <literal>openshift</literal> namespace, you must ensure that the required versions of the images are present on the target cluster.</simpara>
<simpara>You can manually update an image stream tag in order to use a deprecated OpenShift Container Platform 3 image on an OpenShift Container Platform 4.14 cluster.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Clusters</title>
<listitem>
<simpara>The source cluster must be upgraded to the latest MTC z-stream release.</simpara>
</listitem>
<listitem>
<simpara>The MTC version must be the same on all clusters.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Network</title>
<listitem>
<simpara>The clusters have unrestricted network access to each other and to the replication repository.</simpara>
</listitem>
<listitem>
<simpara>If you copy the persistent volumes with <literal>move</literal>, the clusters must have unrestricted network access to the remote volumes.</simpara>
</listitem>
<listitem>
<simpara>You must enable the following ports on an OpenShift Container Platform 3 cluster:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>8443</literal> (API server)</simpara>
</listitem>
<listitem>
<simpara><literal>443</literal> (routes)</simpara>
</listitem>
<listitem>
<simpara><literal>53</literal> (DNS)</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>You must enable the following ports on an OpenShift Container Platform 4 cluster:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>6443</literal> (API server)</simpara>
</listitem>
<listitem>
<simpara><literal>443</literal> (routes)</simpara>
</listitem>
<listitem>
<simpara><literal>53</literal> (DNS)</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>You must enable port <literal>443</literal> on the replication repository if you are using TLS.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Persistent volumes (PVs)</title>
<listitem>
<simpara>The PVs must be valid.</simpara>
</listitem>
<listitem>
<simpara>The PVs must be bound to persistent volume claims.</simpara>
</listitem>
<listitem>
<simpara>If you use snapshots to copy the PVs, the following additional prerequisites apply:</simpara>
<itemizedlist>
<listitem>
<simpara>The cloud provider must support snapshots.</simpara>
</listitem>
<listitem>
<simpara>The PVs must have the same cloud provider.</simpara>
</listitem>
<listitem>
<simpara>The PVs must be located in the same geographic region.</simpara>
</listitem>
<listitem>
<simpara>The PVs must have the same storage class.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="migration-creating-registry-route-for-dim_advanced-migration-options-3-4">
<title>Creating a registry route for direct image migration</title>
<simpara>For direct image migration, you must create a route to the exposed OpenShift image registry on all remote clusters.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The OpenShift image registry must be exposed to external traffic on all remote clusters.</simpara>
<simpara>The OpenShift Container Platform 4 registry is exposed by default.</simpara>
<simpara>The OpenShift Container Platform 3 registry must be <link xlink:href="https://docs.openshift.com/container-platform/3.11/install_config/registry/securing_and_exposing_registry.html#exposing-the-registry">exposed manually</link>.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To create a route to an OpenShift Container Platform 3 registry, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create route passthrough --service=docker-registry -n default</programlisting>
</listitem>
<listitem>
<simpara>To create a route to an OpenShift Container Platform 4 registry, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create route passthrough --service=image-registry -n openshift-image-registry</programlisting>
</listitem>
</itemizedlist>
</section>
<section xml:id="migration-about-configuring-proxies_advanced-migration-options-3-4">
<title>Proxy configuration</title>
<simpara>For OpenShift Container Platform 4.1 and earlier versions, you must configure proxies in the <literal>MigrationController</literal> custom resource (CR) manifest after you install the Migration Toolkit for Containers Operator because these versions do not support a cluster-wide <literal>proxy</literal> object.</simpara>
<simpara>For OpenShift Container Platform 4.2 to 4.14, the Migration Toolkit for Containers (MTC) inherits the cluster-wide proxy settings. You can change the proxy parameters if you want to override the cluster-wide proxy settings.</simpara>
<section xml:id="direct-volume-migration_advanced-migration-options-3-4">
<title>Direct volume migration</title>
<simpara>Direct Volume Migration (DVM) was introduced in MTC 1.4.2. DVM supports only one proxy. The source cluster cannot access the route of the target cluster if the target cluster is also behind a proxy.</simpara>
<simpara>If you want to perform a DVM from a source cluster behind a proxy, you must configure a TCP proxy that works at the transport layer and forwards the SSL connections transparently without decrypting and re-encrypting them with their own SSL certificates. A Stunnel proxy is an example of such a proxy.</simpara>
<section xml:id="tcp-proxy-setup-for-dvm_advanced-migration-options-3-4">
<title>TCP proxy setup for DVM</title>
<simpara>You can set up a direct connection between the source and the target cluster through a TCP proxy and configure the <literal>stunnel_tcp_proxy</literal> variable in the <literal>MigrationController</literal> CR to use the proxy:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: migration.openshift.io/v1alpha1
kind: MigrationController
metadata:
  name: migration-controller
  namespace: openshift-migration
spec:
  [...]
  stunnel_tcp_proxy: http://username:password@ip:port</programlisting>
<simpara>Direct volume migration (DVM) supports only basic authentication for the proxy. Moreover, DVM works only from behind proxies that can tunnel a TCP connection transparently. HTTP/HTTPS proxies in man-in-the-middle mode do not work. The existing cluster-wide proxies might not support this behavior. As a result, the proxy settings for DVM are intentionally kept different from the usual proxy configuration in MTC.</simpara>
</section>
<section xml:id="why-tcp-proxy-instead-of-an-http-https-proxy_advanced-migration-options-3-4">
<title>Why use a TCP proxy instead of an HTTP/HTTPS proxy?</title>
<simpara>You can enable DVM by running Rsync between the source and the target cluster over an OpenShift route.  Traffic is encrypted using Stunnel, a TCP proxy. The Stunnel running on the source cluster initiates a TLS connection with the target Stunnel and transfers data over an encrypted channel.</simpara>
<simpara>Cluster-wide HTTP/HTTPS proxies in OpenShift are usually configured in man-in-the-middle mode where they negotiate their own TLS session with the outside servers. However, this does not work with Stunnel. Stunnel requires that its TLS session be untouched by the proxy, essentially making the proxy a transparent tunnel which simply forwards the TCP connection as-is. Therefore, you must use a TCP proxy.</simpara>
</section>
<section xml:id="dvm-known-issues_advanced-migration-options-3-4">
<title>Known issue</title>
<formalpara>
<title>Migration fails with error <literal>Upgrade request required</literal></title>
<para>The migration Controller uses the SPDY protocol to execute commands within remote pods. If the remote cluster is behind a proxy or a firewall that does not support the SPDY protocol, the migration controller fails to execute remote commands. The migration fails with the error message <literal>Upgrade request required</literal>.
Workaround: Use a proxy that supports the SPDY protocol.</para>
</formalpara>
<simpara>In addition to supporting the SPDY protocol, the proxy or firewall also must pass the <literal>Upgrade</literal> HTTP header to the API server. The client uses this header to open a websocket connection with the API server. If the <literal>Upgrade</literal> header is blocked by the proxy or firewall, the migration fails with the error message <literal>Upgrade request required</literal>.
Workaround: Ensure that the proxy forwards the <literal>Upgrade</literal> header.</simpara>
</section>
</section>
<section xml:id="tuning-network-policies-for-migrations_advanced-migration-options-3-4">
<title>Tuning network policies for migrations</title>
<simpara>OpenShift supports restricting traffic to or from pods using <emphasis>NetworkPolicy</emphasis> or <emphasis>EgressFirewalls</emphasis> based on the network plugin used by the cluster. If any of the source namespaces involved in a migration use such mechanisms to restrict network traffic to pods, the restrictions might inadvertently stop traffic to Rsync pods during migration.</simpara>
<simpara>Rsync pods running on both the source and the target clusters must connect to each other over an OpenShift Route. Existing <emphasis>NetworkPolicy</emphasis> or <emphasis>EgressNetworkPolicy</emphasis> objects can be configured to automatically exempt Rsync pods from these traffic restrictions.</simpara>
<section xml:id="dvm-network-policy-configuration_advanced-migration-options-3-4">
<title>NetworkPolicy configuration</title>
<section xml:id="egress-traffic-from-rsync-pods_advanced-migration-options-3-4">
<title>Egress traffic from Rsync pods</title>
<simpara>You can use the unique labels of Rsync pods to allow egress traffic to pass from them if the <literal>NetworkPolicy</literal> configuration in the source or destination namespaces blocks this type of traffic. The following policy allows <emphasis role="strong">all</emphasis> egress traffic from Rsync pods in the namespace:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-all-egress-from-rsync-pods
spec:
  podSelector:
    matchLabels:
      owner: directvolumemigration
      app: directvolumemigration-rsync-transfer
  egress:
  - {}
  policyTypes:
  - Egress</programlisting>
</section>
<section xml:id="ingress-traffic-to-rsync-pods_advanced-migration-options-3-4">
<title>Ingress traffic to Rsync pods</title>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-all-egress-from-rsync-pods
spec:
  podSelector:
    matchLabels:
      owner: directvolumemigration
      app: directvolumemigration-rsync-transfer
  ingress:
  - {}
  policyTypes:
  - Ingress</programlisting>
</section>
</section>
<section xml:id="egressnetworkpolicy-config_advanced-migration-options-3-4">
<title>EgressNetworkPolicy configuration</title>
<simpara>The <literal>EgressNetworkPolicy</literal> object or <emphasis>Egress Firewalls</emphasis> are OpenShift constructs designed to block egress traffic leaving the cluster.</simpara>
<simpara>Unlike the <literal>NetworkPolicy</literal> object, the Egress Firewall works at a project level because it applies to all pods in the namespace. Therefore, the unique labels of Rsync pods do not exempt only Rsync pods from the restrictions. However, you can add the CIDR ranges of the source or target cluster to the <emphasis>Allow</emphasis> rule of the policy so that a direct connection can be setup between two clusters.</simpara>
<simpara>Based on which cluster the Egress Firewall is present in, you can add the CIDR range of the other cluster to allow egress traffic between the two:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: network.openshift.io/v1
kind: EgressNetworkPolicy
metadata:
  name: test-egress-policy
  namespace: &lt;namespace&gt;
spec:
  egress:
  - to:
      cidrSelector: &lt;cidr_of_source_or_target_cluster&gt;
    type: Deny</programlisting>
</section>
<section xml:id="choosing-alternate-endpoints-for-data-transfer_advanced-migration-options-3-4">
<title>Choosing alternate endpoints for data transfer</title>
<simpara>By default, DVM uses an OpenShift Container Platform route as an endpoint to transfer PV data to destination clusters. You can choose another type of supported endpoint, if cluster topologies allow.</simpara>
<simpara>For each cluster, you can configure an endpoint by setting the <literal>rsync_endpoint_type</literal> variable on the appropriate <emphasis role="strong">destination</emphasis> cluster in your <literal>MigrationController</literal> CR:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: migration.openshift.io/v1alpha1
kind: MigrationController
metadata:
  name: migration-controller
  namespace: openshift-migration
spec:
  [...]
  rsync_endpoint_type: [NodePort|ClusterIP|Route]</programlisting>
</section>
<section xml:id="configuring-supplemental-groups-for-rsync-pods_advanced-migration-options-3-4">
<title>Configuring supplemental groups for Rsync pods</title>
<simpara>When your PVCs use a shared storage, you can configure the access to that storage by adding supplemental groups to Rsync pod definitions in order for the pods to allow access:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Supplementary groups for Rsync pods</title>
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<thead>
<row>
<entry align="left" valign="top">Variable</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Default</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>src_supplemental_groups</literal></simpara></entry>
<entry align="left" valign="top"><simpara>string</simpara></entry>
<entry align="left" valign="top"><simpara>Not set</simpara></entry>
<entry align="left" valign="top"><simpara>Comma-separated list of supplemental groups for source Rsync pods</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>target_supplemental_groups</literal></simpara></entry>
<entry align="left" valign="top"><simpara>string</simpara></entry>
<entry align="left" valign="top"><simpara>Not set</simpara></entry>
<entry align="left" valign="top"><simpara>Comma-separated list of supplemental groups for target Rsync pods</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<formalpara>
<title>Example usage</title>
<para>The <literal>MigrationController</literal> CR can be updated to set values for these supplemental groups:</para>
</formalpara>
<programlisting language="yaml" linenumbering="unnumbered">spec:
  src_supplemental_groups: "1000,2000"
  target_supplemental_groups: "2000,3000"</programlisting>
</section>
</section>
<section xml:id="migration-configuring-proxies_advanced-migration-options-3-4">
<title>Configuring proxies</title>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must be logged in as a user with <literal>cluster-admin</literal> privileges on all clusters.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Get the <literal>MigrationController</literal> CR manifest:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get migrationcontroller &lt;migration_controller&gt; -n openshift-migration</programlisting>
</listitem>
<listitem>
<simpara>Update the proxy parameters:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: migration.openshift.io/v1alpha1
kind: MigrationController
metadata:
  name: &lt;migration_controller&gt;
  namespace: openshift-migration
...
spec:
  stunnel_tcp_proxy: http://&lt;username&gt;:&lt;password&gt;@&lt;ip&gt;:&lt;port&gt; <co xml:id="CO10-1"/>
  noProxy: example.com <co xml:id="CO10-2"/></programlisting>
<calloutlist>
<callout arearefs="CO10-1">
<para>Stunnel proxy URL for direct volume migration.</para>
</callout>
<callout arearefs="CO10-2">
<para>Comma-separated list of destination domain names, domains, IP addresses, or other network CIDRs to exclude proxying.</para>
</callout>
</calloutlist>
<simpara>Preface a domain with <literal>.</literal> to match subdomains only. For example, <literal>.y.com</literal> matches <literal>x.y.com</literal>, but not <literal>y.com</literal>. Use <literal>*</literal> to bypass proxy for all destinations.
If you scale up workers that are not included in the network defined by the <literal>networking.machineNetwork[].cidr</literal> field from the installation configuration, you must add them to this list to prevent connection issues.</simpara>
<simpara>This field is ignored if neither the <literal>httpProxy</literal> nor the <literal>httpsProxy</literal> field is set.</simpara>
</listitem>
<listitem>
<simpara>Save the manifest as <literal>migration-controller.yaml</literal>.</simpara>
</listitem>
<listitem>
<simpara>Apply the updated manifest:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc replace -f migration-controller.yaml -n openshift-migration</programlisting>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="migration-migrating-applications-api_advanced-migration-options-3-4">
<title>Migrating an application by using the MTC API</title>
<simpara>You can migrate an application from the command line by using the Migration Toolkit for Containers (MTC) API.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>MigCluster</literal> CR manifest for the host cluster:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">$ cat &lt;&lt; EOF | oc apply -f -
apiVersion: migration.openshift.io/v1alpha1
kind: MigCluster
metadata:
  name: &lt;host_cluster&gt;
  namespace: openshift-migration
spec:
  isHostCluster: true
EOF</programlisting>
</listitem>
<listitem>
<simpara>Create a <literal>Secret</literal> object manifest for each remote cluster:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">$ cat &lt;&lt; EOF | oc apply -f -
apiVersion: v1
kind: Secret
metadata:
  name: &lt;cluster_secret&gt;
  namespace: openshift-config
type: Opaque
data:
  saToken: &lt;sa_token&gt; <co xml:id="CO11-1"/>
EOF</programlisting>
<calloutlist>
<callout arearefs="CO11-1">
<para>Specify the base64-encoded <literal>migration-controller</literal> service account (SA) token of the remote cluster. You can obtain the token by running the following command:</para>
</callout>
</calloutlist>
<programlisting language="terminal" linenumbering="unnumbered">$ oc sa get-token migration-controller -n openshift-migration | base64 -w 0</programlisting>
</listitem>
<listitem>
<simpara>Create a <literal>MigCluster</literal> CR manifest for each remote cluster:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">$ cat &lt;&lt; EOF | oc apply -f -
apiVersion: migration.openshift.io/v1alpha1
kind: MigCluster
metadata:
  name: &lt;remote_cluster&gt; <co xml:id="CO12-1"/>
  namespace: openshift-migration
spec:
  exposedRegistryPath: &lt;exposed_registry_route&gt; <co xml:id="CO12-2"/>
  insecure: false <co xml:id="CO12-3"/>
  isHostCluster: false
  serviceAccountSecretRef:
    name: &lt;remote_cluster_secret&gt; <co xml:id="CO12-4"/>
    namespace: openshift-config
  url: &lt;remote_cluster_url&gt; <co xml:id="CO12-5"/>
EOF</programlisting>
<calloutlist>
<callout arearefs="CO12-1">
<para>Specify the <literal>Cluster</literal> CR of the remote cluster.</para>
</callout>
<callout arearefs="CO12-2">
<para>Optional: For direct image migration, specify the exposed registry route.</para>
</callout>
<callout arearefs="CO12-3">
<para>SSL verification is enabled if <literal>false</literal>. CA certificates are not required or checked if <literal>true</literal>.</para>
</callout>
<callout arearefs="CO12-4">
<para>Specify the <literal>Secret</literal> object of the remote cluster.</para>
</callout>
<callout arearefs="CO12-5">
<para>Specify the URL of the remote cluster.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Verify that all clusters are in a <literal>Ready</literal> state:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe cluster &lt;cluster&gt;</programlisting>
</listitem>
<listitem>
<simpara>Create a <literal>Secret</literal> object manifest for the replication repository:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">$ cat &lt;&lt; EOF | oc apply -f -
apiVersion: v1
kind: Secret
metadata:
  namespace: openshift-config
  name: &lt;migstorage_creds&gt;
type: Opaque
data:
  aws-access-key-id: &lt;key_id_base64&gt; <co xml:id="CO13-1"/>
  aws-secret-access-key: &lt;secret_key_base64&gt; <co xml:id="CO13-2"/>
EOF</programlisting>
<calloutlist>
<callout arearefs="CO13-1">
<para>Specify the key ID in base64 format.</para>
</callout>
<callout arearefs="CO13-2">
<para>Specify the secret key in base64 format.</para>
</callout>
</calloutlist>
<simpara>AWS credentials are base64-encoded by default. For other storage providers, you must encode your credentials by running the following command with each key:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ echo -n "&lt;key&gt;" | base64 -w 0 <co xml:id="CO14-1"/></programlisting>
<calloutlist>
<callout arearefs="CO14-1">
<para>Specify the key ID or the secret key. Both keys must be base64-encoded.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create a <literal>MigStorage</literal> CR manifest for the replication repository:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">$ cat &lt;&lt; EOF | oc apply -f -
apiVersion: migration.openshift.io/v1alpha1
kind: MigStorage
metadata:
  name: &lt;migstorage&gt;
  namespace: openshift-migration
spec:
  backupStorageConfig:
    awsBucketName: &lt;bucket&gt; <co xml:id="CO15-1"/>
    credsSecretRef:
      name: &lt;storage_secret&gt; <co xml:id="CO15-2"/>
      namespace: openshift-config
  backupStorageProvider: &lt;storage_provider&gt; <co xml:id="CO15-3"/>
  volumeSnapshotConfig:
    credsSecretRef:
      name: &lt;storage_secret&gt; <co xml:id="CO15-4"/>
      namespace: openshift-config
  volumeSnapshotProvider: &lt;storage_provider&gt; <co xml:id="CO15-5"/>
EOF</programlisting>
<calloutlist>
<callout arearefs="CO15-1">
<para>Specify the bucket name.</para>
</callout>
<callout arearefs="CO15-2">
<para>Specify the <literal>Secrets</literal> CR of the object storage. You must ensure that the credentials stored in the <literal>Secrets</literal> CR of the object storage are correct.</para>
</callout>
<callout arearefs="CO15-3">
<para>Specify the storage provider.</para>
</callout>
<callout arearefs="CO15-4">
<para>Optional: If you are copying data by using snapshots, specify the <literal>Secrets</literal> CR of the object storage. You must ensure that the credentials stored in the <literal>Secrets</literal> CR of the object storage are correct.</para>
</callout>
<callout arearefs="CO15-5">
<para>Optional: If you are copying data by using snapshots, specify the storage provider.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Verify that the <literal>MigStorage</literal> CR is in a <literal>Ready</literal> state:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe migstorage &lt;migstorage&gt;</programlisting>
</listitem>
<listitem>
<simpara>Create a <literal>MigPlan</literal> CR manifest:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">$ cat &lt;&lt; EOF | oc apply -f -
apiVersion: migration.openshift.io/v1alpha1
kind: MigPlan
metadata:
  name: &lt;migplan&gt;
  namespace: openshift-migration
spec:
  destMigClusterRef:
    name: &lt;host_cluster&gt;
    namespace: openshift-migration
  indirectImageMigration: true <co xml:id="CO16-1"/>
  indirectVolumeMigration: true <co xml:id="CO16-2"/>
  migStorageRef:
    name: &lt;migstorage&gt; <co xml:id="CO16-3"/>
    namespace: openshift-migration
  namespaces:
    - &lt;source_namespace_1&gt; <co xml:id="CO16-4"/>
    - &lt;source_namespace_2&gt;
    - &lt;source_namespace_3&gt;:&lt;destination_namespace&gt; <co xml:id="CO16-5"/>
  srcMigClusterRef:
    name: &lt;remote_cluster&gt; <co xml:id="CO16-6"/>
    namespace: openshift-migration
EOF</programlisting>
<calloutlist>
<callout arearefs="CO16-1">
<para>Direct image migration is enabled if <literal>false</literal>.</para>
</callout>
<callout arearefs="CO16-2">
<para>Direct volume migration is enabled if <literal>false</literal>.</para>
</callout>
<callout arearefs="CO16-3">
<para>Specify the name of the <literal>MigStorage</literal> CR instance.</para>
</callout>
<callout arearefs="CO16-4">
<para>Specify one or more source namespaces. By default, the destination namespace has the same name.</para>
</callout>
<callout arearefs="CO16-5">
<para>Specify a destination namespace if it is different from the source namespace.</para>
</callout>
<callout arearefs="CO16-6">
<para>Specify the name of the source cluster <literal>MigCluster</literal> instance.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Verify that the <literal>MigPlan</literal> instance is in a <literal>Ready</literal> state:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe migplan &lt;migplan&gt; -n openshift-migration</programlisting>
</listitem>
<listitem>
<simpara>Create a <literal>MigMigration</literal> CR manifest to start the migration defined in the <literal>MigPlan</literal> instance:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">$ cat &lt;&lt; EOF | oc apply -f -
apiVersion: migration.openshift.io/v1alpha1
kind: MigMigration
metadata:
  name: &lt;migmigration&gt;
  namespace: openshift-migration
spec:
  migPlanRef:
    name: &lt;migplan&gt; <co xml:id="CO17-1"/>
    namespace: openshift-migration
  quiescePods: true <co xml:id="CO17-2"/>
  stage: false <co xml:id="CO17-3"/>
  rollback: false <co xml:id="CO17-4"/>
EOF</programlisting>
<calloutlist>
<callout arearefs="CO17-1">
<para>Specify the <literal>MigPlan</literal> CR name.</para>
</callout>
<callout arearefs="CO17-2">
<para>The pods on the source cluster are stopped before migration if <literal>true</literal>.</para>
</callout>
<callout arearefs="CO17-3">
<para>A stage migration, which copies most of the data without stopping the application, is performed if <literal>true</literal>.</para>
</callout>
<callout arearefs="CO17-4">
<para>A completed migration is rolled back if <literal>true</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Verify the migration by watching the <literal>MigMigration</literal> CR progress:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc watch migmigration &lt;migmigration&gt; -n openshift-migration</programlisting>
<simpara>The output resembles the following:</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">Name:         c8b034c0-6567-11eb-9a4f-0bc004db0fbc
Namespace:    openshift-migration
Labels:       migration.openshift.io/migplan-name=django
Annotations:  openshift.io/touch: e99f9083-6567-11eb-8420-0a580a81020c
API Version:  migration.openshift.io/v1alpha1
Kind:         MigMigration
...
Spec:
  Mig Plan Ref:
    Name:       migplan
    Namespace:  openshift-migration
  Stage:        false
Status:
  Conditions:
    Category:              Advisory
    Last Transition Time:  2021-02-02T15:04:09Z
    Message:               Step: 19/47
    Reason:                InitialBackupCreated
    Status:                True
    Type:                  Running
    Category:              Required
    Last Transition Time:  2021-02-02T15:03:19Z
    Message:               The migration is ready.
    Status:                True
    Type:                  Ready
    Category:              Required
    Durable:               true
    Last Transition Time:  2021-02-02T15:04:05Z
    Message:               The migration registries are healthy.
    Status:                True
    Type:                  RegistriesHealthy
  Itinerary:               Final
  Observed Digest:         7fae9d21f15979c71ddc7dd075cb97061895caac5b936d92fae967019ab616d5
  Phase:                   InitialBackupCreated
  Pipeline:
    Completed:  2021-02-02T15:04:07Z
    Message:    Completed
    Name:       Prepare
    Started:    2021-02-02T15:03:18Z
    Message:    Waiting for initial Velero backup to complete.
    Name:       Backup
    Phase:      InitialBackupCreated
    Progress:
      Backup openshift-migration/c8b034c0-6567-11eb-9a4f-0bc004db0fbc-wpc44: 0 out of estimated total of 0 objects backed up (5s)
    Started:        2021-02-02T15:04:07Z
    Message:        Not started
    Name:           StageBackup
    Message:        Not started
    Name:           StageRestore
    Message:        Not started
    Name:           DirectImage
    Message:        Not started
    Name:           DirectVolume
    Message:        Not started
    Name:           Restore
    Message:        Not started
    Name:           Cleanup
  Start Timestamp:  2021-02-02T15:03:18Z
Events:
  Type    Reason   Age                 From                     Message
  ----    ------   ----                ----                     -------
  Normal  Running  57s                 migmigration_controller  Step: 2/47
  Normal  Running  57s                 migmigration_controller  Step: 3/47
  Normal  Running  57s (x3 over 57s)   migmigration_controller  Step: 4/47
  Normal  Running  54s                 migmigration_controller  Step: 5/47
  Normal  Running  54s                 migmigration_controller  Step: 6/47
  Normal  Running  52s (x2 over 53s)   migmigration_controller  Step: 7/47
  Normal  Running  51s (x2 over 51s)   migmigration_controller  Step: 8/47
  Normal  Ready    50s (x12 over 57s)  migmigration_controller  The migration is ready.
  Normal  Running  50s                 migmigration_controller  Step: 9/47
  Normal  Running  50s                 migmigration_controller  Step: 10/47</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="migration-state-migration-cli_advanced-migration-options-3-4">
<title>State migration</title>
<simpara>You can perform repeatable, state-only migrations by using Migration Toolkit for Containers (MTC) to migrate persistent volume claims (PVCs) that constitute an application&#8217;s state. You migrate specified PVCs by excluding other PVCs from the migration plan. You can map the PVCs to ensure that the source and the target PVCs are synchronized. Persistent volume (PV) data is copied to the target cluster. The PV references are not moved, and the application pods continue to run on the source cluster.</simpara>
<simpara>State migration is specifically designed to be used in conjunction with external CD mechanisms, such as OpenShift Gitops. You can migrate application manifests using GitOps while migrating the state using MTC.</simpara>
<simpara>If you have a CI/CD pipeline, you can migrate stateless components by deploying them on the target cluster. Then you can migrate stateful components by using MTC.</simpara>
<simpara>You can perform a state migration between clusters or within the same cluster.</simpara>
<important>
<simpara>State migration migrates only the components that constitute an application&#8217;s state. If you want to migrate an entire namespace, use stage or cutover migration.</simpara>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The state of the application on the source cluster is persisted in <literal>PersistentVolumes</literal> provisioned through <literal>PersistentVolumeClaims</literal>.</simpara>
</listitem>
<listitem>
<simpara>The manifests of the application are available in a central repository that is accessible from both the source and the target clusters.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Migrate persistent volume data from the source to the target cluster.</simpara>
<simpara>You can perform this step as many times as needed. The source application continues running.</simpara>
</listitem>
<listitem>
<simpara>Quiesce the source application.</simpara>
<simpara>You can do this by setting the replicas of workload resources to <literal>0</literal>, either directly on the source cluster or by updating the manifests in GitHub and re-syncing the Argo CD application.</simpara>
</listitem>
<listitem>
<simpara>Clone application manifests to the target cluster.</simpara>
<simpara>You can use Argo CD to clone the application manifests to the target cluster.</simpara>
</listitem>
<listitem>
<simpara>Migrate the remaining volume data from the source to the target cluster.</simpara>
<simpara>Migrate any new data created by the application during the state migration process by performing a final data migration.</simpara>
</listitem>
<listitem>
<simpara>If the cloned application is in a quiesced state, unquiesce it.</simpara>
</listitem>
<listitem>
<simpara>Switch the DNS record to the target cluster to re-direct user traffic to the migrated application.</simpara>
</listitem>
</orderedlist>
<note>
<simpara>MTC 1.6 cannot quiesce applications automatically when performing state migration. It can only migrate PV data. Therefore, you must use your CD mechanisms for quiescing or unquiescing applications.</simpara>
<simpara>MTC 1.7 introduces explicit Stage and Cutover flows. You can use staging to perform initial data transfers as many times as needed. Then you can perform a cutover, in which the source applications are quiesced automatically.</simpara>
</note>
<bridgehead xml:id="additional-resources-for-state-migration_advanced-migration-options-3-4" role="_additional-resources" renderas="sect3">Additional resources</bridgehead>
<itemizedlist>
<listitem>
<simpara>See <link linkend="migration-excluding-pvcs_advanced-migration-options-3-4">Excluding PVCs from migration</link> to select PVCs for state migration.</simpara>
</listitem>
<listitem>
<simpara>See <link linkend="migration-mapping-pvcs_advanced-migration-options-3-4">Mapping PVCs</link> to migrate source PV data to provisioned PVCs on the destination cluster.</simpara>
</listitem>
<listitem>
<simpara>See <link linkend="migration-kubernetes-objects_advanced-migration-options-3-4">Migrating Kubernetes objects</link> to migrate the Kubernetes objects that constitute an application&#8217;s state.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="migration-hooks_advanced-migration-options-3-4">
<title>Migration hooks</title>
<simpara>You can add up to four migration hooks to a single migration plan, with each hook running at a different phase of the migration. Migration hooks perform tasks such as customizing application quiescence, manually migrating unsupported data types, and updating applications after migration.</simpara>
<simpara>A migration hook runs on a source or a target cluster at one of the following migration steps:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>PreBackup</literal>: Before resources are backed up on the source cluster.</simpara>
</listitem>
<listitem>
<simpara><literal>PostBackup</literal>: After resources are backed up on the source cluster.</simpara>
</listitem>
<listitem>
<simpara><literal>PreRestore</literal>: Before resources are restored on the target cluster.</simpara>
</listitem>
<listitem>
<simpara><literal>PostRestore</literal>: After resources are restored on the target cluster.</simpara>
</listitem>
</itemizedlist>
<simpara>You can create a hook by creating an Ansible playbook that runs with the default Ansible image or with a custom hook container.</simpara>
<formalpara>
<title>Ansible playbook</title>
<para>The Ansible playbook is mounted on a hook container as a config map. The hook container runs as a job, using the cluster, service account, and namespace specified in the <literal>MigPlan</literal> custom resource. The job continues to run until it reaches the default limit of 6 retries or a successful completion. This continues even if the initial pod is evicted or killed.</para>
</formalpara>
<simpara>The default Ansible runtime image is <literal>registry.redhat.io/rhmtc/openshift-migration-hook-runner-rhel7:1.8</literal>. This image is based on the Ansible Runner image and includes <literal>python-openshift</literal> for Ansible Kubernetes resources and an updated <literal>oc</literal> binary.</simpara>
<formalpara>
<title>Custom hook container</title>
<para>You can use a custom hook container instead of the default Ansible image.</para>
</formalpara>
<section xml:id="migration-writing-ansible-playbook-hook_advanced-migration-options-3-4">
<title>Writing an Ansible playbook for a migration hook</title>
<simpara>You can write an Ansible playbook to use as a migration hook. The hook is added to a migration plan by using the MTC web console or by specifying values for the <literal>spec.hooks</literal> parameters in the <literal>MigPlan</literal> custom resource (CR) manifest.</simpara>
<simpara>The Ansible playbook is mounted onto a hook container as a config map. The hook container runs as a job, using the cluster, service account, and namespace specified in the <literal>MigPlan</literal> CR. The hook container uses a specified service account token so that the tasks do not require authentication before they run in the cluster.</simpara>
<section xml:id="migration-writing-ansible-playbook-hook-ansible-modules_advanced-migration-options-3-4">
<title>Ansible modules</title>
<simpara>You can use the Ansible <literal>shell</literal> module to run <literal>oc</literal> commands.</simpara>
<formalpara>
<title>Example <literal>shell</literal> module</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">- hosts: localhost
  gather_facts: false
  tasks:
  - name: get pod name
    shell: oc get po --all-namespaces</programlisting>
</para>
</formalpara>
<simpara>You can use <literal>kubernetes.core</literal> modules, such as <literal>k8s_info</literal>, to interact with Kubernetes resources.</simpara>
<formalpara>
<title>Example <literal>k8s_facts</literal> module</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">- hosts: localhost
  gather_facts: false
  tasks:
  - name: Get pod
    k8s_info:
      kind: pods
      api: v1
      namespace: openshift-migration
      name: "{{ lookup( 'env', 'HOSTNAME') }}"
    register: pods

  - name: Print pod name
    debug:
      msg: "{{ pods.resources[0].metadata.name }}"</programlisting>
</para>
</formalpara>
<simpara>You can use the <literal>fail</literal> module to produce a non-zero exit status in cases where a non-zero exit status would not normally be produced, ensuring that the success or failure of a hook is detected. Hooks run as jobs and the success or failure status of a hook is based on the exit status of the job container.</simpara>
<formalpara>
<title>Example <literal>fail</literal> module</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">- hosts: localhost
  gather_facts: false
  tasks:
  - name: Set a boolean
    set_fact:
      do_fail: true

  - name: "fail"
    fail:
      msg: "Cause a failure"
    when: do_fail</programlisting>
</para>
</formalpara>
</section>
<section xml:id="migration-writing-ansible-playbook-hook-environment-variables_advanced-migration-options-3-4">
<title>Environment variables</title>
<simpara>The <literal>MigPlan</literal> CR name and migration namespaces are passed as environment variables to the hook container. These variables are accessed by using the <literal>lookup</literal> plugin.</simpara>
<formalpara>
<title>Example environment variables</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">- hosts: localhost
  gather_facts: false
  tasks:
  - set_fact:
      namespaces: "{{ (lookup( 'env', 'MIGRATION_NAMESPACES')).split(',') }}"

  - debug:
      msg: "{{ item }}"
    with_items: "{{ namespaces }}"

  - debug:
      msg: "{{ lookup( 'env', 'MIGRATION_PLAN_NAME') }}"</programlisting>
</para>
</formalpara>
</section>
</section>
</section>
<section xml:id="migration-plan-options_advanced-migration-options-3-4">
<title>Migration plan options</title>
<simpara>You can exclude, edit, and map components in the <literal>MigPlan</literal> custom resource (CR).</simpara>
<section xml:id="migration-excluding-resources_advanced-migration-options-3-4">
<title>Excluding resources</title>
<simpara>You can exclude resources, for example, image streams, persistent volumes (PVs), or subscriptions, from a Migration Toolkit for Containers (MTC) migration plan to reduce the resource load for migration or to migrate images or PVs with a different tool.</simpara>
<simpara>By default, the MTC excludes service catalog resources and Operator Lifecycle Manager (OLM) resources from migration. These resources are parts of the service catalog API group and the OLM API group, neither of which is supported for migration at this time.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>MigrationController</literal> custom resource manifest:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit migrationcontroller &lt;migration_controller&gt; -n openshift-migration</programlisting>
</listitem>
<listitem>
<simpara>Update the <literal>spec</literal> section by adding parameters to exclude specific resources. For those resources that do not have their own exclusion parameters, add the <literal>additional_excluded_resources</literal> parameter:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: migration.openshift.io/v1alpha1
kind: MigrationController
metadata:
  name: migration-controller
  namespace: openshift-migration
spec:
  disable_image_migration: true <co xml:id="CO18-1"/>
  disable_pv_migration: true <co xml:id="CO18-2"/>
  additional_excluded_resources: <co xml:id="CO18-3"/>
  - resource1
  - resource2
  ...</programlisting>
<calloutlist>
<callout arearefs="CO18-1">
<para>Add <literal>disable_image_migration: true</literal> to exclude image streams from the migration. <literal>imagestreams</literal> is added to the <literal>excluded_resources</literal> list in <literal>main.yml</literal> when the <literal>MigrationController</literal> pod restarts.</para>
</callout>
<callout arearefs="CO18-2">
<para>Add <literal>disable_pv_migration: true</literal> to exclude PVs from the migration plan. <literal>persistentvolumes</literal> and <literal>persistentvolumeclaims</literal> are added to the <literal>excluded_resources</literal> list in <literal>main.yml</literal> when the <literal>MigrationController</literal> pod restarts. Disabling PV migration also disables PV discovery when you create the migration plan.</para>
</callout>
<callout arearefs="CO18-3">
<para>You can add OpenShift Container Platform resources that you want to exclude to the <literal>additional_excluded_resources</literal> list.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Wait two minutes for the <literal>MigrationController</literal> pod to restart so that the changes are applied.</simpara>
</listitem>
<listitem>
<simpara>Verify that the resource is excluded:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get deployment -n openshift-migration migration-controller -o yaml | grep EXCLUDED_RESOURCES -A1</programlisting>
<simpara>The output contains the excluded resources:</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">name: EXCLUDED_RESOURCES
value:
resource1,resource2,imagetags,templateinstances,clusterserviceversions,packagemanifests,subscriptions,servicebrokers,servicebindings,serviceclasses,serviceinstances,serviceplans,imagestreams,persistentvolumes,persistentvolumeclaims</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="migration-mapping-destination-namespaces-in-the-migplan-cr_advanced-migration-options-3-4">
<title>Mapping namespaces</title>
<simpara>If you map namespaces in the <literal>MigPlan</literal> custom resource (CR), you must ensure that the namespaces are not duplicated on the source or the destination clusters because the UID and GID ranges of the namespaces are copied during migration.</simpara>
<formalpara>
<title>Two source namespaces mapped to the same destination namespace</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">spec:
  namespaces:
    - namespace_2
    - namespace_1:namespace_2</programlisting>
</para>
</formalpara>
<simpara>If you want the source namespace to be mapped to a namespace of the same name, you do not need to create a mapping. By default, a source namespace and a target namespace have the same name.</simpara>
<formalpara>
<title>Incorrect namespace mapping</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">spec:
  namespaces:
    - namespace_1:namespace_1</programlisting>
</para>
</formalpara>
<formalpara>
<title>Correct namespace reference</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">spec:
  namespaces:
    - namespace_1</programlisting>
</para>
</formalpara>
</section>
<section xml:id="migration-excluding-pvcs_advanced-migration-options-3-4">
<title>Excluding persistent volume claims</title>
<simpara>You select persistent volume claims (PVCs) for state migration by excluding the PVCs that you do not want to migrate. You exclude PVCs by setting the <literal>spec.persistentVolumes.pvc.selection.action</literal> parameter of the <literal>MigPlan</literal> custom resource (CR) after the persistent volumes (PVs) have been discovered.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara><literal>MigPlan</literal> CR is in a <literal>Ready</literal> state.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Add the <literal>spec.persistentVolumes.pvc.selection.action</literal> parameter to the <literal>MigPlan</literal> CR and set it to <literal>skip</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: migration.openshift.io/v1alpha1
kind: MigPlan
metadata:
  name: &lt;migplan&gt;
  namespace: openshift-migration
spec:
...
  persistentVolumes:
  - capacity: 10Gi
    name: &lt;pv_name&gt;
    pvc:
...
    selection:
      action: skip</programlisting>
</listitem>
</itemizedlist>
</section>
<section xml:id="migration-mapping-pvcs_advanced-migration-options-3-4">
<title>Mapping persistent volume claims</title>
<simpara>You can migrate persistent volume (PV) data from the source cluster to persistent volume claims (PVCs) that are already provisioned in the destination cluster in the <literal>MigPlan</literal> CR by mapping the PVCs. This mapping ensures that the destination PVCs of migrated applications are synchronized with the source PVCs.</simpara>
<simpara>You map PVCs by updating the <literal>spec.persistentVolumes.pvc.name</literal> parameter in the <literal>MigPlan</literal> custom resource (CR) after the PVs have been discovered.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara><literal>MigPlan</literal> CR is in a <literal>Ready</literal> state.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Update the <literal>spec.persistentVolumes.pvc.name</literal> parameter in the <literal>MigPlan</literal> CR:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: migration.openshift.io/v1alpha1
kind: MigPlan
metadata:
  name: &lt;migplan&gt;
  namespace: openshift-migration
spec:
...
  persistentVolumes:
  - capacity: 10Gi
    name: &lt;pv_name&gt;
    pvc:
      name: &lt;source_pvc&gt;:&lt;destination_pvc&gt; <co xml:id="CO19-1"/></programlisting>
<calloutlist>
<callout arearefs="CO19-1">
<para>Specify the PVC on the source cluster and the PVC on the destination cluster. If the destination PVC does not exist, it will be created. You can use this mapping to change the PVC name during migration.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="migration-editing-pvs-in-migplan_advanced-migration-options-3-4">
<title>Editing persistent volume attributes</title>
<simpara>After you create a <literal>MigPlan</literal> custom resource (CR), the <literal>MigrationController</literal> CR discovers the persistent volumes (PVs). The <literal>spec.persistentVolumes</literal> block and the <literal>status.destStorageClasses</literal> block are added to the <literal>MigPlan</literal> CR.</simpara>
<simpara>You can edit the values in the <literal>spec.persistentVolumes.selection</literal> block. If you change values outside the <literal>spec.persistentVolumes.selection</literal> block, the values are overwritten when the <literal>MigPlan</literal> CR is reconciled by the <literal>MigrationController</literal> CR.</simpara>
<note>
<simpara>The default value for the <literal>spec.persistentVolumes.selection.storageClass</literal> parameter is determined by the following logic:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>If the source cluster PV is Gluster or NFS, the default is either <literal>cephfs</literal>, for <literal>accessMode: ReadWriteMany</literal>, or <literal>cephrbd</literal>, for <literal>accessMode: ReadWriteOnce</literal>.</simpara>
</listitem>
<listitem>
<simpara>If the PV is neither Gluster nor NFS <emphasis>or</emphasis> if <literal>cephfs</literal> or <literal>cephrbd</literal> are not available, the default is a storage class for the same provisioner.</simpara>
</listitem>
<listitem>
<simpara>If a storage class for the same provisioner is not available, the default is the default storage class of the destination cluster.</simpara>
</listitem>
</orderedlist>
<simpara>You can change the <literal>storageClass</literal> value to the value of any <literal>name</literal> parameter in the <literal>status.destStorageClasses</literal> block of the <literal>MigPlan</literal> CR.</simpara>
<simpara>If the <literal>storageClass</literal> value is empty, the PV will have no storage class after migration. This option is appropriate if, for example, you want to move the PV to an NFS volume on the destination cluster.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara><literal>MigPlan</literal> CR is in a <literal>Ready</literal> state.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>spec.persistentVolumes.selection</literal> values in the <literal>MigPlan</literal> CR:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: migration.openshift.io/v1alpha1
kind: MigPlan
metadata:
  name: &lt;migplan&gt;
  namespace: openshift-migration
spec:
  persistentVolumes:
  - capacity: 10Gi
    name: pvc-095a6559-b27f-11eb-b27f-021bddcaf6e4
    proposedCapacity: 10Gi
    pvc:
      accessModes:
      - ReadWriteMany
      hasReference: true
      name: mysql
      namespace: mysql-persistent
    selection:
      action: &lt;copy&gt; <co xml:id="CO20-1"/>
      copyMethod: &lt;filesystem&gt; <co xml:id="CO20-2"/>
      verify: true <co xml:id="CO20-3"/>
      storageClass: &lt;gp2&gt; <co xml:id="CO20-4"/>
      accessMode: &lt;ReadWriteMany&gt; <co xml:id="CO20-5"/>
    storageClass: cephfs</programlisting>
<calloutlist>
<callout arearefs="CO20-1">
<para>Allowed values are <literal>move</literal>, <literal>copy</literal>, and <literal>skip</literal>. If only one action is supported, the default value is the supported action. If multiple actions are supported, the default value is <literal>copy</literal>.</para>
</callout>
<callout arearefs="CO20-2">
<para>Allowed values are <literal>snapshot</literal> and <literal>filesystem</literal>. Default value is <literal>filesystem</literal>.</para>
</callout>
<callout arearefs="CO20-3">
<para>The <literal>verify</literal> parameter is displayed if you select the verification option for file system copy in the MTC web console. You can set it to <literal>false</literal>.</para>
</callout>
<callout arearefs="CO20-4">
<para>You can change the default value to the value of any <literal>name</literal> parameter in the <literal>status.destStorageClasses</literal> block of the <literal>MigPlan</literal> CR. If no value is specified, the PV will have no storage class after migration.</para>
</callout>
<callout arearefs="CO20-5">
<para>Allowed values are <literal>ReadWriteOnce</literal> and <literal>ReadWriteMany</literal>. If this value is not specified, the default is the access mode of the source cluster PVC. You can only edit the access mode in the <literal>MigPlan</literal> CR. You cannot edit it by using the MTC web console.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
<bridgehead xml:id="additional-resources-for-editing-pv-attributes_advanced-migration-options-3-4" role="_additional-resources" renderas="sect4">Additional resources</bridgehead>
<itemizedlist>
<listitem>
<simpara>For details about the <literal>move</literal> and <literal>copy</literal> actions, see <link linkend="migration-mtc-workflow_about-mtc-3-4">MTC workflow</link>.</simpara>
</listitem>
<listitem>
<simpara>For details about the <literal>skip</literal> action, see <link linkend="migration-excluding-pvcs_advanced-migration-options-3-4">Excluding PVCs from migration</link>.</simpara>
</listitem>
<listitem>
<simpara>For details about the file system and snapshot copy methods, see <link linkend="migration-understanding-data-copy-methods_about-mtc-3-4">About data copy methods</link>.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="migration-kubernetes-objects_advanced-migration-options-3-4">
<title>Performing a state migration of Kubernetes objects by using the MTC API</title>
<simpara>After you migrate all the PV data, you can use the Migration Toolkit for Containers (MTC) API to perform a one-time state migration of Kubernetes objects that constitute an application.</simpara>
<simpara>You do this by configuring <literal>MigPlan</literal> custom resource (CR) fields to provide a list of Kubernetes resources with an additional label selector to further filter those resources, and then performing a migration by creating a <literal>MigMigration</literal> CR. The <literal>MigPlan</literal> resource is closed after the migration.</simpara>
<note>
<simpara>Selecting Kubernetes resources is an API-only feature. You must update the <literal>MigPlan</literal> CR and create a <literal>MigMigration</literal> CR for it by using the CLI. The MTC web console does not support migrating Kubernetes objects.</simpara>
</note>
<note>
<simpara>After migration, the <literal>closed</literal> parameter of the <literal>MigPlan</literal> CR is set to <literal>true</literal>. You cannot create another <literal>MigMigration</literal> CR for this <literal>MigPlan</literal> CR.</simpara>
</note>
<simpara>You add Kubernetes objects to the <literal>MigPlan</literal> CR by using one of the following options:</simpara>
<itemizedlist>
<listitem>
<simpara>Adding the Kubernetes objects to the <literal>includedResources</literal> section. When the <literal>includedResources</literal> field is specified in the <literal>MigPlan</literal> CR, the plan takes a list of <literal>group-kind</literal> as input. Only resources present in the list are included in the migration.</simpara>
</listitem>
<listitem>
<simpara>Adding the optional <literal>labelSelector</literal> parameter to filter the <literal>includedResources</literal> in the <literal>MigPlan</literal>. When this field is specified, only resources matching the label selector are included in the migration. For example, you can filter a list of <literal>Secret</literal> and <literal>ConfigMap</literal> resources by using the label <literal>app: frontend</literal> as a filter.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Update the <literal>MigPlan</literal> CR to include Kubernetes resources and, optionally, to filter the included resources by adding the <literal>labelSelector</literal> parameter:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>To update the <literal>MigPlan</literal> CR to include Kubernetes resources:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: migration.openshift.io/v1alpha1
kind: MigPlan
metadata:
  name: &lt;migplan&gt;
  namespace: openshift-migration
spec:
  includedResources:
  - kind: &lt;kind&gt; <co xml:id="CO21-1"/>
    group: ""
  - kind: &lt;kind&gt;
    group: ""</programlisting>
<calloutlist>
<callout arearefs="CO21-1">
<para>Specify the Kubernetes object, for example, <literal>Secret</literal> or <literal>ConfigMap</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Optional: To filter the included resources by adding the <literal>labelSelector</literal> parameter:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: migration.openshift.io/v1alpha1
kind: MigPlan
metadata:
  name: &lt;migplan&gt;
  namespace: openshift-migration
spec:
  includedResources:
  - kind: &lt;kind&gt; <co xml:id="CO22-1"/>
    group: ""
  - kind: &lt;kind&gt;
    group: ""
...
  labelSelector:
    matchLabels:
      &lt;label&gt; <co xml:id="CO22-2"/></programlisting>
<calloutlist>
<callout arearefs="CO22-1">
<para>Specify the Kubernetes object, for example, <literal>Secret</literal> or <literal>ConfigMap</literal>.</para>
</callout>
<callout arearefs="CO22-2">
<para>Specify the label of the resources to migrate, for example, <literal>app: frontend</literal>.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create a <literal>MigMigration</literal> CR to migrate the selected Kubernetes resources. Verify that the correct <literal>MigPlan</literal> is referenced in <literal>migPlanRef</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: migration.openshift.io/v1alpha1
kind: MigMigration
metadata:
  generateName: &lt;migplan&gt;
  namespace: openshift-migration
spec:
  migPlanRef:
    name: &lt;migplan&gt;
    namespace: openshift-migration
  stage: false</programlisting>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="migration-controller-options_advanced-migration-options-3-4">
<title>Migration controller options</title>
<simpara>You can edit migration plan limits, enable persistent volume resizing, or enable cached Kubernetes clients in the <literal>MigrationController</literal> custom resource (CR) for large migrations and improved performance.</simpara>
<section xml:id="migration-changing-migration-plan-limits_advanced-migration-options-3-4">
<title>Increasing limits for large migrations</title>
<simpara>You can increase the limits on migration objects and container resources for large migrations with the Migration Toolkit for Containers (MTC).</simpara>
<important>
<simpara>You must test these changes before you perform a migration in a production environment.</simpara>
</important>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>MigrationController</literal> custom resource (CR) manifest:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit migrationcontroller -n openshift-migration</programlisting>
</listitem>
<listitem>
<simpara>Update the following parameters:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">...
mig_controller_limits_cpu: "1" <co xml:id="CO23-1"/>
mig_controller_limits_memory: "10Gi" <co xml:id="CO23-2"/>
...
mig_controller_requests_cpu: "100m" <co xml:id="CO23-3"/>
mig_controller_requests_memory: "350Mi" <co xml:id="CO23-4"/>
...
mig_pv_limit: 100 <co xml:id="CO23-5"/>
mig_pod_limit: 100 <co xml:id="CO23-6"/>
mig_namespace_limit: 10 <co xml:id="CO23-7"/>
...</programlisting>
<calloutlist>
<callout arearefs="CO23-1">
<para>Specifies the number of CPUs available to the <literal>MigrationController</literal> CR.</para>
</callout>
<callout arearefs="CO23-2">
<para>Specifies the amount of memory available to the <literal>MigrationController</literal> CR.</para>
</callout>
<callout arearefs="CO23-3">
<para>Specifies the number of CPU units available for <literal>MigrationController</literal> CR requests. <literal>100m</literal> represents 0.1 CPU units (100 * 1e-3).</para>
</callout>
<callout arearefs="CO23-4">
<para>Specifies the amount of memory available for <literal>MigrationController</literal> CR requests.</para>
</callout>
<callout arearefs="CO23-5">
<para>Specifies the number of persistent volumes that can be migrated.</para>
</callout>
<callout arearefs="CO23-6">
<para>Specifies the number of pods that can be migrated.</para>
</callout>
<callout arearefs="CO23-7">
<para>Specifies the number of namespaces that can be migrated.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create a migration plan that uses the updated parameters to verify the changes.</simpara>
<simpara>If your migration plan exceeds the <literal>MigrationController</literal> CR limits, the MTC console displays a warning message when you save the migration plan.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="migration-enabling-pv-resizing-dvm_advanced-migration-options-3-4">
<title>Enabling persistent volume resizing for direct volume migration</title>
<simpara>You can enable persistent volume (PV) resizing for direct volume migration to avoid running out of disk space on the destination cluster.</simpara>
<simpara>When the disk usage of a PV reaches a configured level, the <literal>MigrationController</literal> custom resource (CR) compares the requested storage capacity of a persistent volume claim (PVC) to its actual provisioned capacity. Then, it calculates the space required on the destination cluster.</simpara>
<simpara>A <literal>pv_resizing_threshold</literal> parameter determines when PV resizing is used. The default threshold is <literal>3%</literal>. This means that PV resizing occurs when the disk usage of a PV is more than <literal>97%</literal>. You can increase this threshold so that PV resizing occurs at a lower disk usage level.</simpara>
<simpara>PVC capacity is calculated according to the following criteria:</simpara>
<itemizedlist>
<listitem>
<simpara>If the requested storage capacity (<literal>spec.resources.requests.storage</literal>) of the PVC is not equal to its actual provisioned capacity (<literal>status.capacity.storage</literal>), the greater value is used.</simpara>
</listitem>
<listitem>
<simpara>If a PV is provisioned through a PVC and then subsequently changed so that its PV and PVC capacities no longer match, the greater value is used.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The PVCs must be attached to one or more running pods so that the <literal>MigrationController</literal> CR can execute commands.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Log in to the host cluster.</simpara>
</listitem>
<listitem>
<simpara>Enable PV resizing by patching the <literal>MigrationController</literal> CR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch migrationcontroller migration-controller -p '{"spec":{"enable_dvm_pv_resizing":true}}' \ <co xml:id="CO24-1"/>
  --type='merge' -n openshift-migration</programlisting>
<calloutlist>
<callout arearefs="CO24-1">
<para>Set the value to <literal>false</literal> to disable PV resizing.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Optional: Update the <literal>pv_resizing_threshold</literal> parameter to increase the threshold:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch migrationcontroller migration-controller -p '{"spec":{"pv_resizing_threshold":41}}' \ <co xml:id="CO25-1"/>
  --type='merge' -n openshift-migration</programlisting>
<calloutlist>
<callout arearefs="CO25-1">
<para>The default value is <literal>3</literal>.</para>
</callout>
</calloutlist>
<simpara>When the threshold is exceeded, the following status message is displayed in the <literal>MigPlan</literal> CR status:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">status:
  conditions:
...
  - category: Warn
    durable: true
    lastTransitionTime: "2021-06-17T08:57:01Z"
    message: 'Capacity of the following volumes will be automatically adjusted to avoid disk capacity issues in the target cluster:  [pvc-b800eb7b-cf3b-11eb-a3f7-0eae3e0555f3]'
    reason: Done
    status: "False"
    type: PvCapacityAdjustmentRequired</programlisting>
<note>
<simpara>For AWS gp2 storage, this message does not appear unless the <literal>pv_resizing_threshold</literal> is 42% or greater because of the way gp2 calculates volume usage and size. (<link xlink:href="https://bugzilla.redhat.com/show_bug.cgi?id=1973148"><emphasis role="strong">BZ#1973148</emphasis></link>)</simpara>
</note>
</listitem>
</orderedlist>
</section>
<section xml:id="migration-enabling-cached-kubernetes-clients_advanced-migration-options-3-4">
<title>Enabling cached Kubernetes clients</title>
<simpara>You can enable cached Kubernetes clients in the <literal>MigrationController</literal> custom resource (CR) for improved performance during migration. The greatest performance benefit is displayed when migrating between clusters in different regions or with significant network latency.</simpara>
<note>
<simpara>Delegated tasks, for example, Rsync backup for direct volume migration or Velero backup and restore, however, do not show improved performance with cached clients.</simpara>
</note>
<simpara>Cached clients require extra memory because the <literal>MigrationController</literal> CR caches all API resources that are required for interacting with <literal>MigCluster</literal> CRs. Requests that are normally sent to the API server are directed to the cache instead. The cache watches the API server for updates.</simpara>
<simpara>You can increase the memory limits and requests of the <literal>MigrationController</literal> CR if <literal>OOMKilled</literal> errors occur after you enable cached clients.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Enable cached clients by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-migration patch migrationcontroller migration-controller --type=json --patch \
  '[{ "op": "replace", "path": "/spec/mig_controller_enable_cache", "value": true}]'</programlisting>
</listitem>
<listitem>
<simpara>Optional: Increase the <literal>MigrationController</literal> CR memory limits by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-migration patch migrationcontroller migration-controller --type=json --patch \
  '[{ "op": "replace", "path": "/spec/mig_controller_limits_memory", "value": &lt;10Gi&gt;}]'</programlisting>
</listitem>
<listitem>
<simpara>Optional: Increase the <literal>MigrationController</literal> CR memory requests by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-migration patch migrationcontroller migration-controller --type=json --patch \
  '[{ "op": "replace", "path": "/spec/mig_controller_requests_memory", "value": &lt;350Mi&gt;}]'</programlisting>
</listitem>
</orderedlist>
</section>
</section>
</chapter>
<chapter xml:id="troubleshooting-3-4">
<title>Troubleshooting</title>
<simpara>This section describes resources for troubleshooting the Migration Toolkit for Containers (MTC).</simpara>
<simpara>For known issues, see the <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/migration_toolkit_for_containers/#mtc-release-notes">MTC release notes</link>.</simpara>
<section xml:id="migration-mtc-workflow_troubleshooting-3-4">
<title>MTC workflow</title>
<simpara>You can migrate Kubernetes resources, persistent volume data, and internal container images to OpenShift Container Platform 4.14 by using the Migration Toolkit for Containers (MTC) web console or the Kubernetes API.</simpara>
<simpara>MTC migrates the following resources:</simpara>
<itemizedlist>
<listitem>
<simpara>A namespace specified in a migration plan.</simpara>
</listitem>
<listitem>
<simpara>Namespace-scoped resources: When the MTC migrates a namespace, it migrates all the objects and resources associated with that namespace, such as services or pods. Additionally, if a resource that exists in the namespace but not at the cluster level depends on a resource that exists at the cluster level, the MTC migrates both resources.</simpara>
<simpara>For example, a security context constraint (SCC) is a resource that exists at the cluster level and a service account (SA) is a resource that exists at the namespace level. If an SA exists in a namespace that the MTC migrates, the MTC automatically locates any SCCs that are linked to the SA and also migrates those SCCs. Similarly, the MTC migrates persistent volumes that are linked to the persistent volume claims of the namespace.</simpara>
<note>
<simpara>Cluster-scoped resources might have to be migrated manually, depending on the resource.</simpara>
</note>
</listitem>
<listitem>
<simpara>Custom resources (CRs) and custom resource definitions (CRDs): MTC automatically migrates CRs and CRDs at the namespace level.</simpara>
</listitem>
</itemizedlist>
<simpara>Migrating an application with the MTC web console involves the following steps:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Install the Migration Toolkit for Containers Operator on all clusters.</simpara>
<simpara>You can install the Migration Toolkit for Containers Operator in a restricted environment with limited or no internet access. The source and target clusters must have network access to each other and to a mirror registry.</simpara>
</listitem>
<listitem>
<simpara>Configure the replication repository, an intermediate object storage that MTC uses to migrate data.</simpara>
<simpara>The source and target clusters must have network access to the replication repository during migration. If you are using a proxy server, you must configure it to allow network traffic between the replication repository and the clusters.</simpara>
</listitem>
<listitem>
<simpara>Add the source cluster to the MTC web console.</simpara>
</listitem>
<listitem>
<simpara>Add the replication repository to the MTC web console.</simpara>
</listitem>
<listitem>
<simpara>Create a migration plan, with one of the following data migration options:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Copy</emphasis>: MTC copies the data from the source cluster to the replication repository, and from the replication repository to the target cluster.</simpara>
<note>
<simpara>If you are using direct image migration or direct volume migration, the images or volumes are copied directly from the source cluster to the target cluster.</simpara>
</note>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/migration-PV-copy.png"/>
</imageobject>
<textobject><phrase>migration PV copy</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<simpara><emphasis role="strong">Move</emphasis>: MTC unmounts a remote volume, for example, NFS, from the source cluster, creates a PV resource on the target cluster pointing to the remote volume, and then mounts the remote volume on the target cluster. Applications running on the target cluster use the same remote volume that the source cluster was using. The remote volume must be accessible to the source and target clusters.</simpara>
<note>
<simpara>Although the replication repository does not appear in this diagram, it is required for migration.</simpara>
</note>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/migration-PV-move.png"/>
</imageobject>
<textobject><phrase>migration PV move</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Run the migration plan, with one of the following options:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Stage</emphasis> copies data to the target cluster without stopping the application.</simpara>
<simpara>A stage migration can be run multiple times so that most of the data is copied to the target before migration. Running one or more stage migrations reduces the duration of the cutover migration.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Cutover</emphasis> stops the application on the source cluster and moves the resources to the target cluster.</simpara>
<simpara>Optional: You can clear the <emphasis role="strong">Halt transactions on the source cluster during migration</emphasis> checkbox.</simpara>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/OCP_3_to_4_App_migration.png"/>
</imageobject>
<textobject><phrase>OCP 3 to 4 App migration</phrase></textobject>
</mediaobject>
</informalfigure>
<bridgehead xml:id="migration-about-mtc-custom-resources_troubleshooting-3-4" renderas="sect3">About MTC custom resources</bridgehead>
<simpara>The Migration Toolkit for Containers (MTC) creates the following custom resources (CRs):</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/migration-architecture.png"/>
</imageobject>
<textobject><phrase>migration architecture diagram</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/darkcircle-1.png" contentwidth="20"/>
</imageobject>
<textobject><phrase>20</phrase></textobject>
</inlinemediaobject> <link xlink:href="https://github.com/konveyor/mig-controller/blob/master/pkg/apis/migration/v1alpha1/migcluster_types.go">MigCluster</link> (configuration, MTC cluster): Cluster definition</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/darkcircle-2.png" contentwidth="20"/>
</imageobject>
<textobject><phrase>20</phrase></textobject>
</inlinemediaobject> <link xlink:href="https://github.com/konveyor/mig-controller/blob/master/pkg/apis/migration/v1alpha1/migstorage_types.go">MigStorage</link> (configuration, MTC cluster): Storage definition</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/darkcircle-3.png" contentwidth="20"/>
</imageobject>
<textobject><phrase>20</phrase></textobject>
</inlinemediaobject> <link xlink:href="https://github.com/konveyor/mig-controller/blob/master/pkg/apis/migration/v1alpha1/migplan_types.go">MigPlan</link> (configuration, MTC cluster): Migration plan</simpara>
<simpara>The <literal>MigPlan</literal> CR describes the source and target clusters, replication repository, and namespaces being migrated. It is associated with 0, 1, or many <literal>MigMigration</literal> CRs.</simpara>
<note>
<simpara>Deleting a <literal>MigPlan</literal> CR deletes the associated <literal>MigMigration</literal> CRs.</simpara>
</note>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/darkcircle-4.png" contentwidth="20"/>
</imageobject>
<textobject><phrase>20</phrase></textobject>
</inlinemediaobject> <link xlink:href="https://github.com/vmware-tanzu/velero/blob/main/pkg/apis/velero/v1/backupstoragelocation_types.go">BackupStorageLocation</link> (configuration, MTC cluster): Location of <literal>Velero</literal> backup objects</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/darkcircle-5.png" contentwidth="20"/>
</imageobject>
<textobject><phrase>20</phrase></textobject>
</inlinemediaobject> <link xlink:href="https://github.com/vmware-tanzu/velero/blob/main/pkg/apis/velero/v1/volume_snapshot_location.go">VolumeSnapshotLocation</link> (configuration, MTC cluster): Location of <literal>Velero</literal> volume snapshots</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/darkcircle-6.png" contentwidth="20"/>
</imageobject>
<textobject><phrase>20</phrase></textobject>
</inlinemediaobject> <link xlink:href="https://github.com/konveyor/mig-controller/blob/master/pkg/apis/migration/v1alpha1/migmigration_types.go">MigMigration</link> (action, MTC cluster): Migration, created every time you stage or migrate data. Each <literal>MigMigration</literal> CR is associated with a <literal>MigPlan</literal> CR.</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/darkcircle-7.png" contentwidth="20"/>
</imageobject>
<textobject><phrase>20</phrase></textobject>
</inlinemediaobject> <link xlink:href="https://github.com/vmware-tanzu/velero/blob/main/pkg/apis/velero/v1/backup.go">Backup</link> (action, source cluster): When you run a migration plan, the <literal>MigMigration</literal> CR creates two <literal>Velero</literal> backup CRs on each source cluster:</simpara>
<itemizedlist>
<listitem>
<simpara>Backup CR #1 for Kubernetes objects</simpara>
</listitem>
<listitem>
<simpara>Backup CR #2 for PV data</simpara>
</listitem>
</itemizedlist>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/darkcircle-8.png" contentwidth="20"/>
</imageobject>
<textobject><phrase>20</phrase></textobject>
</inlinemediaobject> <link xlink:href="https://github.com/vmware-tanzu/velero/blob/main/pkg/apis/velero/v1/restore.go">Restore</link> (action, target cluster): When you run a migration plan, the <literal>MigMigration</literal> CR creates two <literal>Velero</literal> restore CRs on the target cluster:</simpara>
<itemizedlist>
<listitem>
<simpara>Restore CR #1 (using Backup CR #2) for PV data</simpara>
</listitem>
<listitem>
<simpara>Restore CR #2 (using Backup CR #1) for Kubernetes objects</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="migration-mtc-cr-manifests_troubleshooting-3-4">
<title>MTC custom resource manifests</title>
<simpara>Migration Toolkit for Containers (MTC) uses the following custom resource (CR) manifests for migrating applications.</simpara>
<section xml:id="directimagemigration_troubleshooting-3-4">
<title>DirectImageMigration</title>
<simpara>The <literal>DirectImageMigration</literal> CR copies images directly from the source cluster to the destination cluster.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: migration.openshift.io/v1alpha1
kind: DirectImageMigration
metadata:
  labels:
    controller-tools.k8s.io: "1.0"
  name: &lt;direct_image_migration&gt;
spec:
  srcMigClusterRef:
    name: &lt;source_cluster&gt;
    namespace: openshift-migration
  destMigClusterRef:
    name: &lt;destination_cluster&gt;
    namespace: openshift-migration
  namespaces: <co xml:id="CO26-1"/>
    - &lt;source_namespace_1&gt;
    - &lt;source_namespace_2&gt;:&lt;destination_namespace_3&gt; <co xml:id="CO26-2"/></programlisting>
<calloutlist>
<callout arearefs="CO26-1">
<para>One or more namespaces containing images to be migrated. By default, the destination namespace has the same name as the source namespace.</para>
</callout>
<callout arearefs="CO26-2">
<para>Source namespace mapped to a destination namespace with a different name.</para>
</callout>
</calloutlist>
</section>
<section xml:id="directimagestreammigration_troubleshooting-3-4">
<title>DirectImageStreamMigration</title>
<simpara>The <literal>DirectImageStreamMigration</literal> CR copies image stream references directly from the source cluster to the destination cluster.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: migration.openshift.io/v1alpha1
kind: DirectImageStreamMigration
metadata:
  labels:
    controller-tools.k8s.io: "1.0"
  name: &lt;direct_image_stream_migration&gt;
spec:
  srcMigClusterRef:
    name: &lt;source_cluster&gt;
    namespace: openshift-migration
  destMigClusterRef:
    name: &lt;destination_cluster&gt;
    namespace: openshift-migration
  imageStreamRef:
    name: &lt;image_stream&gt;
    namespace: &lt;source_image_stream_namespace&gt;
  destNamespace: &lt;destination_image_stream_namespace&gt;</programlisting>
</section>
<section xml:id="directvolumemigration_troubleshooting-3-4">
<title>DirectVolumeMigration</title>
<simpara>The <literal>DirectVolumeMigration</literal> CR copies persistent volumes (PVs) directly from the source cluster to the destination cluster.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: migration.openshift.io/v1alpha1
kind: DirectVolumeMigration
metadata:
  name: &lt;direct_volume_migration&gt;
  namespace: openshift-migration
spec:
  createDestinationNamespaces: false <co xml:id="CO27-1"/>
  deleteProgressReportingCRs: false <co xml:id="CO27-2"/>
  destMigClusterRef:
    name: &lt;host_cluster&gt; <co xml:id="CO27-3"/>
    namespace: openshift-migration
  persistentVolumeClaims:
  - name: &lt;pvc&gt; <co xml:id="CO27-4"/>
    namespace: &lt;pvc_namespace&gt;
  srcMigClusterRef:
    name: &lt;source_cluster&gt;
    namespace: openshift-migration</programlisting>
<calloutlist>
<callout arearefs="CO27-1">
<para>Set to <literal>true</literal> to create namespaces for the PVs on the destination cluster.</para>
</callout>
<callout arearefs="CO27-2">
<para>Set to <literal>true</literal> to delete <literal>DirectVolumeMigrationProgress</literal> CRs after migration. The default is <literal>false</literal> so that <literal>DirectVolumeMigrationProgress</literal> CRs are retained for troubleshooting.</para>
</callout>
<callout arearefs="CO27-3">
<para>Update the cluster name if the destination cluster is not the host cluster.</para>
</callout>
<callout arearefs="CO27-4">
<para>Specify one or more PVCs to be migrated.</para>
</callout>
</calloutlist>
</section>
<section xml:id="directvolumemigrationprogress_troubleshooting-3-4">
<title>DirectVolumeMigrationProgress</title>
<simpara>The <literal>DirectVolumeMigrationProgress</literal> CR shows the progress of the <literal>DirectVolumeMigration</literal> CR.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: migration.openshift.io/v1alpha1
kind: DirectVolumeMigrationProgress
metadata:
  labels:
    controller-tools.k8s.io: "1.0"
  name: &lt;direct_volume_migration_progress&gt;
spec:
  clusterRef:
    name: &lt;source_cluster&gt;
    namespace: openshift-migration
  podRef:
    name: &lt;rsync_pod&gt;
    namespace: openshift-migration</programlisting>
</section>
<section xml:id="miganalytic_troubleshooting-3-4">
<title>MigAnalytic</title>
<simpara>The <literal>MigAnalytic</literal> CR collects the number of images, Kubernetes resources, and the persistent volume (PV) capacity from an associated <literal>MigPlan</literal> CR.</simpara>
<simpara>You can configure the data that it collects.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: migration.openshift.io/v1alpha1
kind: MigAnalytic
metadata:
  annotations:
    migplan: &lt;migplan&gt;
  name: &lt;miganalytic&gt;
  namespace: openshift-migration
  labels:
    migplan: &lt;migplan&gt;
spec:
  analyzeImageCount: true <co xml:id="CO28-1"/>
  analyzeK8SResources: true <co xml:id="CO28-2"/>
  analyzePVCapacity: true <co xml:id="CO28-3"/>
  listImages: false <co xml:id="CO28-4"/>
  listImagesLimit: 50 <co xml:id="CO28-5"/>
  migPlanRef:
    name: &lt;migplan&gt;
    namespace: openshift-migration</programlisting>
<calloutlist>
<callout arearefs="CO28-1">
<para>Optional: Returns the number of images.</para>
</callout>
<callout arearefs="CO28-2">
<para>Optional: Returns the number, kind, and API version of the Kubernetes resources.</para>
</callout>
<callout arearefs="CO28-3">
<para>Optional: Returns the PV capacity.</para>
</callout>
<callout arearefs="CO28-4">
<para>Returns a list of image names. The default is <literal>false</literal> so that the output is not excessively long.</para>
</callout>
<callout arearefs="CO28-5">
<para>Optional: Specify the maximum number of image names to return if <literal>listImages</literal> is <literal>true</literal>.</para>
</callout>
</calloutlist>
</section>
<section xml:id="migcluster_troubleshooting-3-4">
<title>MigCluster</title>
<simpara>The <literal>MigCluster</literal> CR defines a host, local, or remote cluster.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: migration.openshift.io/v1alpha1
kind: MigCluster
metadata:
  labels:
    controller-tools.k8s.io: "1.0"
  name: &lt;host_cluster&gt; <co xml:id="CO29-1"/>
  namespace: openshift-migration
spec:
  isHostCluster: true <co xml:id="CO29-2"/>
# The 'azureResourceGroup' parameter is relevant only for Microsoft Azure.
  azureResourceGroup: &lt;azure_resource_group&gt; <co xml:id="CO29-3"/>
  caBundle: &lt;ca_bundle_base64&gt; <co xml:id="CO29-4"/>
  insecure: false <co xml:id="CO29-5"/>
  refresh: false <co xml:id="CO29-6"/>
# The 'restartRestic' parameter is relevant for a source cluster.
  restartRestic: true <co xml:id="CO29-7"/>
# The following parameters are relevant for a remote cluster.
  exposedRegistryPath: &lt;registry_route&gt; <co xml:id="CO29-8"/>
  url: &lt;destination_cluster_url&gt; <co xml:id="CO29-9"/>
  serviceAccountSecretRef:
    name: &lt;source_secret&gt; <co xml:id="CO29-10"/>
    namespace: openshift-config</programlisting>
<calloutlist>
<callout arearefs="CO29-1">
<para>Update the cluster name if the <literal>migration-controller</literal> pod is not running on this cluster.</para>
</callout>
<callout arearefs="CO29-2">
<para>The <literal>migration-controller</literal> pod runs on this cluster if <literal>true</literal>.</para>
</callout>
<callout arearefs="CO29-3">
<para>Microsoft Azure only: Specify the resource group.</para>
</callout>
<callout arearefs="CO29-4">
<para>Optional: If you created a certificate bundle for self-signed CA certificates and if the <literal>insecure</literal> parameter value is <literal>false</literal>, specify the base64-encoded certificate bundle.</para>
</callout>
<callout arearefs="CO29-5">
<para>Set to <literal>true</literal> to disable SSL verification.</para>
</callout>
<callout arearefs="CO29-6">
<para>Set to <literal>true</literal> to validate the cluster.</para>
</callout>
<callout arearefs="CO29-7">
<para>Set to <literal>true</literal> to restart the <literal>Restic</literal> pods on the source cluster after the <literal>Stage</literal> pods are created.</para>
</callout>
<callout arearefs="CO29-8">
<para>Remote cluster and direct image migration only: Specify the exposed secure registry path.</para>
</callout>
<callout arearefs="CO29-9">
<para>Remote cluster only: Specify the URL.</para>
</callout>
<callout arearefs="CO29-10">
<para>Remote cluster only: Specify the name of the <literal>Secret</literal> object.</para>
</callout>
</calloutlist>
</section>
<section xml:id="mighook_troubleshooting-3-4">
<title>MigHook</title>
<simpara>The <literal>MigHook</literal> CR defines a migration hook that runs custom code at a specified stage of the migration. You can create up to four migration hooks. Each hook runs during a different phase of the migration.</simpara>
<simpara>You can configure the hook name, runtime duration, a custom image, and the cluster where the hook will run.</simpara>
<simpara>The migration phases and namespaces of the hooks are configured in the <literal>MigPlan</literal> CR.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: migration.openshift.io/v1alpha1
kind: MigHook
metadata:
  generateName: &lt;hook_name_prefix&gt; <co xml:id="CO30-1"/>
  name: &lt;mighook&gt; <co xml:id="CO30-2"/>
  namespace: openshift-migration
spec:
  activeDeadlineSeconds: 1800 <co xml:id="CO30-3"/>
  custom: false <co xml:id="CO30-4"/>
  image: &lt;hook_image&gt; <co xml:id="CO30-5"/>
  playbook: &lt;ansible_playbook_base64&gt; <co xml:id="CO30-6"/>
  targetCluster: source <co xml:id="CO30-7"/></programlisting>
<calloutlist>
<callout arearefs="CO30-1">
<para>Optional: A unique hash is appended to the value for this parameter so that each migration hook has a unique name. You do not need to specify the value of the <literal>name</literal> parameter.</para>
</callout>
<callout arearefs="CO30-2">
<para>Specify the migration hook name, unless you specify the value of the <literal>generateName</literal> parameter.</para>
</callout>
<callout arearefs="CO30-3">
<para>Optional: Specify the maximum number of seconds that a hook can run. The default is <literal>1800</literal>.</para>
</callout>
<callout arearefs="CO30-4">
<para>The hook is a custom image if <literal>true</literal>. The custom image can include Ansible or it can be written in a different programming language.</para>
</callout>
<callout arearefs="CO30-5">
<para>Specify the custom image, for example, <literal>quay.io/konveyor/hook-runner:latest</literal>. Required if <literal>custom</literal> is <literal>true</literal>.</para>
</callout>
<callout arearefs="CO30-6">
<para>Base64-encoded Ansible playbook. Required if <literal>custom</literal> is <literal>false</literal>.</para>
</callout>
<callout arearefs="CO30-7">
<para>Specify the cluster on which the hook will run. Valid values are <literal>source</literal> or <literal>destination</literal>.</para>
</callout>
</calloutlist>
</section>
<section xml:id="migmigration_troubleshooting-3-4">
<title>MigMigration</title>
<simpara>The <literal>MigMigration</literal> CR runs a <literal>MigPlan</literal> CR.</simpara>
<simpara>You can configure a <literal>Migmigration</literal> CR to run a stage or incremental migration, to cancel a migration in progress, or to roll back a completed migration.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: migration.openshift.io/v1alpha1
kind: MigMigration
metadata:
  labels:
    controller-tools.k8s.io: "1.0"
  name: &lt;migmigration&gt;
  namespace: openshift-migration
spec:
  canceled: false <co xml:id="CO31-1"/>
  rollback: false <co xml:id="CO31-2"/>
  stage: false <co xml:id="CO31-3"/>
  quiescePods: true <co xml:id="CO31-4"/>
  keepAnnotations: true <co xml:id="CO31-5"/>
  verify: false <co xml:id="CO31-6"/>
  migPlanRef:
    name: &lt;migplan&gt;
    namespace: openshift-migration</programlisting>
<calloutlist>
<callout arearefs="CO31-1">
<para>Set to <literal>true</literal> to cancel a migration in progress.</para>
</callout>
<callout arearefs="CO31-2">
<para>Set to <literal>true</literal> to roll back a completed migration.</para>
</callout>
<callout arearefs="CO31-3">
<para>Set to <literal>true</literal> to run a stage migration. Data is copied incrementally and the pods on the source cluster are not stopped.</para>
</callout>
<callout arearefs="CO31-4">
<para>Set to <literal>true</literal> to stop the application during migration. The pods on the source cluster are scaled to <literal>0</literal> after the <literal>Backup</literal> stage.</para>
</callout>
<callout arearefs="CO31-5">
<para>Set to <literal>true</literal> to retain the labels and annotations applied during the migration.</para>
</callout>
<callout arearefs="CO31-6">
<para>Set to <literal>true</literal> to check the status of the migrated pods on the destination cluster are checked and to return the names of pods that are not in a <literal>Running</literal> state.</para>
</callout>
</calloutlist>
</section>
<section xml:id="migplan_troubleshooting-3-4">
<title>MigPlan</title>
<simpara>The <literal>MigPlan</literal> CR defines the parameters of a migration plan.</simpara>
<simpara>You can configure destination namespaces, hook phases, and direct or indirect migration.</simpara>
<note>
<simpara>By default, a destination namespace has the same name as the source namespace. If you configure a different destination namespace, you must ensure that the namespaces are not duplicated on the source or the destination clusters because the UID and GID ranges are copied during migration.</simpara>
</note>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: migration.openshift.io/v1alpha1
kind: MigPlan
metadata:
  labels:
    controller-tools.k8s.io: "1.0"
  name: &lt;migplan&gt;
  namespace: openshift-migration
spec:
  closed: false <co xml:id="CO32-1"/>
  srcMigClusterRef:
    name: &lt;source_cluster&gt;
    namespace: openshift-migration
  destMigClusterRef:
    name: &lt;destination_cluster&gt;
    namespace: openshift-migration
  hooks: <co xml:id="CO32-2"/>
    - executionNamespace: &lt;namespace&gt; <co xml:id="CO32-3"/>
      phase: &lt;migration_phase&gt; <co xml:id="CO32-4"/>
      reference:
        name: &lt;hook&gt; <co xml:id="CO32-5"/>
        namespace: &lt;hook_namespace&gt; <co xml:id="CO32-6"/>
      serviceAccount: &lt;service_account&gt; <co xml:id="CO32-7"/>
  indirectImageMigration: true <co xml:id="CO32-8"/>
  indirectVolumeMigration: false <co xml:id="CO32-9"/>
  migStorageRef:
    name: &lt;migstorage&gt;
    namespace: openshift-migration
  namespaces:
    - &lt;source_namespace_1&gt; <co xml:id="CO32-10"/>
    - &lt;source_namespace_2&gt;
    - &lt;source_namespace_3&gt;:&lt;destination_namespace_4&gt; <co xml:id="CO32-11"/>
  refresh: false  <co xml:id="CO32-12"/></programlisting>
<calloutlist>
<callout arearefs="CO32-1">
<para>The migration has completed if <literal>true</literal>. You cannot create another <literal>MigMigration</literal> CR for this <literal>MigPlan</literal> CR.</para>
</callout>
<callout arearefs="CO32-2">
<para>Optional: You can specify up to four migration hooks. Each hook must run during a different migration phase.</para>
</callout>
<callout arearefs="CO32-3">
<para>Optional: Specify the namespace in which the hook will run.</para>
</callout>
<callout arearefs="CO32-4">
<para>Optional: Specify the migration phase during which a hook runs. One hook can be assigned to one phase. Valid values are <literal>PreBackup</literal>, <literal>PostBackup</literal>, <literal>PreRestore</literal>, and <literal>PostRestore</literal>.</para>
</callout>
<callout arearefs="CO32-5">
<para>Optional: Specify the name of the <literal>MigHook</literal> CR.</para>
</callout>
<callout arearefs="CO32-6">
<para>Optional: Specify the namespace of <literal>MigHook</literal> CR.</para>
</callout>
<callout arearefs="CO32-7">
<para>Optional: Specify a service account with <literal>cluster-admin</literal> privileges.</para>
</callout>
<callout arearefs="CO32-8">
<para>Direct image migration is disabled if <literal>true</literal>. Images are copied from the source cluster to the replication repository and from the replication repository to the destination cluster.</para>
</callout>
<callout arearefs="CO32-9">
<para>Direct volume migration is disabled if <literal>true</literal>. PVs are copied from the source cluster to the replication repository and from the replication repository to the destination cluster.</para>
</callout>
<callout arearefs="CO32-10">
<para>Specify one or more source namespaces. If you specify only the source namespace, the destination namespace is the same.</para>
</callout>
<callout arearefs="CO32-11">
<para>Specify the destination namespace if it is different from the source namespace.</para>
</callout>
<callout arearefs="CO32-12">
<para>The <literal>MigPlan</literal> CR is validated if <literal>true</literal>.</para>
</callout>
</calloutlist>
</section>
<section xml:id="migstorage_troubleshooting-3-4">
<title>MigStorage</title>
<simpara>The <literal>MigStorage</literal> CR describes the object storage for the replication repository.</simpara>
<simpara>Amazon Web Services (AWS), Microsoft Azure, Google Cloud Storage, Multi-Cloud Object Gateway, and generic S3-compatible cloud storage are supported.</simpara>
<simpara>AWS and the snapshot copy method have additional parameters.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: migration.openshift.io/v1alpha1
kind: MigStorage
metadata:
  labels:
    controller-tools.k8s.io: "1.0"
  name: &lt;migstorage&gt;
  namespace: openshift-migration
spec:
  backupStorageProvider: &lt;backup_storage_provider&gt; <co xml:id="CO33-1"/>
  volumeSnapshotProvider: &lt;snapshot_storage_provider&gt; <co xml:id="CO33-2"/>
  backupStorageConfig:
    awsBucketName: &lt;bucket&gt; <co xml:id="CO33-3"/>
    awsRegion: &lt;region&gt; <co xml:id="CO33-4"/>
    credsSecretRef:
      namespace: openshift-config
      name: &lt;storage_secret&gt; <co xml:id="CO33-5"/>
    awsKmsKeyId: &lt;key_id&gt; <co xml:id="CO33-6"/>
    awsPublicUrl: &lt;public_url&gt; <co xml:id="CO33-7"/>
    awsSignatureVersion: &lt;signature_version&gt; <co xml:id="CO33-8"/>
  volumeSnapshotConfig:
    awsRegion: &lt;region&gt; <co xml:id="CO33-9"/>
    credsSecretRef:
      namespace: openshift-config
      name: &lt;storage_secret&gt; <co xml:id="CO33-10"/>
  refresh: false <co xml:id="CO33-11"/></programlisting>
<calloutlist>
<callout arearefs="CO33-1">
<para>Specify the storage provider.</para>
</callout>
<callout arearefs="CO33-2">
<para>Snapshot copy method only: Specify the storage provider.</para>
</callout>
<callout arearefs="CO33-3">
<para>AWS only: Specify the bucket name.</para>
</callout>
<callout arearefs="CO33-4">
<para>AWS only: Specify the bucket region, for example, <literal>us-east-1</literal>.</para>
</callout>
<callout arearefs="CO33-5">
<para>Specify the name of the <literal>Secret</literal> object that you created for the storage.</para>
</callout>
<callout arearefs="CO33-6">
<para>AWS only: If you are using the AWS Key Management Service, specify the unique identifier of the key.</para>
</callout>
<callout arearefs="CO33-7">
<para>AWS only: If you granted public access to the AWS bucket, specify the bucket URL.</para>
</callout>
<callout arearefs="CO33-8">
<para>AWS only: Specify the AWS signature version for authenticating requests to the bucket, for example, <literal>4</literal>.</para>
</callout>
<callout arearefs="CO33-9">
<para>Snapshot copy method only: Specify the geographical region of the clusters.</para>
</callout>
<callout arearefs="CO33-10">
<para>Snapshot copy method only: Specify the name of the <literal>Secret</literal> object that you created for the storage.</para>
</callout>
<callout arearefs="CO33-11">
<para>Set to <literal>true</literal> to validate the cluster.</para>
</callout>
</calloutlist>
</section>
</section>
<section xml:id="logs-and-debugging-tools_troubleshooting-3-4">
<title>Logs and debugging tools</title>
<simpara>This section describes logs and debugging tools that you can use for troubleshooting.</simpara>
<section xml:id="migration-viewing-migration-plan-resources_troubleshooting-3-4">
<title>Viewing migration plan resources</title>
<simpara>You can view migration plan resources to monitor a running migration or to troubleshoot a failed migration by using the MTC web console and the command line interface (CLI).</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the MTC web console, click <emphasis role="strong">Migration Plans</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">Migrations</emphasis> number next to a migration plan to view the <emphasis role="strong">Migrations</emphasis> page.</simpara>
</listitem>
<listitem>
<simpara>Click a migration to view the <emphasis role="strong">Migration details</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Expand <emphasis role="strong">Migration resources</emphasis> to view the migration resources and their status in a tree view.</simpara>
<note>
<simpara>To troubleshoot a failed migration, start with a high-level resource that has failed and then work down the resource tree towards the lower-level resources.</simpara>
</note>
</listitem>
<listitem>
<simpara>Click the Options menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> next to a resource and select one of the following options:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Copy <literal>oc describe</literal> command</emphasis> copies the command to your clipboard.</simpara>
<itemizedlist>
<listitem>
<simpara>Log in to the relevant cluster and then run the command.</simpara>
<simpara>The conditions and events of the resource are displayed in YAML format.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">Copy <literal>oc logs</literal> command</emphasis> copies the command to your clipboard.</simpara>
<itemizedlist>
<listitem>
<simpara>Log in to the relevant cluster and then run the command.</simpara>
<simpara>If the resource supports log filtering, a filtered log is displayed.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">View JSON</emphasis> displays the resource data in JSON format in a web browser.</simpara>
<simpara>The data is the same as the output for the <literal>oc get &lt;resource&gt;</literal> command.</simpara>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="migration-viewing-migration-plan-log_troubleshooting-3-4">
<title>Viewing a migration plan log</title>
<simpara>You can view an aggregated log for a migration plan. You use the MTC web console to copy a command to your clipboard and then run the command from the command line interface (CLI).</simpara>
<simpara>The command displays the filtered logs of the following pods:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>Migration Controller</literal></simpara>
</listitem>
<listitem>
<simpara><literal>Velero</literal></simpara>
</listitem>
<listitem>
<simpara><literal>Restic</literal></simpara>
</listitem>
<listitem>
<simpara><literal>Rsync</literal></simpara>
</listitem>
<listitem>
<simpara><literal>Stunnel</literal></simpara>
</listitem>
<listitem>
<simpara><literal>Registry</literal></simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the MTC web console, click <emphasis role="strong">Migration Plans</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">Migrations</emphasis> number next to a migration plan.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">View logs</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click the Copy icon to copy the <literal>oc logs</literal> command to your clipboard.</simpara>
</listitem>
<listitem>
<simpara>Log in to the relevant cluster and enter the command on the CLI.</simpara>
<simpara>The aggregated log for the migration plan is displayed.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="migration-using-mig-log-reader_troubleshooting-3-4">
<title>Using the migration log reader</title>
<simpara>You can use the migration log reader to display a single filtered view of all the migration logs.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Get the <literal>mig-log-reader</literal> pod:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-migration get pods | grep log</programlisting>
</listitem>
<listitem>
<simpara>Enter the following command to display a single migration log:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-migration logs -f &lt;mig-log-reader-pod&gt; -c color <co xml:id="CO34-1"/></programlisting>
<calloutlist>
<callout arearefs="CO34-1">
<para>The <literal>-c plain</literal> option displays the log without colors.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
<section xml:id="migration-accessing-performance-metrics_troubleshooting-3-4">
<title>Accessing performance metrics</title>
<simpara>The <literal>MigrationController</literal> custom resource (CR) records metrics and pulls them into on-cluster monitoring storage. You can query the metrics by using Prometheus Query Language (PromQL) to diagnose migration performance issues. All metrics are reset when the Migration Controller pod restarts.</simpara>
<simpara>You can access the performance metrics and run queries by using the OpenShift Container Platform web console.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the OpenShift Container Platform web console, click <emphasis role="strong">Observe</emphasis> &#8594; <emphasis role="strong">Metrics</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Enter a PromQL query, select a time window to display, and click <emphasis role="strong">Run Queries</emphasis>.</simpara>
<simpara>If your web browser does not display all the results, use the Prometheus console.</simpara>
</listitem>
</orderedlist>
<section xml:id="migration-provided-metrics_troubleshooting-3-4">
<title>Provided metrics</title>
<simpara>The <literal>MigrationController</literal> custom resource (CR) provides metrics for the <literal>MigMigration</literal> CR count and for its API requests.</simpara>
<section xml:id="cam_app_workload_migrations-metric_troubleshooting-3-4">
<title>cam_app_workload_migrations</title>
<simpara>This metric is a count of <literal>MigMigration</literal> CRs over time. It is useful for viewing alongside the <literal>mtc_client_request_count</literal> and <literal>mtc_client_request_elapsed</literal> metrics to collate API request information with migration status changes. This metric is included in Telemetry.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>cam_app_workload_migrations metric</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Queryable label name</entry>
<entry align="left" valign="top">Sample label values</entry>
<entry align="left" valign="top">Label description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>status</simpara></entry>
<entry align="left" valign="top"><simpara><literal>running</literal>, <literal>idle</literal>, <literal>failed</literal>, <literal>completed</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Status of the <literal>MigMigration</literal> CR</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>type</simpara></entry>
<entry align="left" valign="top"><simpara>stage, final</simpara></entry>
<entry align="left" valign="top"><simpara>Type of the <literal>MigMigration</literal> CR</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="mtc_client_request_count-metric_troubleshooting-3-4">
<title>mtc_client_request_count</title>
<simpara>This metric is a cumulative count of Kubernetes API requests that <literal>MigrationController</literal> issued. It is not included in Telemetry.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>mtc_client_request_count metric</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Queryable label name</entry>
<entry align="left" valign="top">Sample label values</entry>
<entry align="left" valign="top">Label description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>cluster</simpara></entry>
<entry align="left" valign="top"><simpara><literal>https://migcluster-url:443</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Cluster that the request was issued against</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>component</simpara></entry>
<entry align="left" valign="top"><simpara><literal>MigPlan</literal>, <literal>MigCluster</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Sub-controller API that issued request</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>function</simpara></entry>
<entry align="left" valign="top"><simpara><literal>(*ReconcileMigPlan).Reconcile</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Function that the request was issued from</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>kind</simpara></entry>
<entry align="left" valign="top"><simpara><literal>SecretList</literal>, <literal>Deployment</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Kubernetes kind the request was issued for</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="mtc_client_request_elapsed-metric_troubleshooting-3-4">
<title>mtc_client_request_elapsed</title>
<simpara>This metric is a cumulative latency, in milliseconds, of Kubernetes API requests that <literal>MigrationController</literal> issued. It is not included in Telemetry.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>mtc_client_request_elapsed metric</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Queryable label name</entry>
<entry align="left" valign="top">Sample label values</entry>
<entry align="left" valign="top">Label description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>cluster</simpara></entry>
<entry align="left" valign="top"><simpara><literal>https://cluster-url.com:443</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Cluster that the request was issued against</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>component</simpara></entry>
<entry align="left" valign="top"><simpara><literal>migplan</literal>, <literal>migcluster</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Sub-controller API that issued request</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>function</simpara></entry>
<entry align="left" valign="top"><simpara><literal>(*ReconcileMigPlan).Reconcile</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Function that the request was issued from</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>kind</simpara></entry>
<entry align="left" valign="top"><simpara><literal>SecretList</literal>, <literal>Deployment</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Kubernetes resource that the request was issued for</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="useful-queries_troubleshooting-3-4">
<title>Useful queries</title>
<simpara>The table lists some helpful queries that can be used for monitoring performance.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Useful queries</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Query</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>mtc_client_request_count</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Number of API requests issued, sorted by request type</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>sum(mtc_client_request_count)</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Total number of API requests issued</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>mtc_client_request_elapsed</literal></simpara></entry>
<entry align="left" valign="top"><simpara>API request latency, sorted by request type</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>sum(mtc_client_request_elapsed)</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Total latency of API requests</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>sum(mtc_client_request_elapsed) / sum(mtc_client_request_count)</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Average latency of API requests</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>mtc_client_request_elapsed / mtc_client_request_count</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Average latency of API requests, sorted by request type</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>cam_app_workload_migrations{status="running"} * 100</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Count of running migrations, multiplied by 100 for easier viewing alongside request counts</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
</section>
</section>
<section xml:id="migration-using-must-gather_troubleshooting-3-4">
<title>Using the must-gather tool</title>
<simpara>You can collect logs, metrics, and information about MTC custom resources by using the <literal>must-gather</literal> tool.</simpara>
<simpara>The <literal>must-gather</literal> data must be attached to all customer cases.</simpara>
<simpara>You can collect data for a one-hour or a 24-hour period and view the data with the Prometheus console.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must be logged in to the OpenShift Container Platform cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You must have the OpenShift CLI (<literal>oc</literal>) installed.</simpara>
</listitem>
<listitem>
<simpara>You must use Red Hat Enterprise Linux (RHEL) 8.x with OADP 1.2.</simpara>
</listitem>
<listitem>
<simpara>You must use Red Hat Enterprise Linux (RHEL) 9.x with OADP 1.3.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to the directory where you want to store the <literal>must-gather</literal> data.</simpara>
</listitem>
<listitem>
<simpara>Run the <literal>oc adm must-gather</literal> command for one of the following data collection options:</simpara>
<itemizedlist>
<listitem>
<simpara>To collect data for the past hour:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>For OADP 1.2, use the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">oc adm must-gather --image=registry.redhat.io/oadp/oadp-mustgather-rhel8:v1.2</programlisting>
</listitem>
<listitem>
<simpara>For OADP 1.3, use the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">oc adm must-gather --image=registry.redhat.io/oadp/oadp-mustgather-rhel9:v1.3</programlisting>
<simpara>The data is saved as <literal>must-gather/must-gather.tar.gz</literal>. You can upload this file to a support case on the <link xlink:href="https://access.redhat.com/">Red Hat Customer Portal</link>.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>To collect data for the past 24 hours:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>For OADP 1.2, use the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">oc adm must-gather --image=registry.redhat.io/oadp/oadp-mustgather-rhel8:v1.2 -- /usr/bin/gather_metrics_dump</programlisting>
</listitem>
<listitem>
<simpara>For OADP 1.3, use the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">oc adm must-gather --image=registry.redhat.io/oadp/oadp-mustgather-rhel9:v1.3 -- /usr/bin/gather_metrics_dump</programlisting>
<simpara>This operation can take a long time. The data is saved as <literal>must-gather/metrics/prom_data.tar.gz</literal>.</simpara>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="migration-debugging-velero-resources_troubleshooting-3-4">
<title>Debugging Velero resources with the Velero CLI tool</title>
<simpara>You can debug <literal>Backup</literal> and <literal>Restore</literal> custom resources (CRs) and retrieve logs with the Velero CLI tool.</simpara>
<simpara>The Velero CLI tool provides more detailed information than the OpenShift CLI tool.</simpara>
<bridgehead xml:id="velero-command-syntax_troubleshooting-3-4" renderas="sect4">Syntax</bridgehead>
<simpara>Use the <literal>oc exec</literal> command to run a Velero CLI command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-migration exec deployment/velero -c velero -- ./velero \
  &lt;backup_restore_cr&gt; &lt;command&gt; &lt;cr_name&gt;</programlisting>
<formalpara>
<title>Example</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-migration exec deployment/velero -c velero -- ./velero \
  backup describe 0e44ae00-5dc3-11eb-9ca8-df7e5254778b-2d8ql</programlisting>
</para>
</formalpara>
<bridgehead xml:id="velero-help-option_troubleshooting-3-4" renderas="sect4">Help option</bridgehead>
<simpara>Use the <literal>velero --help</literal> option to list all Velero CLI commands:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-migration exec deployment/velero -c velero -- ./velero \
  --help</programlisting>
<bridgehead xml:id="velero-describe-command_troubleshooting-3-4" renderas="sect4">Describe command</bridgehead>
<simpara>Use the <literal>velero describe</literal> command to retrieve a summary of warnings and errors associated with a <literal>Backup</literal> or <literal>Restore</literal> CR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-migration exec deployment/velero -c velero -- ./velero \
  &lt;backup_restore_cr&gt; describe &lt;cr_name&gt;</programlisting>
<formalpara>
<title>Example</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-migration exec deployment/velero -c velero -- ./velero \
  backup describe 0e44ae00-5dc3-11eb-9ca8-df7e5254778b-2d8ql</programlisting>
</para>
</formalpara>
<bridgehead xml:id="velero-logs-command_troubleshooting-3-4" renderas="sect4">Logs command</bridgehead>
<simpara>Use the <literal>velero logs</literal> command to retrieve the logs of a <literal>Backup</literal> or <literal>Restore</literal> CR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-migration exec deployment/velero -c velero -- ./velero \
  &lt;backup_restore_cr&gt; logs &lt;cr_name&gt;</programlisting>
<formalpara>
<title>Example</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-migration exec deployment/velero -c velero -- ./velero \
  restore logs ccc7c2d0-6017-11eb-afab-85d0007f5a19-x4lbf</programlisting>
</para>
</formalpara>
</section>
<section xml:id="migration-partial-failure-velero_troubleshooting-3-4">
<title>Debugging a partial migration failure</title>
<simpara>You can debug a partial migration failure warning message by using the Velero CLI to examine the <literal>Restore</literal> custom resource (CR) logs.</simpara>
<simpara>A partial failure occurs when Velero encounters an issue that does not cause a migration to fail. For example, if a custom resource definition (CRD) is missing or if there is a discrepancy between CRD versions on the source and target clusters, the migration completes but the CR is not created on the target cluster.</simpara>
<simpara>Velero logs the issue as a partial failure and then processes the rest of the objects in the <literal>Backup</literal> CR.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Check the status of a <literal>MigMigration</literal> CR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get migmigration &lt;migmigration&gt; -o yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">status:
  conditions:
  - category: Warn
    durable: true
    lastTransitionTime: "2021-01-26T20:48:40Z"
    message: 'Final Restore openshift-migration/ccc7c2d0-6017-11eb-afab-85d0007f5a19-x4lbf: partially failed on destination cluster'
    status: "True"
    type: VeleroFinalRestorePartiallyFailed
  - category: Advisory
    durable: true
    lastTransitionTime: "2021-01-26T20:48:42Z"
    message: The migration has completed with warnings, please look at `Warn` conditions.
    reason: Completed
    status: "True"
    type: SucceededWithWarnings</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Check the status of the <literal>Restore</literal> CR by using the Velero <literal>describe</literal> command:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">$ oc -n {namespace} exec deployment/velero -c velero -- ./velero \
  restore describe &lt;restore&gt;</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">Phase:  PartiallyFailed (run 'velero restore logs ccc7c2d0-6017-11eb-afab-85d0007f5a19-x4lbf' for more information)

Errors:
  Velero:     &lt;none&gt;
  Cluster:    &lt;none&gt;
  Namespaces:
    migration-example:  error restoring example.com/migration-example/migration-example: the server could not find the requested resource</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Check the <literal>Restore</literal> CR logs by using the Velero <literal>logs</literal> command:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">$ oc -n {namespace} exec deployment/velero -c velero -- ./velero \
  restore logs &lt;restore&gt;</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">time="2021-01-26T20:48:37Z" level=info msg="Attempting to restore migration-example: migration-example" logSource="pkg/restore/restore.go:1107" restore=openshift-migration/ccc7c2d0-6017-11eb-afab-85d0007f5a19-x4lbf
time="2021-01-26T20:48:37Z" level=info msg="error restoring migration-example: the server could not find the requested resource" logSource="pkg/restore/restore.go:1170" restore=openshift-migration/ccc7c2d0-6017-11eb-afab-85d0007f5a19-x4lbf</programlisting>
</para>
</formalpara>
<simpara>The <literal>Restore</literal> CR log error message, <literal>the server could not find the requested resource</literal>, indicates the cause of the partially failed migration.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="migration-using-mtc-crs-for-troubleshooting_troubleshooting-3-4">
<title>Using MTC custom resources for troubleshooting</title>
<simpara>You can check the following Migration Toolkit for Containers (MTC) custom resources (CRs) to troubleshoot a failed migration:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>MigCluster</literal></simpara>
</listitem>
<listitem>
<simpara><literal>MigStorage</literal></simpara>
</listitem>
<listitem>
<simpara><literal>MigPlan</literal></simpara>
</listitem>
<listitem>
<simpara><literal>BackupStorageLocation</literal></simpara>
<simpara>The <literal>BackupStorageLocation</literal> CR contains a <literal>migrationcontroller</literal> label to identify the MTC instance that created the CR:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">    labels:
      migrationcontroller: ebe13bee-c803-47d0-a9e9-83f380328b93</programlisting>
</listitem>
<listitem>
<simpara><literal>VolumeSnapshotLocation</literal></simpara>
<simpara>The <literal>VolumeSnapshotLocation</literal> CR contains a <literal>migrationcontroller</literal> label to identify the MTC instance that created the CR:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">    labels:
      migrationcontroller: ebe13bee-c803-47d0-a9e9-83f380328b93</programlisting>
</listitem>
<listitem>
<simpara><literal>MigMigration</literal></simpara>
</listitem>
<listitem>
<simpara><literal>Backup</literal></simpara>
<simpara>MTC changes the reclaim policy of migrated persistent volumes (PVs) to <literal>Retain</literal> on the target cluster. The <literal>Backup</literal> CR contains an <literal>openshift.io/orig-reclaim-policy</literal> annotation that indicates the original reclaim policy. You can manually restore the reclaim policy of the migrated PVs.</simpara>
</listitem>
<listitem>
<simpara><literal>Restore</literal></simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>List the <literal>MigMigration</literal> CRs in the <literal>openshift-migration</literal> namespace:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get migmigration -n openshift-migration</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                   AGE
88435fe0-c9f8-11e9-85e6-5d593ce65e10   6m42s</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Inspect the <literal>MigMigration</literal> CR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe migmigration 88435fe0-c9f8-11e9-85e6-5d593ce65e10 -n openshift-migration</programlisting>
<simpara>The output is similar to the following examples.</simpara>
</listitem>
</orderedlist>
<formalpara>
<title><literal>MigMigration</literal> example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">name:         88435fe0-c9f8-11e9-85e6-5d593ce65e10
namespace:    openshift-migration
labels:       &lt;none&gt;
annotations:  touch: 3b48b543-b53e-4e44-9d34-33563f0f8147
apiVersion:  migration.openshift.io/v1alpha1
kind:         MigMigration
metadata:
  creationTimestamp:  2019-08-29T01:01:29Z
  generation:          20
  resourceVersion:    88179
  selfLink:           /apis/migration.openshift.io/v1alpha1/namespaces/openshift-migration/migmigrations/88435fe0-c9f8-11e9-85e6-5d593ce65e10
  uid:                 8886de4c-c9f8-11e9-95ad-0205fe66cbb6
spec:
  migPlanRef:
    name:        socks-shop-mig-plan
    namespace:   openshift-migration
  quiescePods:  true
  stage:         false
status:
  conditions:
    category:              Advisory
    durable:               True
    lastTransitionTime:  2019-08-29T01:03:40Z
    message:               The migration has completed successfully.
    reason:                Completed
    status:                True
    type:                  Succeeded
  phase:                   Completed
  startTimestamp:         2019-08-29T01:01:29Z
events:                    &lt;none&gt;</programlisting>
</para>
</formalpara>
<formalpara>
<title><literal>Velero</literal> backup CR #2 example output that describes the PV data</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: velero.io/v1
kind: Backup
metadata:
  annotations:
    openshift.io/migrate-copy-phase: final
    openshift.io/migrate-quiesce-pods: "true"
    openshift.io/migration-registry: 172.30.105.179:5000
    openshift.io/migration-registry-dir: /socks-shop-mig-plan-registry-44dd3bd5-c9f8-11e9-95ad-0205fe66cbb6
    openshift.io/orig-reclaim-policy: delete
  creationTimestamp: "2019-08-29T01:03:15Z"
  generateName: 88435fe0-c9f8-11e9-85e6-5d593ce65e10-
  generation: 1
  labels:
    app.kubernetes.io/part-of: migration
    migmigration: 8886de4c-c9f8-11e9-95ad-0205fe66cbb6
    migration-stage-backup: 8886de4c-c9f8-11e9-95ad-0205fe66cbb6
    velero.io/storage-location: myrepo-vpzq9
  name: 88435fe0-c9f8-11e9-85e6-5d593ce65e10-59gb7
  namespace: openshift-migration
  resourceVersion: "87313"
  selfLink: /apis/velero.io/v1/namespaces/openshift-migration/backups/88435fe0-c9f8-11e9-85e6-5d593ce65e10-59gb7
  uid: c80dbbc0-c9f8-11e9-95ad-0205fe66cbb6
spec:
  excludedNamespaces: []
  excludedResources: []
  hooks:
    resources: []
  includeClusterResources: null
  includedNamespaces:
  - sock-shop
  includedResources:
  - persistentvolumes
  - persistentvolumeclaims
  - namespaces
  - imagestreams
  - imagestreamtags
  - secrets
  - configmaps
  - pods
  labelSelector:
    matchLabels:
      migration-included-stage-backup: 8886de4c-c9f8-11e9-95ad-0205fe66cbb6
  storageLocation: myrepo-vpzq9
  ttl: 720h0m0s
  volumeSnapshotLocations:
  - myrepo-wv6fx
status:
  completionTimestamp: "2019-08-29T01:02:36Z"
  errors: 0
  expiration: "2019-09-28T01:02:35Z"
  phase: Completed
  startTimestamp: "2019-08-29T01:02:35Z"
  validationErrors: null
  version: 1
  volumeSnapshotsAttempted: 0
  volumeSnapshotsCompleted: 0
  warnings: 0</programlisting>
</para>
</formalpara>
<formalpara>
<title><literal>Velero</literal> restore CR #2 example output that describes the Kubernetes resources</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: velero.io/v1
kind: Restore
metadata:
  annotations:
    openshift.io/migrate-copy-phase: final
    openshift.io/migrate-quiesce-pods: "true"
    openshift.io/migration-registry: 172.30.90.187:5000
    openshift.io/migration-registry-dir: /socks-shop-mig-plan-registry-36f54ca7-c925-11e9-825a-06fa9fb68c88
  creationTimestamp: "2019-08-28T00:09:49Z"
  generateName: e13a1b60-c927-11e9-9555-d129df7f3b96-
  generation: 3
  labels:
    app.kubernetes.io/part-of: migration
    migmigration: e18252c9-c927-11e9-825a-06fa9fb68c88
    migration-final-restore: e18252c9-c927-11e9-825a-06fa9fb68c88
  name: e13a1b60-c927-11e9-9555-d129df7f3b96-gb8nx
  namespace: openshift-migration
  resourceVersion: "82329"
  selfLink: /apis/velero.io/v1/namespaces/openshift-migration/restores/e13a1b60-c927-11e9-9555-d129df7f3b96-gb8nx
  uid: 26983ec0-c928-11e9-825a-06fa9fb68c88
spec:
  backupName: e13a1b60-c927-11e9-9555-d129df7f3b96-sz24f
  excludedNamespaces: null
  excludedResources:
  - nodes
  - events
  - events.events.k8s.io
  - backups.velero.io
  - restores.velero.io
  - resticrepositories.velero.io
  includedNamespaces: null
  includedResources: null
  namespaceMapping: null
  restorePVs: true
status:
  errors: 0
  failureReason: ""
  phase: Completed
  validationErrors: null
  warnings: 15</programlisting>
</para>
</formalpara>
</section>
</section>
<section xml:id="common-issues-and-concerns_troubleshooting-3-4">
<title>Common issues and concerns</title>
<simpara>This section describes common issues and concerns that can cause issues during migration.</simpara>
<section xml:id="migration-updating-deprecated-internal-images_troubleshooting-3-4">
<title>Updating deprecated internal images</title>
<simpara>If your application uses images from the <literal>openshift</literal> namespace, the required versions of the images must be present on the target cluster.</simpara>
<simpara>If an OpenShift Container Platform 3 image is deprecated in OpenShift Container Platform 4.14, you can manually update the image stream tag by using <literal>podman</literal>.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have <literal>podman</literal> installed.</simpara>
</listitem>
<listitem>
<simpara>You must be logged in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>If you are using insecure registries, add your registry host values to the <literal>[registries.insecure]</literal> section of <literal>/etc/container/registries.conf</literal> to ensure that <literal>podman</literal> does not encounter a TLS verification error.</simpara>
</listitem>
<listitem>
<simpara>The internal registries must be exposed on the source and target clusters.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Ensure that the internal registries are exposed on the OpenShift Container Platform 3 and 4 clusters.</simpara>
<simpara>The OpenShift image registry is exposed by default on OpenShift Container Platform 4.</simpara>
</listitem>
<listitem>
<simpara>If you are using insecure registries, add your registry host values to the <literal>[registries.insecure]</literal> section of <literal>/etc/container/registries.conf</literal> to ensure that <literal>podman</literal> does not encounter a TLS verification error.</simpara>
</listitem>
<listitem>
<simpara>Log in to the OpenShift Container Platform 3 registry:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ podman login -u $(oc whoami) -p $(oc whoami -t) --tls-verify=false &lt;registry_url&gt;:&lt;port&gt;</programlisting>
</listitem>
<listitem>
<simpara>Log in to the OpenShift Container Platform 4 registry:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ podman login -u $(oc whoami) -p $(oc whoami -t) --tls-verify=false &lt;registry_url&gt;:&lt;port&gt;</programlisting>
</listitem>
<listitem>
<simpara>Pull the OpenShift Container Platform 3 image:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ podman pull &lt;registry_url&gt;:&lt;port&gt;/openshift/&lt;image&gt;</programlisting>
</listitem>
<listitem>
<simpara>Tag the OpenShift Container Platform 3 image for the OpenShift Container Platform 4 registry:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ podman tag &lt;registry_url&gt;:&lt;port&gt;/openshift/&lt;image&gt; \ <co xml:id="CO35-1"/>
  &lt;registry_url&gt;:&lt;port&gt;/openshift/&lt;image&gt; <co xml:id="CO35-2"/></programlisting>
<calloutlist>
<callout arearefs="CO35-1">
<para>Specify the registry URL and port for the OpenShift Container Platform 3 cluster.</para>
</callout>
<callout arearefs="CO35-2">
<para>Specify the registry URL and port for the OpenShift Container Platform 4 cluster.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Push the image to the OpenShift Container Platform 4 registry:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ podman push &lt;registry_url&gt;:&lt;port&gt;/openshift/&lt;image&gt; <co xml:id="CO36-1"/></programlisting>
<calloutlist>
<callout arearefs="CO36-1">
<para>Specify the OpenShift Container Platform 4 cluster.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Verify that the image has a valid image stream:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get imagestream -n openshift | grep &lt;image&gt;</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME      IMAGE REPOSITORY                                                      TAGS    UPDATED
my_image  image-registry.openshift-image-registry.svc:5000/openshift/my_image  latest  32 seconds ago</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="migration-dvm-error-node-selectors_troubleshooting-3-4">
<title>Direct volume migration does not complete</title>
<simpara>If direct volume migration does not complete, the target cluster might not have the same <literal>node-selector</literal> annotations as the source cluster.</simpara>
<simpara>Migration Toolkit for Containers (MTC) migrates namespaces with all annotations to preserve security context constraints and scheduling requirements. During direct volume migration, MTC creates Rsync transfer pods on the target cluster in the namespaces that were migrated from the source cluster. If a target cluster namespace does not have the same annotations as the source cluster namespace, the Rsync transfer pods cannot be scheduled. The Rsync pods remain in a <literal>Pending</literal> state.</simpara>
<simpara>You can identify and fix this issue by performing the following procedure.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Check the status of the <literal>MigMigration</literal> CR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe migmigration &lt;pod&gt; -n openshift-migration</programlisting>
<simpara>The output includes the following status message:</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Some or all transfer pods are not running for more than 10 mins on destination cluster</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>On the source cluster, obtain the details of a migrated namespace:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get namespace &lt;namespace&gt; -o yaml <co xml:id="CO37-1"/></programlisting>
<calloutlist>
<callout arearefs="CO37-1">
<para>Specify the migrated namespace.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>On the target cluster, edit the migrated namespace:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit namespace &lt;namespace&gt;</programlisting>
</listitem>
<listitem>
<simpara>Add the missing <literal>openshift.io/node-selector</literal> annotations to the migrated namespace as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Namespace
metadata:
  annotations:
    openshift.io/node-selector: "region=east"
...</programlisting>
</listitem>
<listitem>
<simpara>Run the migration plan again.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="migration-error-messages_troubleshooting-3-4">
<title>Error messages and resolutions</title>
<simpara>This section describes common error messages you might encounter with the Migration Toolkit for Containers (MTC) and how to resolve their underlying causes.</simpara>
<section xml:id="ca-certificate-error-displayed-when-accessing-console-for-first-time_troubleshooting-3-4">
<title>CA certificate error displayed when accessing the MTC console for the first time</title>
<simpara>If the MTC console displays a <literal>CA certificate error</literal> message the first time you try to access it, the likely cause is that a cluster uses self-signed CA certificates.</simpara>
<simpara>Navigate to the <literal>oauth-authorization-server</literal> URL in the error message and accept the certificate. To resolve this issue permanently, install the certificate authority so that it is trusted.</simpara>
<simpara>If the browser displays an <literal>Unauthorized</literal> message after you have accepted the CA certificate, navigate to the MTC console and then refresh the web page.</simpara>
</section>
<section xml:id="oauth-timeout-error-in-console_troubleshooting-3-4">
<title>OAuth timeout error in the MTC console</title>
<simpara>If the MTC console displays a <literal>connection has timed out</literal> message after you have accepted a self-signed certificate, the cause is likely to be one of the following:</simpara>
<itemizedlist>
<listitem>
<simpara>Interrupted network access to the OAuth server</simpara>
</listitem>
<listitem>
<simpara>Interrupted network access to the OpenShift Container Platform console</simpara>
</listitem>
<listitem>
<simpara>Proxy configuration blocking access to the OAuth server. See <link xlink:href="https://access.redhat.com/solutions/5514491">MTC console inaccessible because of OAuth timeout error</link> for details.</simpara>
</listitem>
</itemizedlist>
<simpara>To determine the cause:</simpara>
<itemizedlist>
<listitem>
<simpara>Inspect the MTC console web page with a browser web inspector.</simpara>
</listitem>
<listitem>
<simpara>Check the <literal>Migration UI</literal> pod log for errors.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="certificate-signed-by-unknown-authority-error_troubleshooting-3-4">
<title>Certificate signed by unknown authority error</title>
<simpara>If you use a self-signed certificate to secure a cluster or a replication repository for the Migration Toolkit for Containers (MTC), certificate verification might fail with the following error message: <literal>Certificate signed by unknown authority</literal>.</simpara>
<simpara>You can create a custom CA certificate bundle file and upload it in the MTC web console when you add a cluster or a replication repository.</simpara>
<formalpara>
<title>Procedure</title>
<para>Download a CA certificate from a remote endpoint and save it as a CA bundle file:</para>
</formalpara>
<programlisting language="terminal" linenumbering="unnumbered">$ echo -n | openssl s_client -connect &lt;host_FQDN&gt;:&lt;port&gt; \ <co xml:id="CO38-1"/>
  | sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' &gt; &lt;ca_bundle.cert&gt; <co xml:id="CO38-2"/></programlisting>
<calloutlist>
<callout arearefs="CO38-1">
<para>Specify the host FQDN and port of the endpoint, for example, <literal>api.my-cluster.example.com:6443</literal>.</para>
</callout>
<callout arearefs="CO38-2">
<para>Specify the name of the CA bundle file.</para>
</callout>
</calloutlist>
</section>
<section xml:id="backup-storage-location-errors-in-velero-pod-log_troubleshooting-3-4">
<title>Backup storage location errors in the Velero pod log</title>
<simpara>If a <literal>Velero</literal> <literal>Backup</literal> custom resource contains a reference to a backup storage location (BSL) that does not exist, the <literal>Velero</literal> pod log might display the following error messages:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs &lt;Velero_Pod&gt; -n openshift-migration</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">level=error msg="Error checking repository for stale locks" error="error getting backup storage location: BackupStorageLocation.velero.io \"ts-dpa-1\" not found" error.file="/remote-source/src/github.com/vmware-tanzu/velero/pkg/restic/repository_manager.go:259"</programlisting>
</para>
</formalpara>
<simpara>You can ignore these error messages. A missing BSL cannot cause a migration to fail.</simpara>
</section>
<section xml:id="pod-volume-backup-timeout-error-in-velero-pod-log_troubleshooting-3-4">
<title>Pod volume backup timeout error in the Velero pod log</title>
<simpara>If a migration fails because <literal>Restic</literal> times out, the <literal>Velero</literal> pod log displays the following error:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">level=error msg="Error backing up item" backup=velero/monitoring error="timed out
waiting for all PodVolumeBackups to complete" error.file="/go/src/github.com/
heptio/velero/pkg/restic/backupper.go:165" error.function="github.com/heptio/
velero/pkg/restic.(*backupper).BackupPodVolumes" group=v1</programlisting>
<simpara>The default value of <literal>restic_timeout</literal> is one hour. You can increase this parameter for large migrations, keeping in mind that a higher value may delay the return of error messages.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the OpenShift Container Platform web console, navigate to <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">Installed Operators</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Migration Toolkit for Containers Operator</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">MigrationController</emphasis> tab, click <emphasis role="strong">migration-controller</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">YAML</emphasis> tab, update the following parameter value:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">spec:
  restic_timeout: 1h <co xml:id="CO39-1"/></programlisting>
<calloutlist>
<callout arearefs="CO39-1">
<para>Valid units are <literal>h</literal> (hours), <literal>m</literal> (minutes), and <literal>s</literal> (seconds), for example, <literal>3h30m15s</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Save</emphasis>.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="restic-verification-errors-in-migmigration-custom-resource_troubleshooting-3-4">
<title>Restic verification errors in the MigMigration custom resource</title>
<simpara>If data verification fails when migrating a persistent volume with the file system data copy method, the <literal>MigMigration</literal> CR displays the following error:</simpara>
<formalpara>
<title>MigMigration CR status</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">status:
  conditions:
  - category: Warn
    durable: true
    lastTransitionTime: 2020-04-16T20:35:16Z
    message: There were verify errors found in 1 Restic volume restores. See restore `&lt;registry-example-migration-rvwcm&gt;`
      for details <co xml:id="CO40-1"/>
    status: "True"
    type: ResticVerifyErrors <co xml:id="CO40-2"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO40-1">
<para>The error message identifies the <literal>Restore</literal> CR name.</para>
</callout>
<callout arearefs="CO40-2">
<para><literal>ResticVerifyErrors</literal> is a general error warning type that includes verification errors.</para>
</callout>
</calloutlist>
<note>
<simpara>A data verification error does not cause the migration process to fail.</simpara>
</note>
<simpara>You can check the <literal>Restore</literal> CR to troubleshoot the data verification error.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Log in to the target cluster.</simpara>
</listitem>
<listitem>
<simpara>View the <literal>Restore</literal> CR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe &lt;registry-example-migration-rvwcm&gt; -n openshift-migration</programlisting>
<simpara>The output identifies the persistent volume with <literal>PodVolumeRestore</literal> errors.</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">status:
  phase: Completed
  podVolumeRestoreErrors:
  - kind: PodVolumeRestore
    name: &lt;registry-example-migration-rvwcm-98t49&gt;
    namespace: openshift-migration
  podVolumeRestoreResticErrors:
  - kind: PodVolumeRestore
    name: &lt;registry-example-migration-rvwcm-98t49&gt;
    namespace: openshift-migration</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>View the <literal>PodVolumeRestore</literal> CR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe &lt;migration-example-rvwcm-98t49&gt;</programlisting>
<simpara>The output identifies the <literal>Restic</literal> pod that logged the errors.</simpara>
<formalpara>
<title>PodVolumeRestore CR with Restic pod error</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">  completionTimestamp: 2020-05-01T20:49:12Z
  errors: 1
  resticErrors: 1
  ...
  resticPod: &lt;restic-nr2v5&gt;</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>View the <literal>Restic</literal> pod log to locate the errors:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs -f &lt;restic-nr2v5&gt;</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="restic-permission-error-when-migrating-from-nfs-storage-with-root-squash-enabled_troubleshooting-3-4">
<title>Restic permission error when migrating from NFS storage with root_squash enabled</title>
<simpara>If you are migrating data from NFS storage and <literal>root_squash</literal> is enabled, <literal>Restic</literal> maps to <literal>nfsnobody</literal> and does not have permission to perform the migration. The <literal>Restic</literal> pod log displays the following error:</simpara>
<formalpara>
<title>Restic permission error</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">backup=openshift-migration/&lt;backup_id&gt; controller=pod-volume-backup error="fork/exec
/usr/bin/restic: permission denied" error.file="/go/src/github.com/vmware-tanzu/
velero/pkg/controller/pod_volume_backup_controller.go:280" error.function=
"github.com/vmware-tanzu/velero/pkg/controller.(*podVolumeBackupController).processBackup"
logSource="pkg/controller/pod_volume_backup_controller.go:280" name=&lt;backup_id&gt;
namespace=openshift-migration</programlisting>
</para>
</formalpara>
<simpara>You can resolve this issue by creating a supplemental group for <literal>Restic</literal> and adding the group ID to the <literal>MigrationController</literal> CR manifest.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a supplemental group for <literal>Restic</literal> on the NFS storage.</simpara>
</listitem>
<listitem>
<simpara>Set the <literal>setgid</literal> bit on the NFS directories so that group ownership is inherited.</simpara>
</listitem>
<listitem>
<simpara>Add the <literal>restic_supplemental_groups</literal> parameter to the <literal>MigrationController</literal> CR manifest on the source and target clusters:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">spec:
  restic_supplemental_groups: &lt;group_id&gt; <co xml:id="CO41-1"/></programlisting>
<calloutlist>
<callout arearefs="CO41-1">
<para>Specify the supplemental group ID.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Wait for the <literal>Restic</literal> pods to restart so that the changes are applied.</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="migration-known-issues_troubleshooting-3-4">
<title>Known issues</title>
<simpara>This release has the following known issues:</simpara>
<itemizedlist>
<listitem>
<simpara>During migration, the Migration Toolkit for Containers (MTC) preserves the following namespace annotations:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>openshift.io/sa.scc.mcs</literal></simpara>
</listitem>
<listitem>
<simpara><literal>openshift.io/sa.scc.supplemental-groups</literal></simpara>
</listitem>
<listitem>
<simpara><literal>openshift.io/sa.scc.uid-range</literal></simpara>
<simpara>These annotations preserve the UID range, ensuring that the containers retain their file system permissions on the target cluster. There is a risk that the migrated UIDs could duplicate UIDs within an existing or future namespace on the target cluster. (<link xlink:href="https://bugzilla.redhat.com/show_bug.cgi?id=1748440"><emphasis role="strong">BZ#1748440</emphasis></link>)</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Most cluster-scoped resources are not yet handled by MTC. If your applications require cluster-scoped resources, you might have to create them manually on the target cluster.</simpara>
</listitem>
<listitem>
<simpara>If a migration fails, the migration plan does not retain custom PV settings for quiesced pods. You must manually roll back the migration, delete the migration plan, and create a new migration plan with your PV settings. (<link xlink:href="https://bugzilla.redhat.com/show_bug.cgi?id=1784899"><emphasis role="strong">BZ#1784899</emphasis></link>)</simpara>
</listitem>
<listitem>
<simpara>If a large migration fails because Restic times out, you can increase the <literal>restic_timeout</literal> parameter value (default: <literal>1h</literal>) in the <literal>MigrationController</literal> custom resource (CR) manifest.</simpara>
</listitem>
<listitem>
<simpara>If you select the data verification option for PVs that are migrated with the file system copy method, performance is significantly slower.</simpara>
</listitem>
<listitem>
<simpara>If you are migrating data from NFS storage and <literal>root_squash</literal> is enabled, <literal>Restic</literal> maps to <literal>nfsnobody</literal>. The migration fails and a permission error is displayed in the <literal>Restic</literal> pod log. (<link xlink:href="https://bugzilla.redhat.com/show_bug.cgi?id=1873641"><emphasis role="strong">BZ#1873641</emphasis></link>)</simpara>
<simpara>You can resolve this issue by adding supplemental groups for <literal>Restic</literal> to the <literal>MigrationController</literal> CR manifest:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">spec:
...
  restic_supplemental_groups:
  - 5555
  - 6666</programlisting>
</listitem>
<listitem>
<simpara>If you perform direct volume migration with nodes that are in different availability zones or availability sets, the migration might fail because the migrated pods cannot access the PVC. (<link xlink:href="https://bugzilla.redhat.com/show_bug.cgi?id=1947487"><emphasis role="strong">BZ#1947487</emphasis></link>)</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="rolling-back-migration_troubleshooting-3-4">
<title>Rolling back a migration</title>
<simpara>You can roll back a migration by using the MTC web console or the CLI.</simpara>
<simpara>You can also <link linkend="migration-rolling-back-migration-manually_troubleshooting-3-4">roll back a migration manually</link>.</simpara>
<section xml:id="migration-rolling-back-migration-web-console_troubleshooting-3-4">
<title>Rolling back a migration by using the MTC web console</title>
<simpara>You can roll back a migration by using the Migration Toolkit for Containers (MTC) web console.</simpara>
<note>
<simpara>The following resources remain in the migrated namespaces for debugging after a failed direct volume migration (DVM):</simpara>
<itemizedlist>
<listitem>
<simpara>Config maps (source and destination clusters)</simpara>
</listitem>
<listitem>
<simpara><literal>Secret</literal> objects (source and destination clusters)</simpara>
</listitem>
<listitem>
<simpara><literal>Rsync</literal> CRs (source cluster)</simpara>
</listitem>
</itemizedlist>
<simpara>These resources do not affect rollback. You can delete them manually.</simpara>
<simpara>If you later run the same migration plan successfully, the resources from the failed migration are deleted automatically.</simpara>
</note>
<simpara>If your application was stopped during a failed migration, you must roll back the migration to prevent data corruption in the persistent volume.</simpara>
<simpara>Rollback is not required if the application was not stopped during migration because the original application is still running on the source cluster.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the MTC web console, click <emphasis role="strong">Migration plans</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click the Options menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> beside a migration plan and select <emphasis role="strong">Rollback</emphasis> under <emphasis role="strong">Migration</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Rollback</emphasis> and wait for rollback to complete.</simpara>
<simpara>In the migration plan details, <emphasis role="strong">Rollback succeeded</emphasis> is displayed.</simpara>
</listitem>
<listitem>
<simpara>Verify that rollback was successful in the OpenShift Container Platform web console of the source cluster:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Click <emphasis role="strong">Home</emphasis> &#8594; <emphasis role="strong">Projects</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click the migrated project to view its status.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">Routes</emphasis> section, click <emphasis role="strong">Location</emphasis> to verify that the application is functioning, if applicable.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Workloads</emphasis> &#8594; <emphasis role="strong">Pods</emphasis> to verify that the pods are running in the migrated namespace.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Storage</emphasis> &#8594; <emphasis role="strong">Persistent volumes</emphasis> to verify that the migrated persistent volume is correctly provisioned.</simpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="migration-rolling-back-migration-cli_troubleshooting-3-4">
<title>Rolling back a migration from the command line interface</title>
<simpara>You can roll back a migration by creating a <literal>MigMigration</literal> custom resource (CR) from the command line interface.</simpara>
<note>
<simpara>The following resources remain in the migrated namespaces for debugging after a failed direct volume migration (DVM):</simpara>
<itemizedlist>
<listitem>
<simpara>Config maps (source and destination clusters)</simpara>
</listitem>
<listitem>
<simpara><literal>Secret</literal> objects (source and destination clusters)</simpara>
</listitem>
<listitem>
<simpara><literal>Rsync</literal> CRs (source cluster)</simpara>
</listitem>
</itemizedlist>
<simpara>These resources do not affect rollback. You can delete them manually.</simpara>
<simpara>If you later run the same migration plan successfully, the resources from the failed migration are deleted automatically.</simpara>
</note>
<simpara>If your application was stopped during a failed migration, you must roll back the migration to prevent data corruption in the persistent volume.</simpara>
<simpara>Rollback is not required if the application was not stopped during migration because the original application is still running on the source cluster.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>MigMigration</literal> CR based on the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">$ cat &lt;&lt; EOF | oc apply -f -
apiVersion: migration.openshift.io/v1alpha1
kind: MigMigration
metadata:
  labels:
    controller-tools.k8s.io: "1.0"
  name: &lt;migmigration&gt;
  namespace: openshift-migration
spec:
...
  rollback: true
...
  migPlanRef:
    name: &lt;migplan&gt; <co xml:id="CO42-1"/>
    namespace: openshift-migration
EOF</programlisting>
<calloutlist>
<callout arearefs="CO42-1">
<para>Specify the name of the associated <literal>MigPlan</literal> CR.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>In the MTC web console, verify that the migrated project resources have been removed from the target cluster.</simpara>
</listitem>
<listitem>
<simpara>Verify that the migrated project resources are present in the source cluster and that the application is running.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="migration-rolling-back-migration-manually_troubleshooting-3-4">
<title>Rolling back a migration manually</title>
<simpara>You can roll back a failed migration manually by deleting the <literal>stage</literal> pods and unquiescing the application.</simpara>
<simpara>If you run the same migration plan successfully, the resources from the failed migration are deleted automatically.</simpara>
<note>
<simpara>The following resources remain in the migrated namespaces after a failed direct volume migration (DVM):</simpara>
<itemizedlist>
<listitem>
<simpara>Config maps (source and destination clusters)</simpara>
</listitem>
<listitem>
<simpara><literal>Secret</literal> objects (source and destination clusters)</simpara>
</listitem>
<listitem>
<simpara><literal>Rsync</literal> CRs (source cluster)</simpara>
</listitem>
</itemizedlist>
<simpara>These resources do not affect rollback. You can delete them manually.</simpara>
</note>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Delete the <literal>stage</literal> pods on all clusters:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete $(oc get pods -l migration.openshift.io/is-stage-pod -n &lt;namespace&gt;) <co xml:id="CO43-1"/></programlisting>
<calloutlist>
<callout arearefs="CO43-1">
<para>Namespaces specified in the <literal>MigPlan</literal> CR.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Unquiesce the application on the source cluster by scaling the replicas to their premigration number:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc scale deployment &lt;deployment&gt; --replicas=&lt;premigration_replicas&gt;</programlisting>
<simpara>The <literal>migration.openshift.io/preQuiesceReplicas</literal> annotation in the <literal>Deployment</literal> CR displays the premigration number of replicas:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
    migration.openshift.io/preQuiesceReplicas: "1"</programlisting>
</listitem>
<listitem>
<simpara>Verify that the application pods are running on the source cluster:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pod -n &lt;namespace&gt;</programlisting>
</listitem>
</orderedlist>
<bridgehead xml:id="additional-resources-uninstalling_troubleshooting-3-4" role="_additional-resources" renderas="sect3">Additional resources</bridgehead>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/operators/#olm-deleting-operators-from-a-cluster-using-web-console_olm-deleting-operators-from-cluster">Deleting Operators from a cluster using the web console</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
</chapter>
</book>
